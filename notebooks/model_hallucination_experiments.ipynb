{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Markdown, Latex\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gd1279/.netrc\r\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/gd1279/projects/Rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats, ndimage\n",
    "from scipy.special import factorial\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "import wandb\n",
    "from collections import defaultdict, deque, namedtuple\n",
    "import os\n",
    "import argparse\n",
    "import atari_py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import colorsys\n",
    "\n",
    "from agent import Agent\n",
    "from env import make_env\n",
    "from masker import ALL_MASKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_SCRATCH_FOLDER = '/scratch/gd1279'\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_SCRATCH_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_SCRATCH_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--conv3-num-filters'], dest='conv3_num_filters', nargs=None, const=None, default=64, type=None, choices=None, help='Number of filters in third convolutional layer. Default matches the cannonical Rainbow architecture.', metavar=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that hyperparameters may originally be reported in ATARI game frames instead of agent steps\n",
    "parser = argparse.ArgumentParser(description='Rainbow')\n",
    "parser.add_argument('--id', type=str, default='default', help='Experiment ID')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed')\n",
    "parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n",
    "parser.add_argument('--game', type=str, default='space_invaders', choices=atari_py.list_games(), help='ATARI game')\n",
    "parser.add_argument('--T-max', type=int, default=int(50e6), metavar='STEPS', help='Number of training steps (4x number of frames)')\n",
    "parser.add_argument('--max-episode-length', type=int, default=int(108e3), metavar='LENGTH', help='Max episode length in game frames (0 to disable)')\n",
    "parser.add_argument('--history-length', type=int, default=4, metavar='T', help='Number of consecutive states processed')\n",
    "parser.add_argument('--architecture', type=str, default='canonical', choices=['canonical', 'data-efficient'], metavar='ARCH', help='Network architecture')\n",
    "parser.add_argument('--hidden-size', type=int, default=512, metavar='SIZE', help='Network hidden size')\n",
    "parser.add_argument('--noisy-std', type=float, default=0.1, metavar='σ', help='Initial standard deviation of noisy linear layers')\n",
    "parser.add_argument('--atoms', type=int, default=51, metavar='C', help='Discretised size of value distribution')\n",
    "parser.add_argument('--V-min', type=float, default=-10, metavar='V', help='Minimum of value distribution support')\n",
    "parser.add_argument('--V-max', type=float, default=10, metavar='V', help='Maximum of value distribution support')\n",
    "parser.add_argument('--model', type=str, metavar='PARAMS', help='Pretrained model (state dict)')\n",
    "parser.add_argument('--memory-capacity', type=int, default=int(1e6), metavar='CAPACITY', help='Experience replay memory capacity')\n",
    "parser.add_argument('--replay-frequency', type=int, default=4, metavar='k', help='Frequency of sampling from memory')\n",
    "parser.add_argument('--priority-exponent', type=float, default=0.5, metavar='ω', help='Prioritised experience replay exponent (originally denoted α)')\n",
    "parser.add_argument('--priority-weight', type=float, default=0.4, metavar='β', help='Initial prioritised experience replay importance sampling weight')\n",
    "parser.add_argument('--multi-step', type=int, default=3, metavar='n', help='Number of steps for multi-step return')\n",
    "parser.add_argument('--discount', type=float, default=0.99, metavar='γ', help='Discount factor')\n",
    "parser.add_argument('--target-update', type=int, default=int(8e3), metavar='τ', help='Number of steps after which to update target network')\n",
    "parser.add_argument('--reward-clip', type=int, default=1, metavar='VALUE', help='Reward clipping (0 to disable)')\n",
    "parser.add_argument('--learning-rate', type=float, default=0.0000625, metavar='η', help='Learning rate')\n",
    "parser.add_argument('--adam-eps', type=float, default=1.5e-4, metavar='ε', help='Adam epsilon')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='SIZE', help='Batch size')\n",
    "parser.add_argument('--learn-start', type=int, default=int(20e3), metavar='STEPS', help='Number of steps before starting training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate only')\n",
    "parser.add_argument('--evaluation-interval', type=int, default=100000, metavar='STEPS', help='Number of training steps between evaluations')\n",
    "parser.add_argument('--evaluation-episodes', type=int, default=10, metavar='N', help='Number of evaluation episodes to average over')\n",
    "parser.add_argument('--evaluation-size', type=int, default=500, metavar='N', help='Number of transitions to use for validating Q')\n",
    "parser.add_argument('--render', action='store_true', help='Display screen (testing only)')\n",
    "parser.add_argument('--enable-cudnn', action='store_true', help='Enable cuDNN (faster but nondeterministic)')\n",
    "parser.add_argument('--save-evaluation-gifs', action='store_true', help='Save GIFs of evaluation episodes')\n",
    "parser.add_argument('--evaluation-gif-folder', default=None, help='Folder to save evaluation GIFs in')\n",
    "parser.add_argument('--save-evaluation-states', action='store_true', help='Save the states of evaluation episodes')\n",
    "parser.add_argument('--evaluation-state-folder', default=None, help='Folder to save evaluation state in')\n",
    "\n",
    "# Custom arguments I added\n",
    "\n",
    "SCRATCH_FOLDER = r'/misc/vlgscratch4/LakeGroup/guy/'\n",
    "\n",
    "DEFUALT_WANDB_ENTITY = 'augmented-frostbite'\n",
    "parser.add_argument('--wandb-entity', default=DEFUALT_WANDB_ENTITY)\n",
    "DEFAULT_WANDB_PROJECT = 'initial-experiments'\n",
    "parser.add_argument('--wandb-project', default=DEFAULT_WANDB_PROJECT)\n",
    "DEFAULT_WANDB_DIR = SCRATCH_FOLDER  # wandb creates its own folder inside\n",
    "parser.add_argument('--wandb-dir', default=DEFAULT_WANDB_DIR)\n",
    "parser.add_argument('--wandb-omit-watch', action='store_true')\n",
    "parser.add_argument('--wandb-resume', action='store_true')\n",
    "DEFAULT_MEMORY_SAVE_FOLDER = os.path.join(SCRATCH_FOLDER, 'rainbow_memory')\n",
    "parser.add_argument('--memory-save-folder', default=DEFAULT_MEMORY_SAVE_FOLDER)\n",
    "parser.add_argument('--memory-save-interval', type=int, default=None, help='How often to save the memory, defaults to the evaluation interval')\n",
    "parser.add_argument('--use-native-pickle-serialization', action='store_true', help='Use native pickle saving rather than torch.save()')\n",
    "\n",
    "# Arguments for the augmented representations\n",
    "parser.add_argument('--add-masks', action='store_true', help='Add masks for each semantic object types')\n",
    "parser.add_argument('--maskers', default=None, help='Select specific maskers to use')\n",
    "parser.add_argument('--use-numpy-masker', action='store_true', help='Use the previous, much slower numpy-based masker')\n",
    "parser.add_argument('--omit-pixels', action='store_true', help='Omit the raw pixels from the environment')\n",
    "parser.add_argument('--zero-out-masks-test', action='store_true', help='Test zeroing out particular indices')\n",
    "parser.add_argument('--zero-out-mask-indices', default=None, help='Which indices to zero out each time')\n",
    "parser.add_argument('--custom-mask-grouping', type=str, action='append', nargs='+',\n",
    "                    help='Create custom mask groupings. Provide the name of all masks in a particular group to each invocation of this argument.')\n",
    "\n",
    "# Arguments to give it a soft time cap that will help it not fail\n",
    "parser.add_argument('--soft-time-cap', help='Format: <DD>:HH:MM, stop after some soft cap such that the saving the memory does not fail')\n",
    "\n",
    "# Debugging-related arguments\n",
    "parser.add_argument('--debug-heap', action='store_true')\n",
    "parser.add_argument('--heap-interval', default=1e4)\n",
    "parser.add_argument('--heap-debug-file', default=None)\n",
    "\n",
    "\n",
    "# Arguments to help with matching then number of parameters between models\n",
    "parser.add_argument('--conv1-num-filters', default=32, help='Number of filters in first convolutional layer. Default matches the cannonical Rainbow architecture.')\n",
    "parser.add_argument('--conv2-num-filters', default=64, help='Number of filters in second convolutional layer. Default matches the cannonical Rainbow architecture.')\n",
    "parser.add_argument('--conv3-num-filters', default=64, help='Number of filters in third convolutional layer. Default matches the cannonical Rainbow architecture.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a run and its model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_checkpoint(run, step=None):\n",
    "    files = run.files()\n",
    "    if step is None:\n",
    "        step = max([int(f.name[f.name.rfind('-') + 1:f.name.rfind('.')]) \n",
    "                    for f in files \n",
    "                    if f.name.endswith('.pth')])\n",
    "        \n",
    "    sample_name = [f.name for f in files if f.name.endswith('.pth')][0]\n",
    "    checkpoint_name = sample_name[:sample_name.rfind('-')]\n",
    "    checkpoint_file = f'{checkpoint_name}-{step}.pth'\n",
    "    try:\n",
    "        run.file(checkpoint_file).download(replace=True, root=CHECKPOINT_SCRATCH_FOLDER)\n",
    "    except (wandb.CommError, AttributeError) as e:\n",
    "        return None\n",
    "    \n",
    "    return os.path.join(CHECKPOINT_SCRATCH_FOLDER, checkpoint_file)\n",
    "\n",
    "\n",
    "def setup_args(run):\n",
    "    args = parser.parse_args([])\n",
    "    config = run.config\n",
    "    \n",
    "    for key in config:\n",
    "        if key in args:\n",
    "            args.__setattr__(key, config[key])\n",
    "            \n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if torch.cuda.is_available() and not args.disable_cuda:\n",
    "        args.device = torch.device('cuda')\n",
    "        # torch.cuda.manual_seed(np.random.randint(1, 10000))\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        torch.backends.cudnn.enabled = args.enable_cudnn\n",
    "    else:\n",
    "        args.device = torch.device('cpu')\n",
    "            \n",
    "    return args\n",
    "\n",
    "\n",
    "LOADED_MODEL_CACHE = {}\n",
    "\n",
    "\n",
    "def load_model_from_run(run, step=None, cache=LOADED_MODEL_CACHE):\n",
    "    key = (run, step)\n",
    "    if key not in cache:\n",
    "        checkpoint_path = download_checkpoint(run, step)\n",
    "        if checkpoint_path is None:\n",
    "            return\n",
    "        \n",
    "        args = setup_args(run)\n",
    "        args.model = checkpoint_path\n",
    "\n",
    "        env = make_env(args)\n",
    "        dqn = Agent(args, env)\n",
    "        cache[key] = dqn, env\n",
    "        \n",
    "    return cache[key]\n",
    "\n",
    "\n",
    "FIGURE_TEMPLATE = r'''\\begin{{figure}}[!htb]\n",
    "% \\vspace{{-0.225in}}\n",
    "\\centering\n",
    "\\includegraphics[width=\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "% \\vspace{{-0.2in}}\n",
    "\\end{{figure}}\n",
    "'''\n",
    "WRAPFIGURE_TEMPLATE = r'''\\begin{{wrapfigure}}{{r}}{{0.5\\linewidth}}\n",
    "\\vspace{{-.3in}}\n",
    "\\begin{{spacing}}{{1.0}}\n",
    "\\centering\n",
    "\\includegraphics[width=0.95\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "\\end{{spacing}}\n",
    "% \\vspace{{-.25in}}\n",
    "\\end{{wrapfigure}}'''\n",
    "\n",
    "SAVE_PATH_PREFIX = 'figures'\n",
    "\n",
    "\n",
    "def save(save_path, bbox_inches='tight'):\n",
    "    if save_path is not None:\n",
    "        save_path_no_ext = os.path.splitext(save_path)[0]\n",
    "        print('Figure:\\n')\n",
    "        print(FIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "        print('\\n Wrapfigure:\\n')\n",
    "        print(WRAPFIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "        print('')\n",
    "        \n",
    "        if not save_path.startswith(SAVE_PATH_PREFIX):\n",
    "            save_path = os.path.join(SAVE_PATH_PREFIX, save_path)\n",
    "        \n",
    "        folder, filename = os.path.split(save_path)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=bbox_inches, facecolor=plt.gcf().get_facecolor(), edgecolor='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "\n",
    "ModelResults = namedtuple('ModelResults', \n",
    "                          ('hidden_states', 'q_values', 'state_values', 'actions'))\n",
    "\n",
    "KeyIndex = namedtuple('KeyIndex',\n",
    "                      ('peak_index', 'peak_value', 'start', 'end', 'count', 'indices'))\n",
    "\n",
    "\n",
    "def rgb_to_grayscale(obs):\n",
    "    # My best approximation of how the ALE does it\n",
    "    is_tensor = isinstance(obs, torch.Tensor)\n",
    "    if is_tensor:\n",
    "        rgb = obs.type(torch.float32)\n",
    "    else:\n",
    "        rgb = obs.astype(np.float32)\n",
    "        \n",
    "    gray = rgb[:,:,0] * 0.299 + rgb[:,:,1] * 0.587 + rgb[:,:,2] * 0.114\n",
    "    \n",
    "    if is_tensor:\n",
    "        return gray.type(torch.uint8)\n",
    "    else:\n",
    "        return gray.astype(np.uint8)\n",
    "\n",
    "\n",
    "def observation_to_model(env, obs):\n",
    "    return env._prepare_state(env._to_tensor(rgb_to_grayscale(obs)), env._to_tensor(obs))\n",
    "\n",
    "\n",
    "MAX_STATE_IDX = None\n",
    "SKIP = 2\n",
    "\n",
    "\n",
    "def pass_states_through_model(model, env, observations, max_state_idx=MAX_STATE_IDX, skip=SKIP):\n",
    "    state_buffer = deque([], maxlen=4)\n",
    "    hidden_states = []\n",
    "    q_values = []\n",
    "    state_values = []\n",
    "    actions = []\n",
    "\n",
    "    for frame in observations[:3]:\n",
    "        state_buffer.append(observation_to_model(env, frame))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frame in observations[3:max_state_idx]:\n",
    "            state_buffer.append(observation_to_model(env, frame))\n",
    "            state = torch.cat(list(state_buffer), 0)\n",
    "            \n",
    "            hidden_state = model.online_net.convs(state.unsqueeze(0)).view(-1)\n",
    "            hidden_states.append(hidden_state.detach().cpu().numpy())\n",
    "            \n",
    "            q_values.append(model.expected_q_values(state))\n",
    "            state_values.append(model.evaluate_q(state))\n",
    "            actions.append(model.act(state))\n",
    "\n",
    "    hidden_state_array = np.array(hidden_states[::skip])\n",
    "    q_values_array = np.array(q_values[::skip])\n",
    "    state_value_array = np.array(state_values[::skip])\n",
    "    action_array = np.array(actions[::skip])\n",
    "    \n",
    "    return ModelResults(hidden_state_array, q_values_array, state_value_array, action_array)\n",
    "\n",
    "\n",
    "def plot_entire_state(observations, start_index, num_frames=4):\n",
    "    figure = plt.figure(figsize=(18, 4))\n",
    "    for i in range(num_frames):\n",
    "        ax = plt.subplot(1, num_frames, i + 1)\n",
    "        ax.imshow(observations[start_index + i])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_embeddings_and_state(embeddings, color_values, observations, start_index, special_indices=None, num_frames=4,\n",
    "                              low_alpha=0.1, medium_alpha=0.7, alpha_threshold=0.7):\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(16, 8))\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    \n",
    "    tsne_ax = fig.add_subplot(gs[:, :2])\n",
    "    cmap = matplotlib.cm.get_cmap('Spectral_r')\n",
    "    normalizer = matplotlib.colors.Normalize(np.min(color_values), np.max(color_values))\n",
    "\n",
    "    colors = np.array([cmap(normalizer(d)) for d in color_values])\n",
    "    colors[:,3] = np.abs(color_values) / np.max(np.abs(color_values))\n",
    "    colors[colors[:,3] > alpha_threshold, 3] = medium_alpha\n",
    "    colors[colors[:,3] < alpha_threshold, 3] = low_alpha\n",
    "\n",
    "    mask = np.zeros(Y.shape[0], dtype=bool)\n",
    "    if special_indices is not None:\n",
    "        mask[special_indices] = True\n",
    "        colors[mask, 3] = 1\n",
    "\n",
    "    tsne_ax.scatter(embeddings[~mask, 0], embeddings[~mask, 1], color=colors[~mask])\n",
    "    tsne_ax.scatter(embeddings[mask, 0], embeddings[mask, 1], color='purple', s=50, marker='x')\n",
    "    plt.colorbar(matplotlib.cm.ScalarMappable(norm=normalizer, cmap=cmap), ax=tsne_ax)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        ax = fig.add_subplot(gs[i // 2, 2 + (i % 2)])\n",
    "        ax.imshow(observations[start_index + i])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "ALE_ACTIONS = {\n",
    "    0: 'noop',\n",
    "    1: 'fire',\n",
    "    2: 'up',\n",
    "    3: 'right',\n",
    "    4: 'left',\n",
    "    5: 'down',\n",
    "    6: 'up + right',\n",
    "    7: 'up + left',\n",
    "    8: 'down + right',\n",
    "    9: 'down + left',\n",
    "    10: 'up + fire',\n",
    "    11: 'right + fire',\n",
    "    12: 'left + fire',\n",
    "    13: 'down + fire',\n",
    "    14: 'up + right + fire',\n",
    "    15: 'up + left + fire',\n",
    "    16: 'down + right + fire',\n",
    "    17: 'down + left + fire'\n",
    "}\n",
    "\n",
    "\n",
    "def print_model_state_description(results, name, index, top_k=3):\n",
    "    value = results.state_values[index]\n",
    "    action = results.actions[index]\n",
    "    print(f'{name} had value {value:.3f} => {ALE_ACTIONS[action]} ({action})')\n",
    "    q = results.q_values[index].cpu().numpy()\n",
    "    top_actions = np.argpartition(q, -top_k)[-top_k:]\n",
    "    top_actions = top_actions[np.argsort(q[top_actions])][::-1]\n",
    "    p = F.softmax(results.q_values[index], dim=0)\n",
    "    print(' | '.join([f'({i + 1}) {ALE_ACTIONS[a]} [{a}], Q = {q[a]:.3f}, P = {p[a]:.3f}' for i, a in enumerate(top_actions)]))\n",
    "\n",
    "    \n",
    "def describe_states_by_indices(indices, first_model_results, first_model_name, second_model_results, second_model_name, \n",
    "                               observations, embeddings, color_values, top_k=3, plot_embeddings=False):\n",
    "    for key_index in sorted(indices, key=lambda ki: ki.peak_index):\n",
    "        index = key_index.peak_index\n",
    "        print(f'At index {index}')\n",
    "        print_model_state_description(first_model_results, first_model_name, index, top_k)\n",
    "        print_model_state_description(second_model_results, second_model_name, index, top_k)\n",
    "        \n",
    "        if plot_embeddings:\n",
    "            plot_embeddings_and_state(embeddings, color_values, observations, \n",
    "                                      key_index.peak_index, \n",
    "                                      special_indices=key_index.indices)\n",
    "        else:\n",
    "            plot_entire_state(observations, index)\n",
    "    \n",
    "    \n",
    "def find_diverging_states(values, indices, min_distance=10):\n",
    "    index_values = [values[i] for i in indices]\n",
    "    output = [KeyIndex(indices[0], index_values[0], indices[0], indices[0], 1, [indices[0]])]\n",
    "    \n",
    "    for index, value in zip(indices[1:], index_values[1:]):\n",
    "        current = output[-1]\n",
    "        \n",
    "        # Sufficiently far away, append a new one\n",
    "        if index > current.end + min_distance:\n",
    "            output.append(KeyIndex(index, value, index, index, 1, [index]))\n",
    "            \n",
    "        # Value more extreme, replace peak\n",
    "        elif abs(value) > abs(current.peak_value):  \n",
    "            output[-1] = KeyIndex(index, value, current.start, index, current.count + 1, current.indices + [index])\n",
    "            \n",
    "        # Value not more extreme, extend\n",
    "        else:\n",
    "            output[-1] = KeyIndex(current.peak_index, current.peak_value, current.start, index, current.count + 1, current.indices + [index])\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gameplan\n",
    "\n",
    "1. See that I can locate objects\n",
    "2. See that I can add additional objects\n",
    "3. See what this changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch\n",
    "\n",
    "Using model 306 from the baseline condition, one of the average models, not the 'superstar' one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SAVED_STATES = r'/home/gd1279/scratch/rainbow-evaluation-state-traces/baseline-rainbow-305/evaluation/states/eval-baseline-rainbow-305-34350000-0-env.pickle'\n",
    "\n",
    "with open(SAMPLE_SAVED_STATES, 'rb') as state_file:\n",
    "    sample_full_color_observations = pickle.load(state_file)\n",
    "    \n",
    "    \n",
    "sample_full_color_observations = sample_full_color_observations.astype(np.uint8)\n",
    "print(sample_full_color_observations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_run = api.run('augmented-frostbite/initial-experiments/runs/fdxobftk')\n",
    "baseline_model, baseline_env = load_model_from_run(baseline_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_results = pass_states_through_model(baseline_model, baseline_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "baseline_model_results.hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_jobs=4)\n",
    "Y = tsne.fit_transform(baseline_model_results.hidden_states)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_only_run = api.run('augmented-frostbite/masks-only-replication/runs/0khc2n2c')\n",
    "masks_only_model, masks_only_env = load_model_from_run(masks_only_run)\n",
    "masks_only_model_results = pass_states_through_model(masks_only_model, masks_only_env, sample_full_color_observations, skip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in DEFAULT_MASK_NAMES[1:]:\n",
    "    print(x.replace('_', ' ').title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 1230\n",
    "obs = sample_full_color_observations[frame_idx]\n",
    "state = observation_to_model(masks_only_env, sample_full_color_observations[frame_idx]).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 12.75))\n",
    "plt.subplots_adjust(hspace=0.075, wspace=0.075)\n",
    "\n",
    "pixels_ax = plt.subplot(3, 3, 1)\n",
    "pixels_ax.imshow(cv2.resize(obs, (160, 160), interpolation=cv2.INTER_LINEAR))\n",
    "pixels_ax.set_title('Pixels', fontsize=20)\n",
    "pixels_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "pixels_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "for i, name in enumerate(DEFAULT_MASK_NAMES[1:]):\n",
    "    ax = plt.subplot(3, 3, i + 2)\n",
    "    ax.imshow(state[i], cmap='Greys')\n",
    "    ax.set_title(name.replace('_', ' ').title(), fontsize=20)\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "\n",
    "save('mask_examples.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx = 1230\n",
    "obs = sample_full_color_observations[frame_idx]\n",
    "# state = observation_to_model(masks_only_env, sample_full_color_observations[frame_idx]).cpu().numpy()\n",
    "state = masks_only_env.masker(torch.tensor(sample_full_color_observations[frame_idx]).to(masks_only_env.device)).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 11))\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "pixels_ax = plt.subplot(3, 3, 1)\n",
    "# pixels_ax.imshow(cv2.resize(obs, (160, 160), interpolation=cv2.INTER_LINEAR))\n",
    "pixels_ax.imshow(obs)\n",
    "pixels_ax.set_title('Pixels', fontsize=16)\n",
    "pixels_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "pixels_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "for i, name in enumerate(DEFAULT_MASK_NAMES[1:]):\n",
    "    ax = plt.subplot(3, 3, i + 2)\n",
    "    ax.imshow(state[i], cmap='Greys')\n",
    "    ax.set_title(name.replace('_', ' ').title(), fontsize=16)\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "\n",
    "save('mask_examples_original_res.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "-----------\n",
    "\n",
    "# More organized take at this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelAugmentation = namedtuple('ChannelAugmentation', \n",
    "                                 ('channel_index', 'added_object', 'added_location'))\n",
    "\n",
    "ModelAugmentation = namedtuple('ModelAugmentation', \n",
    "                               ('name', 'model', 'env', 'pixel_augmentations', 'mask_augmentations'))\n",
    "\n",
    "\n",
    "def augment_pixels(state, pixel_augmentation):\n",
    "    dst_slices = [slice(pixel_augmentation.added_location[i], \n",
    "                        pixel_augmentation.added_location[i] + pixel_augmentation.added_object.shape[i])\n",
    "                  for i in range(len(pixel_augmentation.added_location))] \n",
    "    \n",
    "    state[dst_slices[0], dst_slices[1], :] = pixel_augmentation.added_object\n",
    "    return state\n",
    "\n",
    "\n",
    "def augment_mask_channel(state, channel_augmentation):\n",
    "    dst_slices = [slice(channel_augmentation.added_location[i], \n",
    "                        channel_augmentation.added_location[i] + channel_augmentation.added_object.shape[i])\n",
    "                  for i in range(len(channel_augmentation.added_location))] \n",
    "    state[channel_augmentation.channel_index, dst_slices[0], dst_slices[1]] = channel_augmentation.added_object\n",
    "    return state\n",
    "    \n",
    "\n",
    "def modify_observation_to_model_state(obs, model_augmentation, return_pixels=False):\n",
    "    # pixel augmentations if any exist\n",
    "    obs_tensor = model_augmentation.env._to_tensor(obs)\n",
    "    if model_augmentation.pixel_augmentations is not None:\n",
    "        for pixel_aug in model_augmentation.pixel_augmentations:\n",
    "            obs_tensor = augment_pixels(obs_tensor, pixel_aug)\n",
    "    \n",
    "    # take this tensor and drop it to grayscale and low-res\n",
    "    gray_obs_tensor = rgb_to_grayscale(obs_tensor)\n",
    "    \n",
    "    masks = None\n",
    "    \n",
    "    # create masks for this model\n",
    "    if hasattr(model_augmentation.env, 'masker'):\n",
    "        masks = model_augmentation.env.masker(model_augmentation.env._to_tensor(obs))\n",
    "    \n",
    "        # mask augmentations if any exist\n",
    "        if model_augmentation.mask_augmentations is not None:\n",
    "            for augmentation in model_augmentation.mask_augmentations:\n",
    "                masks = augment_mask_channel(masks, augmentation)\n",
    "                \n",
    "        masks = model_augmentation.env._resize(masks.unsqueeze(0))\n",
    "        \n",
    "    # TODO: delete me\n",
    "#     if masks is not None:\n",
    "#         plt.figure(figsize=(12, 12.75))\n",
    "#         plt.subplots_adjust(hspace=0.075, wspace=0.075)\n",
    "\n",
    "#         pixels_ax = plt.subplot(3, 3, 1)\n",
    "#         pixels_ax.imshow(cv2.resize(obs_tensor.cpu().numpy().astype(np.uint8), (160, 160), interpolation=cv2.INTER_LINEAR))\n",
    "#         pixels_ax.set_title('Pixels', fontsize=16)\n",
    "#         pixels_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "#         pixels_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "#         for i, name in enumerate(DEFAULT_MASK_NAMES[1:]):\n",
    "#             ax = plt.subplot(3, 3, i + 2)\n",
    "#             ax.imshow(masks[0, i].cpu().numpy(), cmap='Greys')\n",
    "#             ax.set_title(name.replace('_', ' ').title(), fontsize=16)\n",
    "#             ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "#             ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "            \n",
    "#         plt.show()\n",
    "    \n",
    "    # combine (handles resizing of the raw observation)\n",
    "    final_state = model_augmentation.env._prepare_state(gray_obs_tensor.type(torch.float32), None, masks)\n",
    "    \n",
    "    if return_pixels is True:\n",
    "        return final_state, obs_tensor.type(torch.uint8)\n",
    "#     \n",
    "    return final_state\n",
    "\n",
    "\n",
    "def augmented_state_q_values(observations, augmented_index, model_augmentations, state_length=4, \n",
    "                             return_before_and_after=False, return_variance=False):\n",
    "    if augmented_index < state_length or augmented_index >= len(observations):\n",
    "        raise ValueError(f'Augmented index should be in [{state_length}, {len(observations)}), received {augmented_index}')\n",
    "\n",
    "    before_q_values_per_model = []\n",
    "    before_q_variances_per_model = []\n",
    "    after_q_values_per_model = []\n",
    "    after_q_variances_per_model = []\n",
    "    before_pixels, after_pixels = None, None\n",
    "    \n",
    "    for model_augmentation in model_augmentations:\n",
    "        # TODO: verify I don't have an off-by-one here\n",
    "        state_buffer = [observation_to_model(model_augmentation.env, obs) \n",
    "                        for obs in observations[augmented_index - state_length + 1:augmented_index + 1]]\n",
    "        \n",
    "        if return_before_and_after and before_pixels is None:\n",
    "#             before_pixels = state_buffer[-1][0].cpu().numpy()\n",
    "            before_pixels = observations[augmented_index]\n",
    "        \n",
    "        model_ready_state = torch.cat(list(state_buffer), 0)\n",
    "        if return_variance:\n",
    "            mean, variance = model_augmentation.model.q_value_mean_variance(model_ready_state)\n",
    "            before_q_values_per_model.append(mean)\n",
    "            before_q_variances_per_model.append(variance)\n",
    "        else:\n",
    "            before_q_values_per_model.append(model_augmentation.model.expected_q_values(model_ready_state))\n",
    "\n",
    "        augmented_state = modify_observation_to_model_state(observations[augmented_index], model_augmentation)\n",
    "        if return_before_and_after and after_pixels is None:\n",
    "            augmented_state, after_pixels_tensor = modify_observation_to_model_state(observations[augmented_index], \n",
    "                                                                                     model_augmentation, return_pixels=True)\n",
    "            after_pixels = after_pixels_tensor.cpu().numpy()\n",
    "            \n",
    "        else:\n",
    "            augmented_state = modify_observation_to_model_state(observations[augmented_index], model_augmentation)\n",
    "#             after_pixels = state_buffer[-1][0].cpu().numpy()\n",
    "            \n",
    "        # TODO: consider the case of augmenting more than one consecutive states\n",
    "        state_buffer[-1] = augmented_state\n",
    "        model_ready_state = torch.cat(list(state_buffer), 0)\n",
    "        \n",
    "        if return_variance:\n",
    "            mean, variance = model_augmentation.model.q_value_mean_variance(model_ready_state)\n",
    "            after_q_values_per_model.append(mean)\n",
    "            after_q_variances_per_model.append(variance)\n",
    "        else:\n",
    "            after_q_values_per_model.append(model_augmentation.model.expected_q_values(model_ready_state))\n",
    "\n",
    "    ret_val = [before_q_values_per_model, after_q_values_per_model]\n",
    "    \n",
    "    if return_variance:\n",
    "        ret_val.extend((before_q_variances_per_model, after_q_variances_per_model))\n",
    "        \n",
    "    if return_before_and_after:\n",
    "        ret_val.extend((before_pixels, after_pixels))\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "\n",
    "def max_wrapper(tensor, axis=0):\n",
    "    val, idx = tensor.max(axis)\n",
    "    return float(val), int(idx)\n",
    "\n",
    "\n",
    "def evaluate_augmented_models(observations, augmented_index, model_augmentations, state_length=4,\n",
    "                              before_color='red', after_color='blue', bar_alpha=0.5,\n",
    "                              bar_width=0.8, fontdict=dict(fontsize=16), force_text=None, text_epsilon=0,\n",
    "                              plot_state=True):\n",
    "    if plot_state:\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(8 * len(model_augmentations) + 1, 6))\n",
    "        gs = fig.add_gridspec(2, 7)\n",
    "        before_q_values_per_model, after_q_values_per_model, before_state, after_state = augmented_state_q_values(observations, \n",
    "            augmented_index, model_augmentations, state_length=state_length, return_before_and_after=True)\n",
    "\n",
    "        before_ax = fig.add_subplot(gs[0, 0])\n",
    "        if len(before_state.shape) == 3:\n",
    "            before_ax.imshow(before_state)\n",
    "        else:\n",
    "            before_ax.imshow(before_state, cmap='gray')\n",
    "        before_ax.set_title('Before')\n",
    "        \n",
    "        after_ax = fig.add_subplot(gs[1, 0])\n",
    "        if len(after_state.shape) == 3:\n",
    "            after_ax.imshow(after_state)\n",
    "        else:\n",
    "            after_ax.imshow(after_state, cmap='gray')\n",
    "        after_ax.set_title('After')\n",
    "    \n",
    "    else:\n",
    "        plt.figure(figsize=(8 * len(model_augmentations), 6))\n",
    "        before_q_values_per_model, after_q_values_per_model = augmented_state_q_values(observations, augmented_index, \n",
    "                                                                                       model_augmentations, state_length=state_length)\n",
    "\n",
    "    \n",
    "\n",
    "    for i, (model_augmentation, before_q, after_q) in enumerate(zip(model_augmentations, \n",
    "                                                                                       before_q_values_per_model, after_q_values_per_model, \n",
    "                                                                                      )):\n",
    "        print(f'For model {model_augmentation.name}:')\n",
    "        before_mean, after_mean = before_q.mean().cpu().numpy(), after_q.mean().cpu().numpy()\n",
    "        before_v, after_v = before_q.max().cpu().numpy(), after_q.max().cpu().numpy()\n",
    "        print(f'Baseline mean: {before_mean:.3f} | Augmented mean: {after_mean:.3f} | Difference: {after_mean - before_mean:.3f}')\n",
    "        \n",
    "        diff = after_q - before_q\n",
    "        max_diff, max_diff_idx = max_wrapper(diff.abs())\n",
    "        print(f'Max Q value diff is {max_diff:.3f} for action {ALE_ACTIONS[max_diff_idx]} [{max_diff_idx}]')\n",
    "\n",
    "        if plot_state:\n",
    "            ax = fig.add_subplot(gs[:, 1 + 2 * i :1 + 2 * (i + 1)])\n",
    "        else:\n",
    "            ax = plt.subplot(1, len(model_augmentations), i + 1)\n",
    "        \n",
    "        locations = np.arange(before_q.shape[0]) * bar_width * 2.5\n",
    "        ax.bar(locations, before_q.cpu().numpy(), color=before_color, alpha=bar_alpha)\n",
    "        ax.bar(locations + bar_width, after_q.cpu().numpy(), color=after_color, alpha=bar_alpha)\n",
    "        ax.hlines([before_mean, after_mean], *ax.get_xlim(), colors=[before_color, after_color],\n",
    "                  linestyles='dashed')\n",
    "        \n",
    "        ax.hlines([before_v, after_v], *ax.get_xlim(), colors=[before_color, after_color],\n",
    "                  linestyles='dotted')\n",
    "        \n",
    "        text_fd = {k:fontdict[k] for k in fontdict}\n",
    "        ax.text(locations[max_diff_idx] + bar_width / 2, max(before_q[max_diff_idx], after_q[max_diff_idx]) + text_epsilon,\n",
    "                '*', fontdict=text_fd)\n",
    "        \n",
    "        before_action_q, before_action_idx = max_wrapper(before_q)\n",
    "        after_action_q, after_action_idx = max_wrapper(after_q)\n",
    "        \n",
    "        print(f'Before action: {ALE_ACTIONS[before_action_idx]} [{before_action_idx}] (Q = {before_action_q:.3f}) | After action: {ALE_ACTIONS[after_action_idx]} [{after_action_idx}] (Q = {after_action_q:.3f})')\n",
    "        \n",
    "        if before_action_idx != after_action_idx or (force_text is not None and force_text):\n",
    "            text_fd['color'] = before_color\n",
    "            ax.text(locations[before_action_idx] - (bar_width / 2), before_action_q + text_epsilon, 'B', fontdict=text_fd)\n",
    "\n",
    "            text_fd['color'] = after_color\n",
    "            ax.text(locations[after_action_idx] + (bar_width / 2), after_action_q + text_epsilon, 'A', fontdict=text_fd)\n",
    "        \n",
    "        ax.set_xticks(locations + (bar_width / 2))\n",
    "        ax.set_xticklabels(sorted(ALE_ACTIONS.keys()))\n",
    "        \n",
    "        ax.set_xlabel('Action Index', fontdict=fontdict)\n",
    "        ax.set_ylabel('Q Value', fontdict=fontdict)\n",
    "        ax.set_title(model_augmentation.name, fontdict=fontdict)\n",
    "        print()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_multiple_models_single_augmented_state(observations, augmented_index, model_augmentations_per_group,\n",
    "                                                    key_actions=None, key_actions_aggregator_func=lambda t: t.mean(axis=1), \n",
    "                                                    state_length=4,\n",
    "                                                    before_color='red', after_color='blue', \n",
    "                                                    bar_alpha=0.5, bar_width=0.8, \n",
    "                                                    fontdict=dict(fontsize=16), force_text=None, \n",
    "                                                    text_epsilon=0, plot_state=True, plot_variance=False,\n",
    "                                                    state_value_title='State values',\n",
    "                                                    key_action_title='Key action Q-values',\n",
    "                                                    names=None, save_name=None):\n",
    "    # TODO: if using multiple states or models, we need to average properly over the before_q / after_q tensors\n",
    "    # That is, compute the average mean difference and the average max difference\n",
    "    # The per-action bar charts might make less sense?\n",
    "    # If we do this, we can plot a histogram of MMeanD and MMaxD and over the different random seeds (for one state)\n",
    "    # Or the different states for one (or more) random seeds\n",
    "    \n",
    "    plot_key_actions = key_actions is not None\n",
    "    num_panels = 1 + int(plot_state) + int(plot_key_actions)\n",
    "    constrained_layout = plot_state\n",
    "    fig = plt.figure(constrained_layout=constrained_layout, figsize=(8 * num_panels, 6))\n",
    "    \n",
    "    if plot_state:\n",
    "        gs = fig.add_gridspec(1, num_panels * 2)\n",
    "        _, _, before_state, after_state = augmented_state_q_values(observations, \n",
    "            augmented_index, [model_augmentations_per_group[0][0]], state_length=state_length, return_before_and_after=True)\n",
    "\n",
    "        before_ax = fig.add_subplot(gs[0, 0])\n",
    "        if len(before_state.shape) == 3:\n",
    "            before_ax.imshow(before_state)\n",
    "        else:\n",
    "            before_ax.imshow(before_state, cmap='gray')\n",
    "        before_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        before_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        before_ax.set_title('Before', fontdict=fontdict)\n",
    "        \n",
    "        after_ax = fig.add_subplot(gs[0, 1])\n",
    "        if len(after_state.shape) == 3:\n",
    "            after_ax.imshow(after_state)\n",
    "        else:\n",
    "            after_ax.imshow(after_state, cmap='gray')\n",
    "        after_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        after_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        after_ax.set_title('After', fontdict=fontdict)\n",
    "    \n",
    "    if plot_state:\n",
    "        state_value_ax = fig.add_subplot(gs[0, 2:4])\n",
    "        if plot_key_actions:\n",
    "            key_actions_ax = fig.add_subplot(gs[0, 4:])\n",
    "    else:\n",
    "        state_value_ax = plt.subplot(1, num_panels, 1)\n",
    "        if plot_key_actions:\n",
    "            key_actions_ax = plt.subplot(1, 2, 2)\n",
    "    \n",
    "    all_state_values_before = []\n",
    "    all_state_values_after = []\n",
    "    \n",
    "    if plot_variance:\n",
    "        all_state_variances_before = []\n",
    "        all_state_variances_after = []\n",
    "    \n",
    "    if plot_key_actions:\n",
    "        all_key_action_values_before = []\n",
    "        all_key_action_values_after = []\n",
    "        \n",
    "        all_key_action_variances_before = []\n",
    "        all_key_action_variances_after = []\n",
    "        \n",
    "    if names is None:\n",
    "        names = [aug_group[0].name for aug_group in model_augmentations_per_group]\n",
    "    \n",
    "    for model_augmentations in model_augmentations_per_group:\n",
    "        if plot_variance:\n",
    "            q_values_before, q_values_after, q_variances_before, q_variances_after = augmented_state_q_values(observations, augmented_index, model_augmentations, \n",
    "                                                                                                              state_length=state_length, return_variance=plot_variance)\n",
    "        else:\n",
    "            q_values_before, q_values_after = augmented_state_q_values(observations, augmented_index, model_augmentations, \n",
    "                                                                                                              state_length=state_length, return_variance=plot_variance)\n",
    "        q_values_before = torch.stack(q_values_before)\n",
    "        q_values_after = torch.stack(q_values_after)\n",
    "        \n",
    "        if plot_variance:\n",
    "            q_variances_before = torch.stack(q_variances_before)\n",
    "            q_variances_after = torch.stack(q_variances_after)\n",
    "        \n",
    "        state_values_before, before_max_indices = q_values_before.max(1)\n",
    "        state_values_after, after_max_indices = q_values_after.max(1)\n",
    "        \n",
    "        all_state_values_before.append(state_values_before.cpu().numpy())\n",
    "        all_state_values_after.append(state_values_after.cpu().numpy())\n",
    "        \n",
    "        if plot_variance:\n",
    "            all_state_variances_before.append(q_variances_before.gather(1, before_max_indices.unsqueeze(1)).squeeze().cpu().numpy())\n",
    "            all_state_variances_after.append(q_variances_after.gather(1, after_max_indices.unsqueeze(1)).squeeze().cpu().numpy())\n",
    "        \n",
    "        if plot_key_actions:\n",
    "            all_key_action_values_before.append(key_actions_aggregator_func(q_values_before[:, key_actions]).cpu().numpy())\n",
    "            all_key_action_values_after.append(key_actions_aggregator_func(q_values_after[:, key_actions]).cpu().numpy())\n",
    "            \n",
    "            all_key_action_variances_before.append(key_actions_aggregator_func(q_variances_before[:, key_actions]).cpu().numpy())\n",
    "            all_key_action_variances_after.append(key_actions_aggregator_func(q_variances_after[:, key_actions]).cpu().numpy())\n",
    "            \n",
    "    for name, before, after in zip(names, all_state_values_before, all_state_values_after):\n",
    "        print(f'For model {name} | Before mean = {np.mean(before):.3f}, median = {np.median(before):.3f} | After mean = {np.mean(after):.3f}, median = {np.median(after):.3f} ')\n",
    "        \n",
    "    before_plot_values = [all_state_values_before, ] \n",
    "    after_plot_values = [all_state_values_after, ] \n",
    "    \n",
    "    if plot_variance:\n",
    "        before_plot_values.append(all_state_variances_before)\n",
    "        after_plot_values.append(all_state_variances_after)\n",
    "        \n",
    "    create_box_plot_set(state_value_ax, before_plot_values, after_plot_values, names,\n",
    "                        before_color, after_color, 'Model Type', 'V(s)', state_value_title, \n",
    "                        fontdict, (None, dict(linestyle='dashed')))\n",
    "    \n",
    "    if plot_key_actions:\n",
    "        create_box_plot_set(key_actions_ax, \n",
    "                            [all_key_action_values_before, all_key_action_variances_before], \n",
    "                            [all_key_action_values_after, all_key_action_variances_after], names,\n",
    "                            before_color, after_color, 'Model Type', 'Q(s, a)', key_action_title, \n",
    "                            fontdict, (None, dict(linestyle='dashed')))\n",
    "        \n",
    "    if save_name is not None:\n",
    "        save(save_name)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def create_box_plot_set(ax, value_sets_before, value_sets_after, names, before_color, after_color,\n",
    "                        x_label, y_label, title, fontdict, additional_boxplot_properties=None):\n",
    "    \n",
    "    if additional_boxplot_properties is None:\n",
    "        additional_boxplot_properties = [None] * len(value_sets_before)\n",
    "    \n",
    "    before_positions = np.arange(len(names)) * 2 * len(value_sets_before)\n",
    "    after_positions = before_positions + len(value_sets_before)\n",
    "    \n",
    "    for i, (values_before, values_after, boxplot_props) in enumerate(zip(value_sets_before, \n",
    "                                                                         value_sets_after, \n",
    "                                                                         additional_boxplot_properties)):\n",
    "        if boxplot_props is None:\n",
    "            boxplot_props = {}\n",
    "    \n",
    "        ax.boxplot(np.array(values_before).T, positions=before_positions + i, \n",
    "                   boxprops=dict(color=before_color, **boxplot_props), \n",
    "                   whiskerprops=dict(color=before_color, **boxplot_props),\n",
    "                   capprops=dict(color=before_color, **boxplot_props),\n",
    "                   medianprops=boxplot_props)\n",
    "        ax.boxplot(np.array(values_after).T, positions=after_positions + i, \n",
    "                   boxprops=dict(color=after_color, **boxplot_props), \n",
    "                   whiskerprops=dict(color=after_color, **boxplot_props),\n",
    "                   capprops=dict(color=after_color, **boxplot_props), \n",
    "                   medianprops=boxplot_props)\n",
    "\n",
    "    dashed_line_positions = (before_positions - .5)[1:]\n",
    "    ax.vlines(dashed_line_positions, *ax.get_ylim(), colors='gray', linestyles='dashed')\n",
    "\n",
    "    xtick_positions = np.concatenate([before_positions, after_positions]) + 0.5 * (len(value_sets_before) - 1)\n",
    "    xtick_labels = [f'{name}\\nBefore' for name in names] + [f'{name}\\nAfter' for name in names]\n",
    "    ax.set_xticks(xtick_positions)\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "    ax.set_xlabel(x_label, fontdict=fontdict)\n",
    "    ax.set_ylabel(y_label, fontdict=fontdict)\n",
    "    ax.set_title(title, fontdict=fontdict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_models_single_augmented_state_single_plot(observations, augmented_index, model_augmentations_per_group,\n",
    "                                                       colors, after_ax, values_ax, after_value_positions, median_color,\n",
    "                                                       before_ax=None, before_value_positions=None, additional_boxplot_properties=None,\n",
    "                                                       state_length=4, before_title='Before', after_title='After', names=None,\n",
    "                                                       major_fontdict=dict(fontsize=24), minor_fontdict=dict(fontsize=16)):\n",
    "    \n",
    "    if before_ax is not None and before_value_positions is None:\n",
    "        raise ValueError('Must supply `before_value_positions` when supplying `before_ax`')\n",
    "    \n",
    "    _, _, before_state, after_state = augmented_state_q_values(observations, \n",
    "        augmented_index, [model_augmentations_per_group[0][0]], state_length=state_length, return_before_and_after=True)\n",
    "\n",
    "    if before_ax is not None:\n",
    "        if len(before_state.shape) == 3:\n",
    "            before_ax.imshow(before_state)\n",
    "        else:\n",
    "            before_ax.imshow(before_state, cmap='gray')\n",
    "        before_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        before_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        before_ax.set_title(before_title, fontdict=major_fontdict)\n",
    "        \n",
    "    if len(after_state.shape) == 3:\n",
    "        after_ax.imshow(after_state)\n",
    "    else:\n",
    "        after_ax.imshow(after_state, cmap='gray')\n",
    "    after_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    after_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    after_ax.set_title(after_title, fontdict=major_fontdict)\n",
    "    \n",
    "    all_state_values_before = []\n",
    "    all_state_values_after = []\n",
    "        \n",
    "    \n",
    "    for model_augmentations in model_augmentations_per_group:\n",
    "        q_values_before, q_values_after = augmented_state_q_values(observations, augmented_index, model_augmentations, \n",
    "                                                                   state_length=state_length, return_variance=False)\n",
    "        q_values_before = torch.stack(q_values_before)\n",
    "        q_values_after = torch.stack(q_values_after)\n",
    "        \n",
    "        state_values_before, before_max_indices = q_values_before.max(1)\n",
    "        state_values_after, after_max_indices = q_values_after.max(1)\n",
    "        \n",
    "        all_state_values_before.append(state_values_before.cpu().numpy())\n",
    "        all_state_values_after.append(state_values_after.cpu().numpy())\n",
    "        \n",
    "    if names is None:\n",
    "        names = [aug_group[0].name for aug_group in model_augmentations_per_group]\n",
    "      \n",
    "    for name, before, after in zip(names, all_state_values_before, all_state_values_after):\n",
    "        print(f'For model {name} | Before mean = {np.mean(before):.3f}, median = {np.median(before):.3f} | After mean = {np.mean(after):.3f}, median = {np.median(after):.3f} ')\n",
    "        \n",
    "    \n",
    "    if before_value_positions is not None:\n",
    "        add_boxplots_to_ax(values_ax, all_state_values_before, before_value_positions,\n",
    "                           names, colors, minor_fontdict, median_color, additional_boxplot_properties)\n",
    "        \n",
    "    add_boxplots_to_ax(values_ax, all_state_values_after, after_value_positions,\n",
    "                       names, colors, minor_fontdict, median_color, additional_boxplot_properties)\n",
    "\n",
    "    \n",
    "\n",
    "def add_boxplots_to_ax(ax, value_sets, positions, names, colors, fontdict, median_color,\n",
    "                       additional_boxplot_properties=None):\n",
    "    \n",
    "    if additional_boxplot_properties is None:\n",
    "        additional_boxplot_properties = [dict(lw=2)] * len(positions)\n",
    "    \n",
    "    for i, (values, position, name, color, boxplot_props) in enumerate(zip(value_sets, positions, names, colors, additional_boxplot_properties)):\n",
    "\n",
    "        ax.boxplot(np.array(values).T, positions=[position], widths=0.5,\n",
    "                   boxprops=dict(color=color, **boxplot_props), \n",
    "                   whiskerprops=dict(color=color, **boxplot_props),\n",
    "                   capprops=dict(color=color, **boxplot_props),\n",
    "                   medianprops=dict(color=median_color, **boxplot_props))\n",
    "\n",
    "#     dashed_line_positions = (before_positions - .5)[1:]\n",
    "#     ax.vlines(dashed_line_positions, *ax.get_ylim(), colors='gray', linestyles='dashed')\n",
    "\n",
    "    ax.set_xticks(np.concatenate((ax.get_xticks(), positions)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels() + list(names), **fontdict)\n",
    "\n",
    "#     ax.set_xlabel(x_label, fontdict=fontdict)\n",
    "#     ax.set_ylabel(y_label, fontdict=fontdict)\n",
    "#     ax.set_title(title, fontdict=fontdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGONAL_CONNECTIVITY_STRUCTURE = ndimage.generate_binary_structure(2, 2)\n",
    "\n",
    "\n",
    "def to_tensor(numpy_arr, model):\n",
    "    return torch.Tensor(numpy_arr).to(model.online_net.convs[0].weight.device)\n",
    "\n",
    "\n",
    "def extract_raw_pixels_object(observations, obs_index, model, env, loc):\n",
    "    obs = observations[obs_index]\n",
    "    return env._to_tensor(obs[loc[0], loc[1], :])\n",
    "#     model_ready_obs = to_tensor(observation_to_model(env, observations[obs_index]).cpu().numpy(), model)\n",
    "#     return model_ready_obs.squeeze()[loc]\n",
    "\n",
    "\n",
    "def extract_object(observations, obs_index, model, env, channel_index, \n",
    "                   object_index=0, structure=DIAGONAL_CONNECTIVITY_STRUCTURE,\n",
    "                   return_tensor=True, return_location=False):\n",
    "#     state = observation_to_model(env, observations[obs_index]).cpu().numpy()\n",
    "    state = env.masker(env._to_tensor(observations[obs_index])).cpu().numpy()\n",
    "    labeled, count = ndimage.label(state[channel_index], structure)\n",
    "    locations = ndimage.find_objects(labeled, 0)\n",
    "    loc = locations[object_index]\n",
    "    \n",
    "    object_arr = state[channel_index, loc[0], loc[1]]\n",
    "    if return_tensor:\n",
    "        object_arr = to_tensor(object_arr, model)\n",
    "        \n",
    "    if not return_location:\n",
    "        return object_arr\n",
    "    \n",
    "    return object_arr, loc\n",
    "    \n",
    "\n",
    "def copy_model_augmentation(model_augmentation, name=None, model=None, env=None, \n",
    "                            pixel_augmentations=None, mask_augmentations=None):\n",
    "    if name is None:\n",
    "        name = model_augmentation.name\n",
    "    \n",
    "    if model is None:\n",
    "        model = model_augmentation.model\n",
    "        \n",
    "    if env is None:\n",
    "        env = model_augmentation.env\n",
    "    \n",
    "    if pixel_augmentations is None:\n",
    "        pixel_augmentations = model_augmentation.pixel_augmentations\n",
    "        \n",
    "    if mask_augmentations is None:\n",
    "        mask_augmentations = model_augmentation.mask_augmentations\n",
    "        \n",
    "    return ModelAugmentation(name, model, env, pixel_augmentations, mask_augmentations)\n",
    "\n",
    "\n",
    "DEFAULT_GROUPED_OBJECTS_INDICES = (0, 1, 3, 4, 5, 6)\n",
    "\n",
    "\n",
    "def make_augmentations_all_models(b_aug, m_p_aug, m_o_aug, g_o_aug, pixels_tensor, mask_tensor, mask_channel_index, locations,\n",
    "                                  g_o_indices=DEFAULT_GROUPED_OBJECTS_INDICES):\n",
    "    pixel_augs = [ChannelAugmentation(0, pixels_tensor, loc) for loc in locations]\n",
    "    mask_augs = [ChannelAugmentation(mask_channel_index, mask_tensor, loc) for loc in locations]\n",
    "    grouped_mask_augs = None\n",
    "    if mask_channel_index in g_o_indices:\n",
    "        grouped_mask_augs = [ChannelAugmentation(0, mask_tensor, loc) for loc in locations]\n",
    "    \n",
    "    return copy_model_augmentation(b_aug, pixel_augmentations=pixel_augs),\\\n",
    "        copy_model_augmentation(m_p_aug, pixel_augmentations=pixel_augs, mask_augmentations=mask_augs),\\\n",
    "        copy_model_augmentation(m_o_aug, mask_augmentations=mask_augs),\\\n",
    "        copy_model_augmentation(g_o_aug, pixel_augmentations=pixel_augs, mask_augmentations=grouped_mask_augs)\n",
    "\n",
    "\n",
    "def plot_tensors(*tensors, norm=False):\n",
    "    normalizer = None\n",
    "    if norm:\n",
    "        normalizer = matplotlib.colors.Normalize(0, 1)\n",
    "    \n",
    "    n = len(tensors)\n",
    "    plt.figure(figsize=(4 * n, 4))\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        ax.imshow(tensors[i].cpu().numpy(), cmap='gray', norm=normalizer)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_observations(observations, start, count, step):\n",
    "    plt.figure(figsize=(4 * count, 4))\n",
    "    for i, idx in enumerate(range(start, start + count * step, step)):\n",
    "        ax = plt.subplot(1, count, i + 1)\n",
    "        ax.imshow(observations[idx])\n",
    "        ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.set_title(idx)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_observations_by_indices(observations, indices):\n",
    "    n = len(indices)\n",
    "    plt.figure(figsize=(4 * n, 4))\n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        ax.imshow(observations[idx])\n",
    "        ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.set_title(idx)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def change_intensity(tensor, additive=0, multiplicative=1, min_val=0, max_val=1):\n",
    "    return torch.clamp(tensor * multiplicative + additive, min_val, max_val)\n",
    "    \n",
    "\n",
    "DEFAULT_MASK_NAMES = (\n",
    "    'None',\n",
    "    'player',\n",
    "    'bad_animal',\n",
    "    'land',\n",
    "    'bear',\n",
    "    'unvisited_floes',\n",
    "    'visited_floes',\n",
    "    'good_animal',\n",
    "    'igloo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5671, 210, 160, 3)\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-306-29150000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-306-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-306-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-305-10000000.pth\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SAVED_STATES = r'/home/gd1279/scratch/rainbow-evaluation-state-traces/baseline-rainbow-305/evaluation/states/eval-baseline-rainbow-305-34350000-0-env.pickle'\n",
    "\n",
    "with open(SAMPLE_SAVED_STATES, 'rb') as state_file:\n",
    "    sample_full_color_observations = pickle.load(state_file)\n",
    "    \n",
    "    \n",
    "sample_full_color_observations = sample_full_color_observations.astype(np.uint8)\n",
    "print(sample_full_color_observations.shape)\n",
    "\n",
    "\n",
    "baseline_run = api.run('augmented-frostbite/initial-experiments/runs/fdxobftk')\n",
    "baseline_model, baseline_env = load_model_from_run(baseline_run)\n",
    "# baseline_model_results = pass_states_through_model(baseline_model, baseline_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "masks_and_pixels_run = api.run('augmented-frostbite/masks-and-pixels-replication/runs/grh1bzvv')\n",
    "masks_and_pixels_model, masks_and_pixels_env = load_model_from_run(masks_and_pixels_run)\n",
    "# masks_and_pixels_model_results = pass_states_through_model(masks_and_pixels_model, masks_and_pixels_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "masks_only_run = api.run('augmented-frostbite/masks-only-replication/runs/0khc2n2c')\n",
    "masks_only_model, masks_only_env = load_model_from_run(masks_only_run)\n",
    "# masks_only_model_results = pass_states_through_model(masks_only_model, masks_only_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "grouped_objects_run = api.run('augmented-frostbite/grouped-masks-moving-objects/runs/3hjg1r97')\n",
    "grouped_objects_model, grouped_objects_env = load_model_from_run(grouped_objects_run)\n",
    "\n",
    "baseline_aug_template = ModelAugmentation('Baseline', baseline_model, baseline_env, list(), list())\n",
    "masks_and_pixels_aug_template = ModelAugmentation('Masks+Pixels', masks_and_pixels_model, masks_and_pixels_env, list(), list())\n",
    "masks_only_aug_template = ModelAugmentation('Masks-Only', masks_only_model, masks_only_env, list(), list())\n",
    "grouped_objects_aug_template = ModelAugmentation('Grouped-Objects', grouped_objects_model, grouped_objects_env, list(), list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-300-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-309-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-306-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-308-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-304-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-305-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-307-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-303-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-301-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-302-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-309-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-308-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-307-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-306-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-304-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-305-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-302-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-303-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-301-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-300-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-302-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-308-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-301-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-305-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-304-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-303-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-300-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-307-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-309-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-306-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-300-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-301-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-304-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-309-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-307-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-302-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-303-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-306-10000000.pth\n",
      "Loading pretrained model: /scratch/gd1279/grouped-masks-moving-objects-305-10000000.pth\n",
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "def load_all_models(*run_urls, run_checker=lambda t: True, step=None):\n",
    "    runs = [run for run in api.runs(run_urls[0]) if run_checker(run)]\n",
    "    for url in run_urls[1:]:\n",
    "        runs.extend([run for run in api.runs(url) if run_checker(run)])\n",
    "    \n",
    "    loaded_models_and_envs = filter(lambda x: x is not None, [load_model_from_run(run, step=step) for run in runs])\n",
    "    return zip(*loaded_models_and_envs)\n",
    "\n",
    "\n",
    "def make_augmentation_per_model(base_augmentation, models):\n",
    "    return [copy_model_augmentation(base_augmentation, model=model) for model in models]\n",
    "\n",
    "\n",
    "all_baseline_models, all_baseline_envs = load_all_models('augmented-frostbite/initial-experiments/runs', \n",
    "                                                         run_checker=lambda run: run.name.lower().startswith('baseline-rainbow-3'),\n",
    "                                                         step=10000000)\n",
    "\n",
    "all_masks_and_pixels_models, all_masks_and_pixels_envs = load_all_models(#'augmented-frostbite/masks-and-pixels-fixed-resume/runs',\n",
    "                                                                         'augmented-frostbite/masks-and-pixels-replication/runs',\n",
    "                                                                         step=10000000)\n",
    "\n",
    "all_masks_only_models, all_masks_only_envs = load_all_models(#'augmented-frostbite/masks-only/runs',\n",
    "                                                             'augmented-frostbite/masks-only-replication/runs',\n",
    "                                                             step=10000000)\n",
    "\n",
    "all_grouped_objects_models, all_grouped_objects_envs = load_all_models(#'augmented-frostbite/masks-only/runs',\n",
    "                                                             'augmented-frostbite/grouped-masks-moving-objects/runs',\n",
    "                                                             step=10000000)\n",
    "\n",
    "print(len(all_baseline_models), len(all_masks_and_pixels_models), len(all_masks_only_models), len(all_grouped_objects_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 9\n"
     ]
    }
   ],
   "source": [
    "print(len(all_baseline_models), len(all_masks_and_pixels_models), len(all_masks_only_models), len(all_grouped_objects_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent\n",
      "bad_animal\n",
      "land\n",
      "bear\n",
      "unvisited_floe\n",
      "visited_floe\n",
      "good_animal\n",
      "igloo\n"
     ]
    }
   ],
   "source": [
    "for md in all_masks_and_pixels_envs[0].masker.masker_definitions:\n",
    "    for key in ALL_MASKERS:\n",
    "        if ALL_MASKERS[key] == md:\n",
    "            print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focusing on the multiple-models ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_to_hsv = np.vectorize(colorsys.rgb_to_hsv)\n",
    "hsv_to_rgb = np.vectorize(colorsys.hsv_to_rgb)\n",
    "\n",
    "\n",
    "def shift_hue(arr, hue=None, sat=None, val=None):\n",
    "    r, g, b = np.rollaxis(arr, axis=-1)\n",
    "    h, s, v = rgb_to_hsv(r, g, b)\n",
    "    if hue is not None:\n",
    "        h = hue\n",
    "    if sat is not None:\n",
    "        s = sat\n",
    "    if val is not None:\n",
    "        v = val\n",
    "    r, g, b = hsv_to_rgb(h, s, v)\n",
    "    arr = np.dstack((r, g, b))\n",
    "    return arr.astype(np.uint8)\n",
    "\n",
    "\n",
    "def shift_tensor_hue(input_tensor, hue=None, sat=None, val=None):\n",
    "    shifted_np_array = shift_hue(input_tensor.type(torch.uint8).cpu().numpy(), hue, sat, val)\n",
    "    return torch.tensor(shifted_np_array, dtype=torch.float32).to(input_tensor.device)\n",
    "\n",
    "\n",
    "def do_multiple_augmentation_comparison(src_index, dst_index, channel_index, new_locations, hsv=None, object_index=0,\n",
    "                                        object_pixels_and_mask=None,\n",
    "                                        key_actions=None, plot_variance=False, save_name=None):\n",
    "    \n",
    "    if object_pixels_and_mask is not None:\n",
    "        if new_locations is None:\n",
    "            raise ValueError('Cannot provide `object_pixels_and_mask` without providing `new_locations`')\n",
    "            \n",
    "        pixels_tensor, mask_tensor = object_pixels_and_mask\n",
    "    else:\n",
    "        mask_tensor, object_loc = extract_object(sample_full_color_observations, src_index, masks_only_model, masks_only_env,\n",
    "                                          channel_index=channel_index, object_index=object_index, return_location=True)\n",
    "        pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, src_index, baseline_model, baseline_env, object_loc)\n",
    "    \n",
    "    if hsv is not None:\n",
    "        pixels_tensor = shift_tensor_hue(pixels_tensor, *hsv)\n",
    "    \n",
    "    if new_locations is None:\n",
    "        new_locations = [(object_loc[0].start, object_loc[1].start)]\n",
    "    \n",
    "    if callable(new_locations):\n",
    "        new_locations = new_locations(object_loc)\n",
    "    \n",
    "    baseline_aug, masks_and_pixels_aug, masks_only_aug, grouped_objects_aug =\\\n",
    "        make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                      pixels_tensor, mask_tensor, channel_index, new_locations)\n",
    "\n",
    "    all_baseline_augs = make_augmentation_per_model(baseline_aug, all_baseline_models)\n",
    "    all_masks_and_pixels_augs = make_augmentation_per_model(masks_and_pixels_aug, all_masks_and_pixels_models)\n",
    "    all_masks_only_augs = make_augmentation_per_model(masks_only_aug, all_masks_only_models)\n",
    "    all_grouped_objects_augs = make_augmentation_per_model(grouped_objects_aug, all_grouped_objects_models)\n",
    "\n",
    "    evaluate_multiple_models_single_augmented_state(sample_full_color_observations, dst_index, \n",
    "                                                    (all_baseline_augs, all_masks_and_pixels_augs, all_masks_only_augs, all_grouped_objects_augs),\n",
    "                                                    key_actions=key_actions, plot_variance=plot_variance,\n",
    "                                                    names=('Pixels', 'Pixels+Objects', 'Objects', 'Grouped Objects'),\n",
    "                                                    save_name=save_name)\n",
    "\n",
    "    \n",
    "FINAL_NAMES = ('Pixels', 'Pixels+Objects', 'Objects', 'Grouped Objects')    \n",
    "FINAL_NAMES_FOR_TICKS = ('Pixels', 'Pixels+\\nObjects', 'Objects', 'Grouped\\nObjects')   \n",
    "\n",
    "DEFAULT_CM = plt.get_cmap('Dark2')\n",
    "DEFAULT_COLORS = [DEFAULT_CM(i) for i in range(len(FINAL_NAMES))]\n",
    "DEFAULT_MEDIAN_COLOR = DEFAULT_CM(len(FINAL_NAMES) + 1)\n",
    "    \n",
    "def do_multiple_augmentation_comparison_single_plot(src_index, dst_index, channel_index, new_locations, \n",
    "                                                    after_ax, values_ax, after_value_positions,\n",
    "                                                    before_ax=None, before_value_positions=None, additional_boxplot_properties=None,\n",
    "                                                    colors=DEFAULT_COLORS, median_color=DEFAULT_MEDIAN_COLOR, before_title='Before', after_title='After',\n",
    "                                                    major_fontdict=dict(fontsize=24),\n",
    "                                                    minor_fontdict=dict(fontsize=16),\n",
    "                                                    names=FINAL_NAMES,\n",
    "                                                    hsv=None, object_index=0, object_pixels_and_mask=None\n",
    "                                                    ):\n",
    "    \n",
    "    if object_pixels_and_mask is not None:\n",
    "        if new_locations is None:\n",
    "            raise ValueError('Cannot provide `object_pixels_and_mask` without providing `new_locations`')\n",
    "            \n",
    "        pixels_tensor, mask_tensor = object_pixels_and_mask\n",
    "    else:\n",
    "        mask_tensor, object_loc = extract_object(sample_full_color_observations, src_index, masks_only_model, masks_only_env,\n",
    "                                          channel_index=channel_index, object_index=object_index, return_location=True)\n",
    "        pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, src_index, baseline_model, baseline_env, object_loc)\n",
    "    \n",
    "    if hsv is not None:\n",
    "        pixels_tensor = shift_tensor_hue(pixels_tensor, *hsv)\n",
    "    \n",
    "    if new_locations is None:\n",
    "        new_locations = [(object_loc[0].start, object_loc[1].start)]\n",
    "    \n",
    "    if callable(new_locations):\n",
    "        new_locations = new_locations(object_loc)\n",
    "    \n",
    "    baseline_aug, masks_and_pixels_aug, masks_only_aug, grouped_objects_aug  =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, \n",
    "                                  masks_only_aug_template, grouped_objects_aug_template,\n",
    "                                  pixels_tensor, mask_tensor, channel_index, new_locations)\n",
    "\n",
    "    all_baseline_augs = make_augmentation_per_model(baseline_aug, all_baseline_models)\n",
    "    all_masks_and_pixels_augs = make_augmentation_per_model(masks_and_pixels_aug, all_masks_and_pixels_models)\n",
    "    all_masks_only_augs = make_augmentation_per_model(masks_only_aug, all_masks_only_models)\n",
    "    all_grouped_objects_augs = make_augmentation_per_model(grouped_objects_aug, all_grouped_objects_models)\n",
    "    \n",
    "    multiple_models_single_augmented_state_single_plot(sample_full_color_observations, dst_index, \n",
    "                                                       (all_baseline_augs, all_masks_and_pixels_augs, all_masks_only_augs, all_grouped_objects_augs),\n",
    "                                                       colors, after_ax, values_ax, after_value_positions, median_color,\n",
    "                                                       before_ax=before_ax, before_value_positions=before_value_positions, \n",
    "                                                       additional_boxplot_properties=additional_boxplot_properties,\n",
    "                                                       state_length=4, before_title=before_title, after_title=after_title, names=names,\n",
    "                                                       major_fontdict=major_fontdict,\n",
    "                                                       minor_fontdict=minor_fontdict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating these plots in hopefully thier final incarnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERESTING_DST_INDICES = (94, 1112)\n",
    "NUM_COMPARISON_MODELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.838, median = 6.584 | After mean = 5.996, median = 6.573 \n",
      "For model Pixels+Objects | Before mean = 6.704, median = 6.847 | After mean = 5.249, median = 4.553 \n",
      "For model Objects | Before mean = 6.426, median = 6.502 | After mean = 4.436, median = 4.086 \n",
      "For model Grouped Objects | Before mean = 6.303, median = 5.130 | After mean = 5.307, median = 4.609 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.838, median = 6.584 | After mean = 5.425, median = 6.141 \n",
      "For model Pixels+Objects | Before mean = 6.704, median = 6.847 | After mean = 4.886, median = 4.609 \n",
      "For model Objects | Before mean = 6.426, median = 6.502 | After mean = 4.735, median = 4.610 \n",
      "For model Grouped Objects | Before mean = 6.303, median = 5.130 | After mean = 5.782, median = 5.516 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.838, median = 6.584 | After mean = 5.297, median = 5.133 \n",
      "For model Pixels+Objects | Before mean = 6.704, median = 6.847 | After mean = 5.595, median = 5.753 \n",
      "For model Objects | Before mean = 6.426, median = 6.502 | After mean = 5.385, median = 4.806 \n",
      "For model Grouped Objects | Before mean = 6.303, median = 5.130 | After mean = 5.294, median = 5.073 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "CHANNEL_INDEX = 1\n",
    "NEW_LOCATIONS = None\n",
    "START_LOC = (107, 12)\n",
    "NUM_AFTER_STATES = 3\n",
    "\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(9, 6))\n",
    "gs = fig.add_gridspec(2, 1 + NUM_AFTER_STATES)\n",
    "before_ax = fig.add_subplot(gs[0, 0])\n",
    "after_axes = [fig.add_subplot(gs[0, i]) for i in range(1, 1 + NUM_AFTER_STATES)]\n",
    "    \n",
    "values_ax = fig.add_subplot(gs[1, :])\n",
    "before_positions = np.arange(NUM_COMPARISON_MODELS)\n",
    "after_positions = [np.arange(NUM_COMPARISON_MODELS) + (NUM_COMPARISON_MODELS * i) for i in range(1, 1 + NUM_AFTER_STATES)]\n",
    "\n",
    "after_titles = ['First Row', 'Second Row', 'Third Row']\n",
    "\n",
    "major_fontdict = dict(fontsize=20)\n",
    "minor_fontdict = dict(fontsize=14)\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[0],):\n",
    "    for i, row_loc_inc in enumerate((0, 25, 55)):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        \n",
    "        b_ax, b_value_positions = None, None\n",
    "        if i == 0:\n",
    "            b_ax = before_ax \n",
    "            b_value_positions = before_positions\n",
    "        \n",
    "        do_multiple_augmentation_comparison_single_plot(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS, \n",
    "                                                    after_ax=after_axes[i], values_ax=values_ax, after_value_positions=after_positions[i],\n",
    "                                                    before_ax=b_ax, before_value_positions=b_value_positions, additional_boxplot_properties=None,\n",
    "                                                    before_title='Original State', after_title=after_titles[i],\n",
    "                                                    major_fontdict=major_fontdict,\n",
    "                                                    minor_fontdict=minor_fontdict,\n",
    "                                                    hsv=None, object_index=0, object_pixels_and_mask=None\n",
    "                                                    )\n",
    "        \n",
    "\n",
    "dashed_line_positions = np.arange(1, NUM_AFTER_STATES + 1) * NUM_COMPARISON_MODELS - 0.5\n",
    "values_ax.vlines(dashed_line_positions, *values_ax.get_ylim(), colors='gray', linestyles='dashed')\n",
    "        \n",
    "values_ax.set_xticks([])\n",
    "# values_ax.set_xticks(np.concatenate([before_positions] + after_positions))\n",
    "# values_ax.set_xticklabels(list(FINAL_NAMES_FOR_TICKS) * 4, fontsize=12, rotation=0)\n",
    "\n",
    "values_ax.set_yticklabels(values_ax.get_yticks(), fontsize=12)\n",
    "values_ax.set_ylabel('V(s)', **major_fontdict)\n",
    "\n",
    "values_ax.set_xlabel('Condition', **major_fontdict)\n",
    "\n",
    "legend_handles = []\n",
    "for color, name in zip(DEFAULT_COLORS, FINAL_NAMES):\n",
    "    legend_handles.append(patches.Patch(color=color, label=name))\n",
    "    \n",
    "plt.legend(handles=legend_handles, loc='best', **minor_fontdict)\n",
    "\n",
    "\n",
    "# save('crabs_panel.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 3.681, median = 3.915 | After mean = 3.885, median = 4.320 \n",
      "For model Pixels+Objects | Before mean = 4.193, median = 4.247 | After mean = 5.355, median = 5.060 \n",
      "For model Objects | Before mean = 4.896, median = 4.239 | After mean = 5.499, median = 5.246 \n",
      "For model Grouped Objects | Before mean = 3.870, median = 3.614 | After mean = 3.184, median = 2.865 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 3.681, median = 3.915 | After mean = 4.017, median = 4.586 \n",
      "For model Pixels+Objects | Before mean = 4.193, median = 4.247 | After mean = 5.383, median = 5.361 \n",
      "For model Objects | Before mean = 4.896, median = 4.239 | After mean = 5.499, median = 5.246 \n",
      "For model Grouped Objects | Before mean = 3.870, median = 3.614 | After mean = 2.943, median = 2.999 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 3.681, median = 3.915 | After mean = 3.754, median = 3.939 \n",
      "For model Pixels+Objects | Before mean = 4.193, median = 4.247 | After mean = 5.258, median = 4.893 \n",
      "For model Objects | Before mean = 4.896, median = 4.239 | After mean = 5.499, median = 5.246 \n",
      "For model Grouped Objects | Before mean = 3.870, median = 3.614 | After mean = 3.436, median = 3.524 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "CHANNEL_INDEX = 6\n",
    "NUM_AFTER_STATES = 3\n",
    "START_LOC = (107, 12)\n",
    "row_loc_inc = 0\n",
    "\n",
    "colors = ('green', 'blue', 'red')\n",
    "\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(9, 6))\n",
    "gs = fig.add_gridspec(2, 1 + NUM_AFTER_STATES)\n",
    "before_ax = fig.add_subplot(gs[0, 0])\n",
    "after_axes = [fig.add_subplot(gs[0, i]) for i in range(1, 1 + NUM_AFTER_STATES)]\n",
    "    \n",
    "values_ax = fig.add_subplot(gs[1, :])\n",
    "before_positions = np.arange(NUM_COMPARISON_MODELS)\n",
    "after_positions = [np.arange(NUM_COMPARISON_MODELS) + (NUM_COMPARISON_MODELS * i) for i in range(1, 1 + NUM_AFTER_STATES)]\n",
    "\n",
    "after_titles = ['First Row', 'Second Row', 'Third Row']\n",
    "\n",
    "major_fontdict = dict(fontsize=20)\n",
    "minor_fontdict = dict(fontsize=14)\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[1],):\n",
    "    for i, hue in enumerate((None, 0.5, 0.9)):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        \n",
    "        b_ax, b_value_positions = None, None\n",
    "        if i == 0:\n",
    "            b_ax = before_ax \n",
    "            b_value_positions = before_positions\n",
    "        \n",
    "        do_multiple_augmentation_comparison_single_plot(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS, \n",
    "                                                    after_ax=after_axes[i], values_ax=values_ax, after_value_positions=after_positions[i],\n",
    "                                                    before_ax=b_ax, before_value_positions=b_value_positions, additional_boxplot_properties=None,\n",
    "                                                    before_title='Original State', after_title=after_titles[i],\n",
    "                                                    major_fontdict=major_fontdict,\n",
    "                                                    minor_fontdict=minor_fontdict,\n",
    "                                                    hsv=(hue, hue, None), object_index=0, object_pixels_and_mask=None\n",
    "                                                    )\n",
    "        \n",
    "\n",
    "dashed_line_positions = np.arange(1, NUM_AFTER_STATES + 1) * NUM_COMPARISON_MODELS - 0.5\n",
    "values_ax.vlines(dashed_line_positions, *values_ax.get_ylim(), colors='gray', linestyles='dashed')\n",
    "        \n",
    "values_ax.set_xticks([])\n",
    "# values_ax.set_xticks(np.concatenate([before_positions] + after_positions))\n",
    "# values_ax.set_xticklabels(list(FINAL_NAMES_FOR_TICKS) * 4, fontsize=12, rotation=0)\n",
    "\n",
    "values_ax.set_yticklabels(values_ax.get_yticks(), fontsize=12)\n",
    "values_ax.set_ylabel('V(s)', **major_fontdict)\n",
    "\n",
    "values_ax.set_xlabel('Condition', **major_fontdict)\n",
    "\n",
    "legend_handles = []\n",
    "for color, name in zip(DEFAULT_COLORS, FINAL_NAMES):\n",
    "    legend_handles.append(patches.Patch(color=color, label=name))\n",
    "    \n",
    "plt.legend(handles=legend_handles, loc='best', **minor_fontdict)\n",
    "\n",
    "# TODO add legend\n",
    "\n",
    "# save('fish_panel.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4654, device='cuda:0')\n",
      "tensor(0.5491, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alien_pixels_tensor = torch.tensor([[  0,   0,  52, 243,  52,   0,   0],\n",
    "       [  0,   0, 243, 243, 243,   0,   0],\n",
    "       [ 52, 243, 202, 243, 202, 243,  52],\n",
    "       [243, 243, 148, 243, 148, 243, 243],\n",
    "       [ 52, 243,  93, 243,  93, 243,  52],\n",
    "       [243,   0,   0,   0,   0,   0, 243],\n",
    "       [  0, 243,   0,   0,   0, 243,   0]], dtype=torch.float32, device=baseline_env.device) / 255\n",
    "\n",
    "print(alien_pixels_tensor.mean())\n",
    "# print(bad_animal_pixels_tensor.mean(), fish_pixels_tensor.mean(), igloo_pixels_tensor.mean(), player_pixels_tensor.mean())\n",
    "\n",
    "alien_mask_tensor = change_intensity(alien_pixels_tensor, multiplicative=2)\n",
    "print(alien_mask_tensor.mean())\n",
    "# print(bad_animal_mask_tensor.mean(), fish_mask_tensor.mean(), igloo_mask_tensor.mean(), player_mask_tensor.mean())\n",
    "\n",
    "\n",
    "alien_pixels_tensor_color = alien_pixels_tensor.repeat(3, 1, 1).permute(1, 2, 0)\n",
    "\n",
    "water_color = torch.tensor([  0.,  28., 136.], device='cuda:0') / 255\n",
    "\n",
    "black_indices = torch.all(alien_pixels_tensor_color == alien_pixels_tensor_color[0, 0], axis=2)\n",
    "gray_indices = torch.all(alien_pixels_tensor_color == alien_pixels_tensor_color[0, 2], axis=2)\n",
    "\n",
    "alien_pixels_tensor_color[black_indices + gray_indices] = water_color\n",
    "\n",
    "alien_pixels_and_mask = (alien_pixels_tensor_color * 255, alien_mask_tensor)\n",
    "plot_tensors(*alien_pixels_and_mask, norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.919, median = 6.670 | After mean = 3.435, median = 3.465 \n",
      "For model Pixels+Objects | Before mean = 6.169, median = 6.392 | After mean = 8.011, median = 8.353 \n",
      "For model Objects | Before mean = 6.175, median = 6.040 | After mean = 7.886, median = 8.167 \n",
      "For model Grouped Objects | Before mean = 6.460, median = 6.308 | After mean = 5.558, median = 5.938 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.919, median = 6.670 | After mean = 3.948, median = 4.228 \n",
      "For model Pixels+Objects | Before mean = 6.169, median = 6.392 | After mean = 3.881, median = 4.279 \n",
      "For model Objects | Before mean = 6.175, median = 6.040 | After mean = 2.496, median = 2.115 \n",
      "For model Grouped Objects | Before mean = 6.460, median = 6.308 | After mean = 5.010, median = 4.646 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.919, median = 6.670 | After mean = 3.256, median = 3.074 \n",
      "For model Pixels+Objects | Before mean = 6.169, median = 6.392 | After mean = 8.411, median = 8.302 \n",
      "For model Objects | Before mean = 6.175, median = 6.040 | After mean = 8.379, median = 8.639 \n",
      "For model Grouped Objects | Before mean = 6.460, median = 6.308 | After mean = 5.704, median = 5.707 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Pixels | Before mean = 5.919, median = 6.670 | After mean = 3.256, median = 3.074 \n",
      "For model Pixels+Objects | Before mean = 6.169, median = 6.392 | After mean = 4.168, median = 4.866 \n",
      "For model Objects | Before mean = 6.175, median = 6.040 | After mean = 2.558, median = 2.480 \n",
      "For model Grouped Objects | Before mean = 6.460, median = 6.308 | After mean = 5.704, median = 5.707 \n",
      "Figure:\n",
      "\n",
      "\\begin{figure}[!htb]\n",
      "% \\vspace{-0.225in}\n",
      "\\centering\n",
      "\\includegraphics[width=\\linewidth]{figures/alien_panel_smaller_fonts.pdf}\n",
      "\\caption{ {\\bf FIGURE TITLE.} FIGURE DESCRIPTION.}\n",
      "\\label{fig:alien-panel-smaller-fonts}\n",
      "% \\vspace{-0.2in}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      " Wrapfigure:\n",
      "\n",
      "\\begin{wrapfigure}{r}{0.5\\linewidth}\n",
      "\\vspace{-.3in}\n",
      "\\begin{spacing}{1.0}\n",
      "\\centering\n",
      "\\includegraphics[width=0.95\\linewidth]{figures/alien_panel_smaller_fonts.pdf}\n",
      "\\caption{ {\\bf FIGURE TITLE.} FIGURE DESCRIPTION.}\n",
      "\\label{fig:alien-panel-smaller-fonts}\n",
      "\\end{spacing}\n",
      "% \\vspace{-.25in}\n",
      "\\end{wrapfigure}\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INDEX = 875\n",
    "FISH_CHANNEL_INDEX = 6\n",
    "\n",
    "LOCATION_COLLECTIONS = (\n",
    "    [(80, 12 + 12 * i) for i in range(12)],\n",
    "    [(107, 12 + 12 * i) for i in range(6)],\n",
    "    [(107, 6 + 12 * i) for i in range(8, 12)],\n",
    "    [(135, 12 + 12 * i) for i in range(12)],\n",
    "    [(160, 12 + 12 * i) for i in range(12)],\n",
    "#     [(85, 12 + 12 * i) for i in range(12)],\n",
    ")\n",
    "\n",
    "NEW_LOCATIONS = []\n",
    "for loc in LOCATION_COLLECTIONS:\n",
    "    NEW_LOCATIONS.extend(loc)\n",
    "\n",
    "NUM_AFTER_STATES = 4\n",
    "    \n",
    "fig = plt.figure(constrained_layout=True, figsize=(10, 6))\n",
    "gs = fig.add_gridspec(2, 1 + NUM_AFTER_STATES)\n",
    "before_ax = fig.add_subplot(gs[0, 0])\n",
    "after_axes = [fig.add_subplot(gs[0, i]) for i in range(1, 1 + NUM_AFTER_STATES)]\n",
    "    \n",
    "values_ax = fig.add_subplot(gs[1, :])\n",
    "before_positions = np.arange(NUM_COMPARISON_MODELS)\n",
    "after_positions = [np.arange(NUM_COMPARISON_MODELS) + (NUM_COMPARISON_MODELS * i) for i in range(1, 1 + NUM_AFTER_STATES)]\n",
    "\n",
    "after_titles = ['Fish', 'Crabs', 'Fish Aliens', 'Crab Aliens']\n",
    "\n",
    "major_fontdict = dict(fontsize=16)\n",
    "minor_fontdict = dict(fontsize=12)\n",
    "\n",
    "# FISH\n",
    "IDX = 0\n",
    "        \n",
    "do_multiple_augmentation_comparison_single_plot(SRC_OBS_INDEX, DST_OBS_INDEX, FISH_CHANNEL_INDEX, NEW_LOCATIONS, \n",
    "                                            after_ax=after_axes[IDX], values_ax=values_ax, after_value_positions=after_positions[IDX],\n",
    "                                            before_ax=before_ax, before_value_positions=before_positions, additional_boxplot_properties=None,\n",
    "                                            before_title='Original', after_title=after_titles[IDX],\n",
    "                                            major_fontdict=major_fontdict,\n",
    "                                            minor_fontdict=minor_fontdict,\n",
    "                                            object_index=0, object_pixels_and_mask=None\n",
    "                                            )\n",
    "\n",
    "\n",
    "# CRABS\n",
    "IDX = 1\n",
    "CRABS_SRC_OBS_INDEX = 1270\n",
    "CRABS_CHANNEL_INDEX = 1\n",
    "\n",
    "do_multiple_augmentation_comparison_single_plot(CRABS_SRC_OBS_INDEX, DST_OBS_INDEX, CRABS_CHANNEL_INDEX, NEW_LOCATIONS, \n",
    "                                            after_ax=after_axes[IDX], values_ax=values_ax, after_value_positions=after_positions[IDX],\n",
    "                                            before_ax=None, before_value_positions=None, additional_boxplot_properties=None,\n",
    "                                            before_title='Original State', after_title=after_titles[IDX],\n",
    "                                            major_fontdict=major_fontdict,\n",
    "                                            minor_fontdict=minor_fontdict,\n",
    "                                            object_index=0, object_pixels_and_mask=None\n",
    "                                            )\n",
    "\n",
    "\n",
    "# Aliens as fish\n",
    "IDX = 2\n",
    "do_multiple_augmentation_comparison_single_plot(SRC_OBS_INDEX, DST_OBS_INDEX, FISH_CHANNEL_INDEX, NEW_LOCATIONS, \n",
    "                                            after_ax=after_axes[IDX], values_ax=values_ax, after_value_positions=after_positions[IDX],\n",
    "                                            before_ax=before_ax, before_value_positions=before_positions, additional_boxplot_properties=None,\n",
    "                                            before_title='Original', after_title=after_titles[IDX],\n",
    "                                            major_fontdict=major_fontdict,\n",
    "                                            minor_fontdict=minor_fontdict,\n",
    "                                            object_index=0, object_pixels_and_mask=alien_pixels_and_mask\n",
    "                                            )\n",
    "\n",
    "# Aliens as crabs\n",
    "IDX = 3\n",
    "do_multiple_augmentation_comparison_single_plot(CRABS_SRC_OBS_INDEX, DST_OBS_INDEX, CRABS_CHANNEL_INDEX, NEW_LOCATIONS, \n",
    "                                            after_ax=after_axes[IDX], values_ax=values_ax, after_value_positions=after_positions[IDX],\n",
    "                                            before_ax=before_ax, before_value_positions=before_positions, additional_boxplot_properties=None,\n",
    "                                            before_title='Original', after_title=after_titles[IDX],\n",
    "                                            major_fontdict=major_fontdict,\n",
    "                                            minor_fontdict=minor_fontdict,\n",
    "                                            object_index=0, object_pixels_and_mask=alien_pixels_and_mask\n",
    "                                            )\n",
    "\n",
    "        \n",
    "\n",
    "dashed_line_positions = np.arange(1, NUM_AFTER_STATES + 1) * NUM_COMPARISON_MODELS - 0.5\n",
    "values_ax.vlines(dashed_line_positions, *values_ax.get_ylim(), colors='gray', linestyles='dashed')\n",
    "        \n",
    "values_ax.set_xticks([])\n",
    "# values_ax.set_xticks(np.concatenate([before_positions] + after_positions))\n",
    "# values_ax.set_xticklabels(list(FINAL_NAMES_FOR_TICKS) * 4, fontsize=12, rotation=0)\n",
    "\n",
    "values_ax.set_yticklabels(values_ax.get_yticks(), fontsize=12)\n",
    "values_ax.set_ylabel('V(s)', **major_fontdict)\n",
    "\n",
    "values_ax.set_xlabel('Condition', **major_fontdict)\n",
    "\n",
    "legend_handles = []\n",
    "for color, name in zip(DEFAULT_COLORS, FINAL_NAMES):\n",
    "    legend_handles.append(patches.Patch(color=color, label=name))\n",
    "    \n",
    "plt.legend(handles=legend_handles, loc='lower left', **minor_fontdict)\n",
    "\n",
    "# save('alien_panel_smaller_fonts.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERESTING_DST_INDICES = (94, 1112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INDEX = 875\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "LOCATION_COLLECTIONS = (\n",
    "    [(80, 12 + 12 * i) for i in range(12)],\n",
    "    [(107, 12 + 12 * i) for i in range(6)],\n",
    "    [(107, 6 + 12 * i) for i in range(8, 12)],\n",
    "    [(135, 12 + 12 * i) for i in range(12)],\n",
    "    [(160, 12 + 12 * i) for i in range(12)],\n",
    "#     [(85, 12 + 12 * i) for i in range(12)],\n",
    ")\n",
    "\n",
    "NEW_LOCATIONS = []\n",
    "for loc in LOCATION_COLLECTIONS:\n",
    "    NEW_LOCATIONS.extend(loc)\n",
    "\n",
    "do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS, plot_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1200\n",
    "DST_OBS_INDEX = 2904\n",
    "CHANNEL_INDEX = 7\n",
    "# NEW_LOCATIONS = lambda loc: [(loc[0].start, loc[1].start + 10 * i) for i in range(12)]\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS, plot_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "CHANNEL_INDEX = 7\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "for DST_OBS_INDEX in INTERESTING_DST_INDICES:\n",
    "    do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "CHANNEL_INDEX = 6\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "START_LOC = (107, 12)\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[0], ):\n",
    "    for row_loc_inc in (0, 25, 55):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 1\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "START_LOC = (107, 12)\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[0],):\n",
    "    for row_loc_inc in (0, 25, 55):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "CHANNEL_INDEX = 1\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "START_LOC = (107, 12)\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[0],):\n",
    "    for i, row_loc_inc in enumerate((0, 25, 55)):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS,\n",
    "                                            save_name=f'invasion_of_the_crabs_{i}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SRC_OBS_INDEX = 2600\n",
    "\n",
    "CHANNEL_INDEX = 7\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "for DST_OBS_INDEX in INTERESTING_DST_INDICES:\n",
    "    do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS, hsv=(0.7, 0.8, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "START_LOC = (107, 12)\n",
    "row_loc_inc = 0\n",
    "\n",
    "colors = ('green', 'blue', 'red')\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[1],):\n",
    "    for i, hue in enumerate((None, 0.5, 0.9)):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, CHANNEL_INDEX, NEW_LOCATIONS,\n",
    "                                            hsv=(hue, hue, None),\n",
    "                                            save_name=f'what_about_{colors[i]}_fish.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And with an alien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_pixels_tensor = torch.tensor([[  0,   0,  52, 243,  52,   0,   0],\n",
    "       [  0,   0, 243, 243, 243,   0,   0],\n",
    "       [ 52, 243, 202, 243, 202, 243,  52],\n",
    "       [243, 243, 148, 243, 148, 243, 243],\n",
    "       [ 52, 243,  93, 243,  93, 243,  52],\n",
    "       [243,   0,   0,   0,   0,   0, 243],\n",
    "       [  0, 243,   0,   0,   0, 243,   0]], dtype=torch.float32, device=bad_animal_pixels_tensor.device) / 255\n",
    "\n",
    "print(alien_pixels_tensor.mean())\n",
    "print(bad_animal_pixels_tensor.mean(), fish_pixels_tensor.mean(), igloo_pixels_tensor.mean(), player_pixels_tensor.mean())\n",
    "\n",
    "alien_mask_tensor = change_intensity(alien_pixels_tensor, multiplicative=2)\n",
    "print(alien_mask_tensor.mean())\n",
    "print(bad_animal_mask_tensor.mean(), fish_mask_tensor.mean(), igloo_mask_tensor.mean(), player_mask_tensor.mean())\n",
    "\n",
    "\n",
    "alien_pixels_tensor_color = alien_pixels_tensor.repeat(3, 1, 1).permute(1, 2, 0)\n",
    "\n",
    "water_color = torch.tensor([  0.,  28., 136.], device='cuda:0') / 255\n",
    "\n",
    "black_indices = torch.all(alien_pixels_tensor_color == alien_pixels_tensor_color[0, 0], axis=2)\n",
    "gray_indices = torch.all(alien_pixels_tensor_color == alien_pixels_tensor_color[0, 2], axis=2)\n",
    "\n",
    "alien_pixels_tensor_color[black_indices + gray_indices] = water_color\n",
    "\n",
    "alien_pixels_and_mask = (alien_pixels_tensor_color * 255, alien_mask_tensor)\n",
    "plot_tensors(*alien_pixels_and_mask, norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MASK_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "# CHANNEL_INDEX = 1\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "START_LOC = (107, 12)\n",
    "row_loc_inc = 0\n",
    "\n",
    "for DST_OBS_INDEX in (INTERESTING_DST_INDICES[0],):\n",
    "    for channel_index in (1, 4, 5, 6):\n",
    "#     for row_loc_inc in (0, 25, 55):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 12 * i) for i in range(12)]\n",
    "        do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, channel_index, NEW_LOCATIONS,\n",
    "                                           object_pixels_and_mask=alien_pixels_and_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "# CHANNEL_INDEX = 1\n",
    "NEW_LOCATIONS = None\n",
    "\n",
    "START_LOC = (146, 12)\n",
    "row_loc_inc = 0\n",
    "\n",
    "for DST_OBS_INDEX in (755,):\n",
    "    for i, channel_index in enumerate((1, 5, 6)):\n",
    "#     for row_loc_inc in (0, 25, 55):\n",
    "        NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 10 * i) for i in range(8)]\n",
    "        do_multiple_augmentation_comparison(SRC_OBS_INDEX, DST_OBS_INDEX, channel_index, NEW_LOCATIONS,\n",
    "                                            object_pixels_and_mask=alien_pixels_and_mask,\n",
    "                                            save_name=f'save_me_aliens_{DEFAULT_MASK_NAMES[channel_index + 1]}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B -- Same shape, new color\n",
    "* New color in this case means new intensity since grayscale.\n",
    "* Can probaly repeat the same experiments as above but just changing the pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.1 Another player, new color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_INDEX = 1449\n",
    "CHANEL_INDEX = 0\n",
    "\n",
    "player_mask_tensor, player_loc = extract_object(sample_full_color_observations, OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANEL_INDEX, object_index=0, return_location=True)\n",
    "player_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, OBS_INDEX, baseline_model, baseline_env, player_loc)\n",
    "\n",
    "lightened_player_pixels_tensor = change_intensity(player_pixels_tensor, multiplicative=3.0)\n",
    "\n",
    "# plot_tensors(player_mask_tensor, player_pixels_tensor, lightened_player_pixels_tensor, norm=True)\n",
    "\n",
    "NEW_PLAYER_LOCATION = (45, 45)\n",
    "\n",
    "baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_player_pixels_tensor, player_mask_tensor, CHANEL_INDEX, [NEW_PLAYER_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, OBS_INDEX, \n",
    "                         [baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_INDEX = 1449\n",
    "CHANEL_INDEX = 0\n",
    "\n",
    "player_mask_tensor, player_loc = extract_object(sample_full_color_observations, OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANEL_INDEX, object_index=0, return_location=True)\n",
    "player_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, OBS_INDEX, baseline_model, baseline_env, player_loc)\n",
    "\n",
    "darkened_player_pixels_tensor = change_intensity(player_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(player_mask_tensor, player_pixels_tensor, darkened_player_pixels_tensor, norm=True)\n",
    "\n",
    "NEW_PLAYER_LOCATION = (45, 45)\n",
    "\n",
    "baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_player_pixels_tensor, player_mask_tensor, CHANEL_INDEX, [NEW_PLAYER_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, OBS_INDEX, \n",
    "                         [baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.2 Adding a completed igloo to a state without a complete igloo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 2600\n",
    "DST_OBS_INEX = 2100\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "lightened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=1.75)\n",
    "\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 2600\n",
    "DST_OBS_INEX = 2100\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "darkened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor, darkened_igloo_pixels_tensor, norm=True)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "lightened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=1.75)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "darkened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.3 Adding many of a particular good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor, lightened_fish_tensor, norm=True)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor, lightened_fish_tensor, norm=True)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 330\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, 5 + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 330\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, 5 + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.4 Same but with a bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, bad_animal_loc[1].start + 5 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, bad_animal_loc[1].start + 5 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "# NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "# NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C -- New shape (and presumably, new color)\n",
    "* What happens if an alien from space invaders comes to play Frostbite?\n",
    "* The current alien is a little bit larger than some existing other objects\n",
    "* **TODO: do we use the same image as the mask and the pixels?** Let's make it a little bit later brighter, like the other images tend to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_pixels_tensor = torch.tensor([[  0,   0,  52, 243,  52,   0,   0],\n",
    "       [  0,   0, 243, 243, 243,   0,   0],\n",
    "       [ 52, 243, 202, 243, 202, 243,  52],\n",
    "       [243, 243, 148, 243, 148, 243, 243],\n",
    "       [ 52, 243,  93, 243,  93, 243,  52],\n",
    "       [243,   0,   0,   0,   0,   0, 243],\n",
    "       [  0, 243,   0,   0,   0, 243,   0]], dtype=torch.float32, device=bad_animal_pixels_tensor.device) / 255\n",
    "\n",
    "print(alien_pixels_tensor.mean())\n",
    "print(bad_animal_pixels_tensor.mean(), fish_pixels_tensor.mean(), igloo_pixels_tensor.mean(), player_pixels_tensor.mean())\n",
    "\n",
    "alien_mask_tensor = change_intensity(alien_pixels_tensor, multiplicative=2)\n",
    "print(alien_mask_tensor.mean())\n",
    "print(bad_animal_mask_tensor.mean(), fish_mask_tensor.mean(), igloo_mask_tensor.mean(), player_mask_tensor.mean())\n",
    "\n",
    "plot_tensors(alien_pixels_tensor, alien_mask_tensor, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_alien_pixels_tensor = torch.tensor([[  0,  24, 243,  24,   0],\n",
    "       [ 24, 243, 243, 243,  24],\n",
    "       [243, 122, 243, 122, 243],\n",
    "       [ 24,   0, 243,   0,  24],\n",
    "       [ 44,   0,   0,   0,  44]], dtype=torch.float32, device=bad_animal_pixels_tensor.device) / 255\n",
    "                                         \n",
    "print(small_alien_pixels_tensor.mean())\n",
    "print(bad_animal_pixels_tensor.mean(), fish_pixels_tensor.mean(), igloo_pixels_tensor.mean(), player_pixels_tensor.mean())\n",
    "\n",
    "small_alien_mask_tensor = change_intensity(small_alien_pixels_tensor, multiplicative=2)\n",
    "print(small_alien_mask_tensor.mean())\n",
    "print(bad_animal_mask_tensor.mean(), fish_mask_tensor.mean(), igloo_mask_tensor.mean(), player_mask_tensor.mean())\n",
    "\n",
    "darker_small_alien_tensor = change_intensity(small_alien_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "plot_tensors(alien_pixels_tensor, alien_mask_tensor, small_alien_pixels_tensor, small_alien_mask_tensor, darker_small_alien_tensor, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.1 Aliens as bad animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 1\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((0, 10, 20), (0, 10, 20))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          small_alien_pixels_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.2 Darker alien as bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 1\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((10, 20, 30), (10, 20, 30))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          darker_small_alien_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.3 Aliens as good animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 6\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((0, 10, 20), (0, 10, 20))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          small_alien_pixels_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.4 Darker alien as good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 6\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((0, 10, 20), (0, 10, 20))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          darker_small_alien_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.5 Aliens as visited and unvisited ice floes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 6\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for name, channel_index in zip(('Unvisited Floes', 'Visited Floes'), (4, 5)):\n",
    "    display(Markdown(f'## {name}'))\n",
    "    \n",
    "    for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                       ((0, 10, 20), (0, 10, 20))):\n",
    "        for row_loc_inc in row_loc_incs:\n",
    "\n",
    "            NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "            baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "                make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                              darker_small_alien_tensor, small_alien_mask_tensor, channel_index, NEW_LOCATIONS)\n",
    "\n",
    "            evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                     [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                      force_text=True)\n",
    "\n",
    "            display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D Same as above but in aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INDEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 10 * i) for i in range(12)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "all_baseline_fish_augs = make_augmentation_per_model(baseline_fish_aug, all_baseline_models)\n",
    "all_masks_and_pixels_fish_augs = make_augmentation_per_model(masks_and_pixels_fish_aug, all_masks_and_pixels_models)\n",
    "all_masks_only_fish_augs = make_augmentation_per_model(masks_only_fish_aug, all_masks_only_models)\n",
    "\n",
    "\n",
    "evaluate_multiple_models_single_augmented_state(sample_full_color_observations, DST_OBS_INDEX, \n",
    "                                                (all_baseline_fish_augs, all_masks_and_pixels_fish_augs, all_masks_only_fish_augs),\n",
    "#                                                 key_actions=[5, 8, 9, 13, 16, 17],\n",
    "                                                names=('Rainbow', 'With Objects', 'Only Objects'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A -- Same shape, same colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.1 Additional player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OBS_INDEX = 1449\n",
    "\n",
    "player_mask_tensor, player_loc = extract_object(sample_full_color_observations, OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=0, object_index=0, return_location=True)\n",
    "player_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, OBS_INDEX, baseline_model, baseline_env, player_loc)\n",
    "\n",
    "NEW_PLAYER_LOCATION = (45, 45)\n",
    "\n",
    "baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  player_pixels_tensor, player_mask_tensor, 0, [NEW_PLAYER_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, OBS_INDEX, \n",
    "                         [baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.2 Adding a completed igloo to a state without a complete igloo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1200\n",
    "DST_OBS_INEX = 2904\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 2600\n",
    "DST_OBS_INEX = 2100\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.3 Adding many of a particular good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 13 * i) for i in range(10)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 330\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, 5 + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.4 Same but with a bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, bad_animal_loc[1].start + 5 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Plotting samples states to grab objects from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observations(sample_full_color_observations, 1230, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: finding intriguing states to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_sets = (baseline_model_results, masks_and_pixels_model_results, masks_only_model_results)\n",
    "\n",
    "q_value_arrays = [np.array([x.cpu().numpy() for x in result_set.q_values])\n",
    "                  for result_set in result_sets]\n",
    "mean_q_value_arrays = [np.tile(q_vals.mean(1), (q_vals.shape[1], 1)).T\n",
    "                       for q_vals in q_value_arrays]\n",
    "msd_array = np.array([np.power(q - mean, 2).mean(1) for (q, mean)\n",
    "                      in zip(q_value_arrays, mean_q_value_arrays)])\n",
    "\n",
    "mean_q_values = np.array([q_vals.mean(1) for q_vals in q_value_arrays])\n",
    "\n",
    "indices_without_extrema_q = np.argwhere(np.all(np.logical_and(mean_q_values > 3, mean_q_values < 7), axis=0))[:,0]\n",
    "\n",
    "indices_with_all_models_msd = np.argwhere(np.all(msd_array > 1, axis=0))[:,0]\n",
    "indices_with_two_models_msd = np.argwhere(np.sum(msd_array > 1, axis=0) > 0.5)[:,0]\n",
    "\n",
    "interesting_indices = sorted(set(list(indices_without_extrema_q)).intersection(set(list(indices_with_two_models_msd))))\n",
    "print(len(indices_without_extrema_q), len(indices_with_two_models_msd), len(interesting_indices))\n",
    "print(interesting_indices)\n",
    "\n",
    "s = 5\n",
    "for i in range(len(interesting_indices) // s):\n",
    "    plot_observations_by_indices(sample_full_color_observations, interesting_indices[i * s:(i + 1) * s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values(result_sets, names, **kwargs):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    q_value_arrays = [np.array([x.cpu().numpy() for x in result_set.q_values])\n",
    "                         for result_set in result_sets]\n",
    "    mean_q_value_arrays = [np.tile(q_vals.mean(1), (q_vals.shape[1], 1)).T\n",
    "                           for q_vals in q_value_arrays]\n",
    "    msd_arrays = [np.power(q - mean, 2).mean(1) for (q, mean)\n",
    "                  in zip(q_value_arrays, mean_q_value_arrays)]\n",
    "    \n",
    "    mean_ax = plt.subplot(2, 2, 1)\n",
    "    msd_ax = plt.subplot(2, 2, 2)\n",
    "    \n",
    "    for vals, msds, name in zip(q_value_arrays, msd_arrays, names):\n",
    "        mean_ax.plot(vals.mean(1), label=name, **kwargs)\n",
    "        msd_ax.plot(msds, label=name, **kwargs)\n",
    "        \n",
    "    mean_array = np.array([np.array([x.cpu().numpy() for x in res.q_values]).mean(1)\n",
    "                           for res in result_sets])\n",
    "    \n",
    "    min_max_mean_ax = plt.subplot(2, 2, 3)\n",
    "    min_max_mean_ax.plot(mean_array.min(0), label='Min')\n",
    "    min_max_mean_ax.plot(mean_array.max(0), label='Max')\n",
    "    \n",
    "    mean_msd_ax = plt.subplot(2, 2, 4)\n",
    "        \n",
    "    mean_ax.legend(loc='best')\n",
    "    msd_ax.legend(loc='best')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_q_values((baseline_model_results, masks_and_pixels_model_results, masks_only_model_results),\n",
    "              ('Baseline', 'Masks+Pixels', 'Masks-Only'), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_without_extrema_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(100, 100 + 10 * 10, 10))dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = masks_only_run.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h['human_hours'][h['human_hours'].last_valid_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_hours_all_models(*run_urls, run_checker=lambda t: True):\n",
    "    runs = [run for run in api.runs(run_urls[0]) if run_checker(run)]\n",
    "    for url in run_urls[1:]:\n",
    "        runs.extend([run for run in api.runs(url) if run_checker(run)])\n",
    "    \n",
    "    all_human_hours = []\n",
    "    for run in runs:\n",
    "        h = run.history()\n",
    "        human_hours = h['human_hours'][h['human_hours'].last_valid_index()]\n",
    "        all_human_hours.append(human_hours)\n",
    "    \n",
    "    all_human_hours = np.array(all_human_hours)\n",
    "    print(np.min(all_human_hours), np.mean(all_human_hours), np.max(all_human_hours))\n",
    "\n",
    "\n",
    "human_hours_all_models('augmented-frostbite/initial-experiments/runs', run_checker=lambda run: run.name.lower().startswith('baseline-rainbow-3'))\n",
    "\n",
    "human_hours_all_models('augmented-frostbite/masks-and-pixels-fixed-resume/runs',\n",
    "                       'augmented-frostbite/masks-and-pixels-replication/runs')\n",
    "\n",
    "human_hours_all_models('augmented-frostbite/masks-only/runs',\n",
    "                       'augmented-frostbite/masks-only-replication/runs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [observation_to_model(baseline_env, sample_full_color_observations[i])\n",
    "         for i in range(100, 104)]\n",
    "model_state = torch.cat(state, 0)\n",
    "model_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    probs = baseline_model.online_net(model_state.unsqueeze(0))\n",
    "    mean = (probs * baseline_model.support).sum(2)\n",
    "    var = (probs * (baseline_model.support - mean) ** 2).sum(2)\n",
    "    mean = mean.squeeze(0)\n",
    "    var = var.squeeze(0)\n",
    "    print(mean)\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = baseline_model.support.cpu().numpy()\n",
    "for i in range(var.shape[0]):\n",
    "    p = probs.squeeze(0)[i].cpu().numpy()\n",
    "    dist = rv_discrete(values=(x, p))\n",
    "    print(np.allclose(dist.var(), var[i].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.stats??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.q_value_mean_variance(model_state)[0] == baseline_model.expected_q_values(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rainbow] *",
   "language": "python",
   "name": "conda-env-rainbow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
