{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML, Markdown, Latex\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/gd1279/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/gd1279/projects/Rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats, ndimage\n",
    "from scipy.special import factorial\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "import wandb\n",
    "from collections import defaultdict, deque, namedtuple\n",
    "import os\n",
    "import argparse\n",
    "import atari_py\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "from agent import Agent\n",
    "from env import make_env\n",
    "from masker import ALL_MASKERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_SCRATCH_FOLDER = '/scratch/gd1279'\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_SCRATCH_FOLDER):\n",
    "    os.mkdir(CHECKPOINT_SCRATCH_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--heap-debug-file'], dest='heap_debug_file', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that hyperparameters may originally be reported in ATARI game frames instead of agent steps\n",
    "parser = argparse.ArgumentParser(description='Rainbow')\n",
    "parser.add_argument('--id', type=str, default='default', help='Experiment ID')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed')\n",
    "parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n",
    "parser.add_argument('--game', type=str, default='space_invaders', choices=atari_py.list_games(), help='ATARI game')\n",
    "parser.add_argument('--T-max', type=int, default=int(50e6), metavar='STEPS', help='Number of training steps (4x number of frames)')\n",
    "parser.add_argument('--max-episode-length', type=int, default=int(108e3), metavar='LENGTH', help='Max episode length in game frames (0 to disable)')\n",
    "parser.add_argument('--history-length', type=int, default=4, metavar='T', help='Number of consecutive states processed')\n",
    "parser.add_argument('--architecture', type=str, default='canonical', choices=['canonical', 'data-efficient'], metavar='ARCH', help='Network architecture')\n",
    "parser.add_argument('--hidden-size', type=int, default=512, metavar='SIZE', help='Network hidden size')\n",
    "parser.add_argument('--noisy-std', type=float, default=0.1, metavar='σ', help='Initial standard deviation of noisy linear layers')\n",
    "parser.add_argument('--atoms', type=int, default=51, metavar='C', help='Discretised size of value distribution')\n",
    "parser.add_argument('--V-min', type=float, default=-10, metavar='V', help='Minimum of value distribution support')\n",
    "parser.add_argument('--V-max', type=float, default=10, metavar='V', help='Maximum of value distribution support')\n",
    "parser.add_argument('--model', type=str, metavar='PARAMS', help='Pretrained model (state dict)')\n",
    "parser.add_argument('--memory-capacity', type=int, default=int(1e6), metavar='CAPACITY', help='Experience replay memory capacity')\n",
    "parser.add_argument('--replay-frequency', type=int, default=4, metavar='k', help='Frequency of sampling from memory')\n",
    "parser.add_argument('--priority-exponent', type=float, default=0.5, metavar='ω', help='Prioritised experience replay exponent (originally denoted α)')\n",
    "parser.add_argument('--priority-weight', type=float, default=0.4, metavar='β', help='Initial prioritised experience replay importance sampling weight')\n",
    "parser.add_argument('--multi-step', type=int, default=3, metavar='n', help='Number of steps for multi-step return')\n",
    "parser.add_argument('--discount', type=float, default=0.99, metavar='γ', help='Discount factor')\n",
    "parser.add_argument('--target-update', type=int, default=int(8e3), metavar='τ', help='Number of steps after which to update target network')\n",
    "parser.add_argument('--reward-clip', type=int, default=1, metavar='VALUE', help='Reward clipping (0 to disable)')\n",
    "parser.add_argument('--learning-rate', type=float, default=0.0000625, metavar='η', help='Learning rate')\n",
    "parser.add_argument('--adam-eps', type=float, default=1.5e-4, metavar='ε', help='Adam epsilon')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='SIZE', help='Batch size')\n",
    "parser.add_argument('--learn-start', type=int, default=int(20e3), metavar='STEPS', help='Number of steps before starting training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate only')\n",
    "parser.add_argument('--evaluation-interval', type=int, default=100000, metavar='STEPS', help='Number of training steps between evaluations')\n",
    "parser.add_argument('--evaluation-episodes', type=int, default=10, metavar='N', help='Number of evaluation episodes to average over')\n",
    "parser.add_argument('--evaluation-size', type=int, default=500, metavar='N', help='Number of transitions to use for validating Q')\n",
    "parser.add_argument('--render', action='store_true', help='Display screen (testing only)')\n",
    "parser.add_argument('--enable-cudnn', action='store_true', help='Enable cuDNN (faster but nondeterministic)')\n",
    "parser.add_argument('--save-evaluation-gifs', action='store_true', help='Save GIFs of evaluation episodes')\n",
    "parser.add_argument('--evaluation-gif-folder', default=None, help='Folder to save evaluation GIFs in')\n",
    "parser.add_argument('--save-evaluation-states', action='store_true', help='Save the states of evaluation episodes')\n",
    "parser.add_argument('--evaluation-state-folder', default=None, help='Folder to save evaluation state in')\n",
    "\n",
    "# Custom arguments I added\n",
    "\n",
    "SCRATCH_FOLDER = r'/misc/vlgscratch4/LakeGroup/guy/'\n",
    "\n",
    "DEFUALT_WANDB_ENTITY = 'augmented-frostbite'\n",
    "parser.add_argument('--wandb-entity', default=DEFUALT_WANDB_ENTITY)\n",
    "DEFAULT_WANDB_PROJECT = 'initial-experiments'\n",
    "parser.add_argument('--wandb-project', default=DEFAULT_WANDB_PROJECT)\n",
    "DEFAULT_WANDB_DIR = SCRATCH_FOLDER  # wandb creates its own folder inside\n",
    "parser.add_argument('--wandb-dir', default=DEFAULT_WANDB_DIR)\n",
    "parser.add_argument('--wandb-omit-watch', action='store_true')\n",
    "parser.add_argument('--wandb-resume', action='store_true')\n",
    "DEFAULT_MEMORY_SAVE_FOLDER = os.path.join(SCRATCH_FOLDER, 'rainbow_memory')\n",
    "parser.add_argument('--memory-save-folder', default=DEFAULT_MEMORY_SAVE_FOLDER)\n",
    "parser.add_argument('--memory-save-interval', type=int, default=None, help='How often to save the memory, defaults to the evaluation interval')\n",
    "parser.add_argument('--use-native-pickle-serialization', action='store_true', help='Use native pickle saving rather than torch.save()')\n",
    "\n",
    "# Arguments for the augmented representations\n",
    "parser.add_argument('--add-masks', action='store_true', help='Add masks for each semantic object types')\n",
    "parser.add_argument('--maskers', default=None, help='Select specific maskers to use')\n",
    "parser.add_argument('--use-numpy-masker', action='store_true', help='Use the previous, much slower numpy-based masker')\n",
    "parser.add_argument('--omit-pixels', action='store_true', help='Omit the raw pixels from the environment')\n",
    "\n",
    "# Arguments to give it a soft time cap that will help it not fail\n",
    "parser.add_argument('--soft-time-cap', help='Format: <DD>:HH:MM, stop after some soft cap such that the saving the memory does not fail')\n",
    "\n",
    "# Debugging-related arguments\n",
    "parser.add_argument('--debug-heap', action='store_true')\n",
    "parser.add_argument('--heap-interval', default=1e4)\n",
    "parser.add_argument('--heap-debug-file', default=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a run and its model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_checkpoint(run, step=None):\n",
    "    files = run.files()\n",
    "    if step is None:\n",
    "        step = max([int(f.name[f.name.rfind('-') + 1:f.name.rfind('.')]) \n",
    "                    for f in files \n",
    "                    if f.name.endswith('.pth')])\n",
    "        \n",
    "    sample_name = [f.name for f in files if f.name.endswith('.pth')][0]\n",
    "    checkpoint_name = sample_name[:sample_name.rfind('-')]\n",
    "    checkpoint_file = f'{checkpoint_name}-{step}.pth'\n",
    "    run.file(checkpoint_file).download(replace=True, root=CHECKPOINT_SCRATCH_FOLDER)\n",
    "    return os.path.join(CHECKPOINT_SCRATCH_FOLDER, checkpoint_file)\n",
    "\n",
    "\n",
    "def setup_args(run):\n",
    "    args = parser.parse_args([])\n",
    "    config = run.config\n",
    "    \n",
    "    for key in config:\n",
    "        if key in args:\n",
    "            args.__setattr__(key, config[key])\n",
    "            \n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if torch.cuda.is_available() and not args.disable_cuda:\n",
    "        args.device = torch.device('cuda')\n",
    "        # torch.cuda.manual_seed(np.random.randint(1, 10000))\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "        torch.backends.cudnn.enabled = args.enable_cudnn\n",
    "    else:\n",
    "        args.device = torch.device('cpu')\n",
    "            \n",
    "    return args\n",
    "\n",
    "\n",
    "LOADED_MODEL_CACHE = {}\n",
    "\n",
    "\n",
    "def load_model_from_run(run, step=None, cache=LOADED_MODEL_CACHE):\n",
    "    key = (run, step)\n",
    "    if key not in cache:\n",
    "        checkpoint_path = download_checkpoint(run, step)\n",
    "        args = setup_args(run)\n",
    "        args.model = checkpoint_path\n",
    "\n",
    "        env = make_env(args)\n",
    "        dqn = Agent(args, env)\n",
    "        cache[key] = dqn, env\n",
    "        \n",
    "    return cache[key]\n",
    "\n",
    "\n",
    "FIGURE_TEMPLATE = r'''\\begin{{figure}}[!htb]\n",
    "% \\vspace{{-0.225in}}\n",
    "\\centering\n",
    "\\includegraphics[width=\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "% \\vspace{{-0.2in}}\n",
    "\\end{{figure}}\n",
    "'''\n",
    "WRAPFIGURE_TEMPLATE = r'''\\begin{{wrapfigure}}{{r}}{{0.5\\linewidth}}\n",
    "\\vspace{{-.3in}}\n",
    "\\begin{{spacing}}{{1.0}}\n",
    "\\centering\n",
    "\\includegraphics[width=0.95\\linewidth]{{figures/{save_path}}}\n",
    "\\caption{{ {{\\bf FIGURE TITLE.}} FIGURE DESCRIPTION.}}\n",
    "\\label{{fig:{label_name}}}\n",
    "\\end{{spacing}}\n",
    "% \\vspace{{-.25in}}\n",
    "\\end{{wrapfigure}}'''\n",
    "\n",
    "SAVE_PATH_PREFIX = 'figures'\n",
    "\n",
    "\n",
    "def save(save_path, bbox_inches='tight'):\n",
    "    if save_path is not None:\n",
    "        save_path_no_ext = os.path.splitext(save_path)[0]\n",
    "        print('Figure:\\n')\n",
    "        print(FIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "        print('\\n Wrapfigure:\\n')\n",
    "        print(WRAPFIGURE_TEMPLATE.format(save_path=save_path, label_name=save_path_no_ext.replace('/', '-').replace('_', '-')))\n",
    "        print('')\n",
    "        \n",
    "        if not save_path.startswith(SAVE_PATH_PREFIX):\n",
    "            save_path = os.path.join(SAVE_PATH_PREFIX, save_path)\n",
    "        \n",
    "        folder, filename = os.path.split(save_path)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=bbox_inches, facecolor=plt.gcf().get_facecolor(), edgecolor='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "\n",
    "ModelResults = namedtuple('ModelResults', \n",
    "                          ('hidden_states', 'q_values', 'state_values', 'actions'))\n",
    "\n",
    "KeyIndex = namedtuple('KeyIndex',\n",
    "                      ('peak_index', 'peak_value', 'start', 'end', 'count', 'indices'))\n",
    "\n",
    "\n",
    "def rgb_to_grayscale(obs):\n",
    "    # My best approximation of how the ALE does it\n",
    "    is_tensor = isinstance(obs, torch.Tensor)\n",
    "    if is_tensor:\n",
    "        rgb = obs.type(torch.float32)\n",
    "    else:\n",
    "        rgb = obs.astype(np.float32)\n",
    "        \n",
    "    gray = rgb[:,:,0] * 0.299 + rgb[:,:,1] * 0.587 + rgb[:,:,2] * 0.114\n",
    "    \n",
    "    if is_tensor:\n",
    "        return gray.type(torch.uint8)\n",
    "    else:\n",
    "        return gray.astype(np.uint8)\n",
    "\n",
    "\n",
    "def observation_to_model(env, obs):\n",
    "    return env._prepare_state(env._to_tensor(rgb_to_grayscale(obs)), env._to_tensor(obs))\n",
    "\n",
    "\n",
    "MAX_STATE_IDX = None\n",
    "SKIP = 2\n",
    "\n",
    "\n",
    "def pass_states_through_model(model, env, observations, max_state_idx=MAX_STATE_IDX, skip=SKIP):\n",
    "    state_buffer = deque([], maxlen=4)\n",
    "    hidden_states = []\n",
    "    q_values = []\n",
    "    state_values = []\n",
    "    actions = []\n",
    "\n",
    "    for frame in observations[:3]:\n",
    "        state_buffer.append(observation_to_model(env, frame))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frame in observations[3:max_state_idx]:\n",
    "            state_buffer.append(observation_to_model(env, frame))\n",
    "            state = torch.cat(list(state_buffer), 0)\n",
    "            \n",
    "            hidden_state = model.online_net.convs(state.unsqueeze(0)).view(-1)\n",
    "            hidden_states.append(hidden_state.detach().cpu().numpy())\n",
    "            \n",
    "            q_values.append(model.expected_q_values(state))\n",
    "            state_values.append(model.evaluate_q(state))\n",
    "            actions.append(model.act(state))\n",
    "\n",
    "    hidden_state_array = np.array(hidden_states[::skip])\n",
    "    q_values_array = np.array(q_values[::skip])\n",
    "    state_value_array = np.array(state_values[::skip])\n",
    "    action_array = np.array(actions[::skip])\n",
    "    \n",
    "    return ModelResults(hidden_state_array, q_values_array, state_value_array, action_array)\n",
    "\n",
    "\n",
    "def plot_entire_state(observations, start_index, num_frames=4):\n",
    "    figure = plt.figure(figsize=(18, 4))\n",
    "    for i in range(num_frames):\n",
    "        ax = plt.subplot(1, num_frames, i + 1)\n",
    "        ax.imshow(observations[start_index + i])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_embeddings_and_state(embeddings, color_values, observations, start_index, special_indices=None, num_frames=4,\n",
    "                              low_alpha=0.1, medium_alpha=0.7, alpha_threshold=0.7):\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(16, 8))\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    \n",
    "    tsne_ax = fig.add_subplot(gs[:, :2])\n",
    "    cmap = matplotlib.cm.get_cmap('Spectral_r')\n",
    "    normalizer = matplotlib.colors.Normalize(np.min(color_values), np.max(color_values))\n",
    "\n",
    "    colors = np.array([cmap(normalizer(d)) for d in color_values])\n",
    "    colors[:,3] = np.abs(color_values) / np.max(np.abs(color_values))\n",
    "    colors[colors[:,3] > alpha_threshold, 3] = medium_alpha\n",
    "    colors[colors[:,3] < alpha_threshold, 3] = low_alpha\n",
    "\n",
    "    mask = np.zeros(Y.shape[0], dtype=bool)\n",
    "    if special_indices is not None:\n",
    "        mask[special_indices] = True\n",
    "        colors[mask, 3] = 1\n",
    "\n",
    "    tsne_ax.scatter(embeddings[~mask, 0], embeddings[~mask, 1], color=colors[~mask])\n",
    "    tsne_ax.scatter(embeddings[mask, 0], embeddings[mask, 1], color='purple', s=50, marker='x')\n",
    "    plt.colorbar(matplotlib.cm.ScalarMappable(norm=normalizer, cmap=cmap), ax=tsne_ax)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        ax = fig.add_subplot(gs[i // 2, 2 + (i % 2)])\n",
    "        ax.imshow(observations[start_index + i])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "ALE_ACTIONS = {\n",
    "    0: 'noop',\n",
    "    1: 'fire',\n",
    "    2: 'up',\n",
    "    3: 'right',\n",
    "    4: 'left',\n",
    "    5: 'down',\n",
    "    6: 'up + right',\n",
    "    7: 'up + left',\n",
    "    8: 'down + right',\n",
    "    9: 'down + left',\n",
    "    10: 'up + fire',\n",
    "    11: 'right + fire',\n",
    "    12: 'left + fire',\n",
    "    13: 'down + fire',\n",
    "    14: 'up + right + fire',\n",
    "    15: 'up + left + fire',\n",
    "    16: 'down + right + fire',\n",
    "    17: 'down + left + fire'\n",
    "}\n",
    "\n",
    "\n",
    "def print_model_state_description(results, name, index, top_k=3):\n",
    "    value = results.state_values[index]\n",
    "    action = results.actions[index]\n",
    "    print(f'{name} had value {value:.3f} => {ALE_ACTIONS[action]} ({action})')\n",
    "    q = results.q_values[index].cpu().numpy()\n",
    "    top_actions = np.argpartition(q, -top_k)[-top_k:]\n",
    "    top_actions = top_actions[np.argsort(q[top_actions])][::-1]\n",
    "    p = F.softmax(results.q_values[index], dim=0)\n",
    "    print(' | '.join([f'({i + 1}) {ALE_ACTIONS[a]} [{a}], Q = {q[a]:.3f}, P = {p[a]:.3f}' for i, a in enumerate(top_actions)]))\n",
    "\n",
    "    \n",
    "def describe_states_by_indices(indices, first_model_results, first_model_name, second_model_results, second_model_name, \n",
    "                               observations, embeddings, color_values, top_k=3, plot_embeddings=False):\n",
    "    for key_index in sorted(indices, key=lambda ki: ki.peak_index):\n",
    "        index = key_index.peak_index\n",
    "        print(f'At index {index}')\n",
    "        print_model_state_description(first_model_results, first_model_name, index, top_k)\n",
    "        print_model_state_description(second_model_results, second_model_name, index, top_k)\n",
    "        \n",
    "        if plot_embeddings:\n",
    "            plot_embeddings_and_state(embeddings, color_values, observations, \n",
    "                                      key_index.peak_index, \n",
    "                                      special_indices=key_index.indices)\n",
    "        else:\n",
    "            plot_entire_state(observations, index)\n",
    "    \n",
    "    \n",
    "def find_diverging_states(values, indices, min_distance=10):\n",
    "    index_values = [values[i] for i in indices]\n",
    "    output = [KeyIndex(indices[0], index_values[0], indices[0], indices[0], 1, [indices[0]])]\n",
    "    \n",
    "    for index, value in zip(indices[1:], index_values[1:]):\n",
    "        current = output[-1]\n",
    "        \n",
    "        # Sufficiently far away, append a new one\n",
    "        if index > current.end + min_distance:\n",
    "            output.append(KeyIndex(index, value, index, index, 1, [index]))\n",
    "            \n",
    "        # Value more extreme, replace peak\n",
    "        elif abs(value) > abs(current.peak_value):  \n",
    "            output[-1] = KeyIndex(index, value, current.start, index, current.count + 1, current.indices + [index])\n",
    "            \n",
    "        # Value not more extreme, extend\n",
    "        else:\n",
    "            output[-1] = KeyIndex(current.peak_index, current.peak_value, current.start, index, current.count + 1, current.indices + [index])\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gameplan\n",
    "\n",
    "1. See that I can locate objects\n",
    "2. See that I can add additional objects\n",
    "3. See what this changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch\n",
    "\n",
    "Using model 306 from the baseline condition, one of the average models, not the 'superstar' one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5671, 210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SAVED_STATES = r'/home/gd1279/scratch/rainbow-evaluation-state-traces/baseline-rainbow-305/evaluation/states/eval-baseline-rainbow-305-34350000-0-env.pickle'\n",
    "\n",
    "with open(SAMPLE_SAVED_STATES, 'rb') as state_file:\n",
    "    sample_full_color_observations = pickle.load(state_file)\n",
    "    \n",
    "    \n",
    "sample_full_color_observations = sample_full_color_observations.astype(np.uint8)\n",
    "print(sample_full_color_observations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model: /scratch/gd1279/baseline-rainbow-306-29150000.pth\n"
     ]
    }
   ],
   "source": [
    "baseline_run = api.run('augmented-frostbite/initial-experiments/runs/fdxobftk')\n",
    "baseline_model, baseline_env = load_model_from_run(baseline_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5668, 3136)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model_results = pass_states_through_model(baseline_model, baseline_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "baseline_model_results.hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5668, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne = TSNE(n_jobs=4)\n",
    "Y = tsne.fit_transform(baseline_model_results.hidden_states)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model: /scratch/gd1279/masks-only-replication-306-10000000.pth\n"
     ]
    }
   ],
   "source": [
    "masks_only_run = api.run('augmented-frostbite/masks-only-replication/runs/0khc2n2c')\n",
    "masks_only_model, masks_only_env = load_model_from_run(masks_only_run)\n",
    "masks_only_model_results = pass_states_through_model(masks_only_model, masks_only_env, sample_full_color_observations, skip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player\n",
      "Bad Animal\n",
      "Land\n",
      "Bear\n",
      "Unvisited Floes\n",
      "Visited Floes\n",
      "Good Animal\n",
      "Igloo\n"
     ]
    }
   ],
   "source": [
    "for x in DEFAULT_MASK_NAMES[1:]:\n",
    "    print(x.replace('_', ' ').title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure:\n",
      "\n",
      "\\begin{figure}[!htb]\n",
      "% \\vspace{-0.225in}\n",
      "\\centering\n",
      "\\includegraphics[width=\\linewidth]{figures/mask_examples.pdf}\n",
      "\\caption{ {\\bf FIGURE TITLE.} FIGURE DESCRIPTION.}\n",
      "\\label{fig:mask-examples}\n",
      "% \\vspace{-0.2in}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      " Wrapfigure:\n",
      "\n",
      "\\begin{wrapfigure}{r}{0.5\\linewidth}\n",
      "\\vspace{-.3in}\n",
      "\\begin{spacing}{1.0}\n",
      "\\centering\n",
      "\\includegraphics[width=0.95\\linewidth]{figures/mask_examples.pdf}\n",
      "\\caption{ {\\bf FIGURE TITLE.} FIGURE DESCRIPTION.}\n",
      "\\label{fig:mask-examples}\n",
      "\\end{spacing}\n",
      "% \\vspace{-.25in}\n",
      "\\end{wrapfigure}\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x918 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame_idx = 1230\n",
    "obs = sample_full_color_observations[frame_idx]\n",
    "state = observation_to_model(masks_only_env, sample_full_color_observations[frame_idx]).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 12.75))\n",
    "plt.subplots_adjust(hspace=0.075, wspace=0.075)\n",
    "\n",
    "pixels_ax = plt.subplot(3, 3, 1)\n",
    "pixels_ax.imshow(cv2.resize(obs, (160, 160), interpolation=cv2.INTER_LINEAR))\n",
    "pixels_ax.set_title('Pixels', fontsize=16)\n",
    "pixels_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "pixels_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "for i, name in enumerate(DEFAULT_MASK_NAMES[1:]):\n",
    "    ax = plt.subplot(3, 3, i + 2)\n",
    "    ax.imshow(state[i], cmap='Greys')\n",
    "    ax.set_title(name.replace('_', ' ').title(), fontsize=16)\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "\n",
    "save('mask_examples.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure:\n",
      "\n",
      "\\begin{figure}[!htb]\n",
      "% \\vspace{-0.225in}\n",
      "\\centering\n",
      "\\includegraphics[width=\\linewidth]{figures/mask_examples_original_res.pdf}\n",
      "\\caption{ {\\bf FIGURE TITLE.} FIGURE DESCRIPTION.}\n",
      "\\label{fig:mask-examples-original-res}\n",
      "% \\vspace{-0.2in}\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      " Wrapfigure:\n",
      "\n",
      "\\begin{wrapfigure}{r}{0.5\\linewidth}\n",
      "\\vspace{-.3in}\n",
      "\\begin{spacing}{1.0}\n",
      "\\centering\n",
      "\\includegraphics[width=0.95\\linewidth]{figures/mask_examples_original_res.pdf}\n",
      "\\caption{ {\\bf FIGURE TITLE.} FIGURE DESCRIPTION.}\n",
      "\\label{fig:mask-examples-original-res}\n",
      "\\end{spacing}\n",
      "% \\vspace{-.25in}\n",
      "\\end{wrapfigure}\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAJ2CAYAAADv8NNuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5hcRYH+8fclkZuCgAFkFZgVEERBfjggF0F2FxEjghEUlksS1xV1dWHFdQVcJbiuoq4ogiiIGkJEEdagKzEIkaAiKBMviAFEIUoACQFEQAiX1O+Pqk5OTrp7uivd0316vp/n6Wemz6k6fU53zXm76lT3OIQgAADQnnV6vQMAAFQRAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZBhXAWp7uu1QuD1i+1e232N7Yiqz2PbMLu5DsD2jW9tHb/RD20L/qNMenrF9t+1v2t6xw4/VVruyfUzap1904LF7cj7rl/PoxF7vQI+8WdISSRun38+WtIWkD0uaIukvvds1VFyztoXxp9YeJkjaTtKHJM23/dIQwsM92qdp6edutncJIfx6Lba1t+LxjUvjNUB/GUL4Xfr9+7a3l3SipA+HENb6XRnGtYZtq4f71JTtZ0l6OvCtKt1QbA/X2b5H0lWS9pH0vbHeGdsvkPQP6bFfpxim/567vRDCDR3atUoaV0O4TdwoaWPbWxSHQ2yvY3tBWvbcWmHbu9h+3PanihuxfXwatnvC9jLbX7a9WbMHtv1i23NsL031/mj70tqwHypvZdsqr7C9ue3zbP/W9l9t32X74nSSq5U5PA1XvbxO/QW2byjcn2j7FNu32l5u+x7bn7a9fqHMUNrev9j+ZDqhL5e0ScePHPXURreeVVtge3vbF9m+M51X7rD9BdublivbPjGdj56wPWJ7vzYf/zjF8/5pkq6TdIztCaXHOCC1kUNtn5POZctsz7a9SansakOptmekZTvZvtL2Y+mc9ta0/rjUPh+1fY3t7UrbO8r2D2zfn8r8wvY09SkCNPpbSc9IerS4MISwQtKxkjaSdJ4k2d5A0jck/UbSB2tlbZ8h6fOSrpZ0qKT3SzpY0vfKDbTkCkkvkPQuSa+VdLLiCY3XZjDUbVvJZpKekHSKYlt5v6QdFHsqtdD7tqR7JL2jWNH2TpJeLemLhcWzJf2npIslvV7SxyW9TdLX6jz2ByW9WNLxipctnmj/0NCCCemNzXq2XyLpY5KWSlpQKPM3ku6S9G+K54CPKPYS5xY3ZPttkj4r6RpJb5Q0U9LXJa0RtE1Mk3RLCOFGSbMkPV/SQQ3KniUpSDpa0umSDk/LWnGp4rntjZIWSvqK7Y8pnudOlvRWSTsqttWiF0m6TNIxqe7/SbrA9jtbfNyxFUIYNzdJ0xUbxI6Kw9ebKp6YnpF0eSqzWNLMUr0pqd5bJZ0v6RFJOxTWD6VtfLhUb99U742FZUHSjPT7pHT/0F4/N9x607ZK25ggaeu0nSmF5TMkPSzp2YVlZ0p6SNIG6f5+qd7U0jaPSct3S/eH0v2fS3Kvn7dBvRXaQ/l2t6Q9Rqk7UdKrUvn/l5atoxiy80plj0zlGrarQtk9U9lT0v1NJD0u6RulcgekcheWlp+j+EbLhWUrz2eFtrpaO0x/C09LekDSxoXlJ6Sy2zbY33XSc/ElSb8qrVvtcXt1G6+9nFslPSXpQUnnKr5D/6dGhUMIcxR7oF+Q9HZJJ4QQbi8UeY3ii/219G5zYhqC/ali2O7fYNMPSLpD0hm23257h7U7LPSBttqW7XelYf9HFU8yf0yrijM1z5e0oaR/THXWV+xJzAohPJ7KHCzpSUmXldrg99P6chu8PKQzEbpqiqQ9FMPrjZIWSZqbeqOSJNvr2j41DW0+rth+fpRW19rBC9Ptm6Xt/69iu2nFNEkrFEcqFEL4s+IIx2HFS1QFV5Tu/1rSepK2bOGxVl7fDSE8pNjrviGEUJygeWv6uXVtge0dbH/d9t2Kz8NTkv5Zq/899I3xGqC1Rr2T4rv6qSGEB0epc6Fi41mqNYcdate3fqdVL3rttpGk59XbYDqBvUbSiOJw22/T9Y93tX1E6Bctty3b/6oYsldLepPiSXavtHrldcsQwj2KJ7raMNabFYd/zytsbgtJ60p6TKu3v6VpfbkN3ptxbGjfzSGEkRDCjSGEbyte3rFiT63m4+n+bMWh9z0V24O0qh1slX7eV9x4CKHWs2vK9rqSjpJ0vaRHbG+SrmfOSY/xljrVyu12eWmfmnmodP/JBstWbs/2cxQnWL1ccZh3P8W/pa8onnv7znidqHJzWDUzblS2N1R8EW9WvEZ1hqT3ForUGvBBWrORFNevIYRwh6Sptq3YcN4j6Vzbi0MIYz5LD2utnbZ1lKT5IYT31RbY/tsGZc9V/PjDKxSHhn8UQlhUWP+A4vBao0kl95Tu0/vsgRDC47bvkLRrYfFRiqMJH60tSGFSVHvDs1rvL40y1H2DXvIGxTdd+6r+OWqa4lBpL+0taVtJ+4UQflxb2M8TKvt2x/rMWYoTfXaTdIikz9qeF0K4Mq2/SnFoZJsQwlU5D5B6o7+0fZLixI+XqQfT3DGmNtSanzl+a72CIYQf2L5V8drnvorXNovmSfqApOeGEOZ3ekfRGenN+HaKkxBrNlQcLSgqt4MlitdA36L4Zr7mcLV2Hp+mODpxmOJ1+fK66ba3CyH8voVtdcuG6efK5yLNRD6sN7szOgJ0FLYPVxyDPy71Fj9n+yBJF9reNYSwNITwe9ufkHSO47eMXKvYG9hacYj2ghDCNXW2vatiOF+iOPw7QXHywdOSftD9o0OPzZP0AdunSvqZpL+XdEST8l9QbC/LFK99rRRCWGD764rXQM9M21uhOGlosqQPhBB+2/EjwGh2sz1Jcdh2K8URps0Uv2CjZp6kabZ/rXgeeJPi50RXCiGssH264ozUryp+EmB7xaHOpl/84vgRqtdJml3vzZXtPymed6YqfrylV36ieCyft32apGcrzipfJqneNdqeI0CbsL214rDG10IIswur3irpJkkzbb8+RKfavkXSu9MtKL5jnC/pdtX3J8VJIycpThB4QvFC/SEhhIXdOCb0lY8ozoR8r+J1oGsVP8ZwR4PylyoG6MwQwvI664+V9K+Kk5Y+qHjNarGkK1W6doYxc2nh9/sVLwMdXBi9kuJrZkn/ne7PVZww9rPihkIIX05Duyel9Tenn8VzUz1HK57rv1JvZQjhVts/UbyUNKOFY+qKEML9tqdI+rTiR1nuUWzvm6m3wd6QmYgHVIPttytOHHpxO9fwAXQHPVCgz9neWfG62emKHz8hPIE+QA8U6HO2FyheE/uJpKPTx1oA9BgBCgBAhvH6RQoAAKwVAhQAgAxtTSKyzXgvWhZCcLt1Jk2aFIaGhrqwNxhECxcuXBZC2LzderQztGrx4sVatmxZ3XMZs3DRV4aGhjQyMtLr3UBF2P5DTj3aGVo1PDzccB1DuAAAZCBAAQDIQIACAJChY9dAd9l9Xqc2hQHwu1v/tde7AABdRQ8UAIAMBCgAABkq+TGWI6ceuMayS2ZdPeq68vri8tHWAQBQVMkArWkUco0Cs/Z7MWzLwVtvHQAAZZUewj1y6oFr9DjL4UkIAgC6oZI90HpDr82GYwEA6LRK90ABAOiVSgYovUsAQK9VcghXypsxe8msq1e7blqs12wdAABllQzQVsKtlRm6OdsFAECq6BAuAAC9RoACAJCh50O47X4zUDe/LYhvIgIAtKqnAVr+/Gar3wxUm/AzVvsCAEBZT4dwcwOqGx9jISwBAO3o+RBuTas9vrEIOnqfAIDRMIkIAIAMfROg3biumauf9gUA0J96PoTbaOZrL74ZiFm4AIBW9TxA++mbgQhNAECr+mYIFwCAKiFAAQDI0LEh3DftvHWnNoUBcP7idXu9CwDQVR0L0B2eeapTm8IAWF+h17sAAF3VsQB9ke7q1KawFm7Y4/asenvduENH92M9PdnR7QFAv+lYgG69/8Wd2hTWwg2P75FVr9Ov37Oue7Cj2wOAfsMkIgAAMnSsB7p4+ZJObQptuHHFlI5s59JCz3WPdeas9faeDAzhAhhsHQvQ507aqFObQjuWdn6TnXgtJ0xkcAPAYOtYgN635f2d2hTa0YUA7cRr+dTEpzuwJwDQvzoWoM961qc6tSm05ecd32InXkv7+A7sCQD0r55/Fy7a9/Ofdz40m21/99137+rjAUAVcaEKAIAMBCgAABkYwh1gJ510Ut3lZ5555hjvCQAMHnqg41CjYAUAtI4ABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAy8DGWCmr1m4EWLFjQ3R0BgHGMHigAABkIUAAAMgxcgM6ZM0dz5qz9P4QGAKCZgQtQAADGwsBMIir3Ouv1QqdMmTJWuwMAGHDjqgfK8C4AoFMGIkAJRQDAWBuIAAUAYKwNRIBybRMAMNYGIkDnzJmjKVOmEKQAgDEzEAFKcAIAxtpABGgzhCsAoBsGPkABAOgGAhQAgAwD801EZfWGbhnOBQB0ysAEaKMvU+BLFgAA3TAwAdqod0mvEwDQDVwDBQAgQ8d6oAcccECnNgUAQN/r3BDuC8/o2KYwAO47u9d7AABdxRAuAAAZCFAAADIQoAAAZCBAK+I3x/+wpWUA0M9st7SsCgjQCqgFZTEwCU8AVVMLymJgVjU8pQH6IoVB99Lz9296HwCqIITQ9H6VEKAVUu51EqIAqqjc66xqiDKEWwEvPX9//eb4H64MzNpPhnEBVEkIQbZXBmbtZ1WHcQnQCiAoAQyCqgZlIwRohdSbTAQAVVNvMlEVcQ20ArjWCWAQVPVaZyP0QAEAyECAAgCQgQAFACADAQoAQAYmEdWx6PuHNly380HfoV6b9QBgEBGgBVUJpKrUA4BBxhBuUpVQqko9ABh0474HWpVAqko9ABgvxnUPtCqhVJV6ADCejNsArUooVaUeAIw343YINzcMqAcAkMZxDxQAgLVBgAIAkIEABQAgAwEKAECGcTuJqFU3zZ232v1dJx/coz0BgHzlf149aP+bsxfogTZw09x5a4RnbTkAVIXtNcKzthxrhwBtotbbLP8EgCqp9TbLP7F2CNAmbpo7T7tOPniNnwBQJbYVQljjJ9YOATqK2pBt+ScAVEltyLb8E/kI0AboaQIYBPQ0u4cAbaJeiBKsAKqmXogSrGuPj7GMgsAEMAgIzM6jBwoAQAYCFACADAQoAAAZCFAAADL0bBLRESdMbrjuss/NpV6b9QAAY2vMA7QqgVSVegCA3mAIFwCADGMaoFXp1VWlHgCgd8ZkCLcqgVSVegCA3utqgFYlkKpSDwDQP9zO1zvZblz4hWd0Yn8wKO47W+HJJW3/u4fh4eEwMjLSjT3CALK9MIQw3G492hlaNTw8rJGRkbrnsp5+F+6sg85b+fvU77+jh3uCQVX8l018FyiATuqbWbjFMAW6gf9/CKCTehagBCa6jcAE0E190wOVCNXRnDj3Xp04995e70alEaoAOqVnAbr9hlvVvaG+YnDWfidMAaB3+qoHivpOnHuvzpq81RrLAAC907MA3efyQ3v10JVULzDLoYrVMesWQDf19GMsN7zoiNXun3Tmkz3ak2qq1zNFc4QqgE7pqyHcM09at9e70JfqhSTBmYdJRAA6pecBSq+zNcXArP1OiLaGXieAbujpEK5Er7MdBGYeep0AuqHnPVAAAKqopz3QSZMuqLN06pjvBwAA7aIHCgBAhp72QKeeSm8T3cUEIgDdQg8UAIAMHeuBHnHC5IbrLvvcXOq1WQ8A0N+6NoTbT2FUlXoAgOroyhBuP4VSVeoBAKqFa6AAAGToeID2U6+uKvUAANXTsWug/RRIVakHAKiuMfkcaG6IDHo9AEB1cQ0UAIAMBCgAABkIUAAAMvT8/4Gif900d95q93edfHCP9gQA8pX/J3CnviObHijqKodno2UA0M/K4dloWQ4CFGsgKAEMgk4FZSMEKBqqDdmWfwJAldSGbMs/1xYBirp2nXywbpo7b42fAFAlIQTZXuNnJxCgWEMxLMs/AaAqimFZ/tkJBCjqqjdcyxAugKqpN1zbqSFcPsaChghMAIOgU4FZRg8UAIAMBCgAABkIUAAAMhCgAABk6NgkouI/le6nf1pdlXoAgGrpeA+0n0KpKvUAANXT0QDtp1CqSj0AQDV1bAi3UUhUJcgIzs7adIvn9noXAKCruj6JqJ9Cq5/qDbqHlj7c610AgK7q6jcR9VNg9VO9QXfECZM1/6xZvd4NAOiqrgRoVYKM4Oys8dzjBjD+uJ3vCLTduPALz+jE/mBQ3He2wpNL2v63B8PDw2FkZKQbe4QBZHthCGG43Xq0M7RqeHhYIyMjdc9lfJECAAAZCFAAADJ07hrokpM7tikAAPodPVAAADIQoAAAZCBAAQDIQIACAJCBAAUAIENXv8qvnnm7796R7Wwze7Z23nlnLVq0qGGZnXfeeeXvjcoVywAA0KoxD9CnD16x1tt40bEXr/y9FoCLFi1qGJi13wlLAECnVG4Itxie0po9y0b3i0FbvJWXFes0690CAMa3Me+BbnPUJtl1J048V5JGHbotq5WvhWgxTMtlAABoxZgH6AU/y+v0vnOfc1b+3mrQlYd126lPmAIAmhnzAL3m3P9ou84ls7dZIwhbCbh2rnmWt0ePFADQTOf+nRlQEkLg35mhq/h3Zug2/p0ZAAAdRoACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMjQ7jcR3S/pD93bHQyQbUMIm7dbiTaGNtHO0G0N21hbAQoAACKGcAEAyECAAgCQgQAFACDDuA1Q29NtB9vb93g/Fthe0Mt9QHcV2lrt9oztu21/0/aOvd4/1Gd7pu0lDdYdkF7LA7v02ENp+9PbqFNrZ0OFZTNs/30X9i/YnjFKmQNK7b54++dUpu3j7Cdj/g+1gXHszZKWSJogaTtJH5I03/ZLQwgP93TP0G/ulbS3pN+3UeeKVOfewrLTJP23pB90btfadoKkG0vL2jmuvkWAAmPnlyGE36Xfr7N9j6SrJO0j6XtjsQO21wshLB+Lx0K+9Brd0Gad+yXd3509Wiu3hBDaOpaqGLdDuKOxvYfty2wvsf247dtsf8z2BqVyC2z/2PaBtn9u+6+2b7Y9pc42j7J9q+3ltn9TrwzGlb+kn8+qLbD9ctvfsf1QanfX2d6vWCmjbb7B9i9sL5f0L90/rPHJ9mLbs9Pf+S22H7M9YvtVhTLvt/2k7efVqb/I9rfT72sMbabX/SrbD6TX/Q7b5xbWrzaEa7v2GcUPFoZOZxTKv9r2fNuPpH290vbLSvs0wfZHbd+bzm0LbL+0I09YE7aPtf0r20/YXmb7Ittb1Sl3fKncl21vVipzYno9Hk9/VyOdOvcSoI1tI+mXkt4p6WBJZ0n6J0lfrVN2u7T+TElvUhxCudSF66vpWsnFkm5PZT6V6nANbPyYYHui7fVsv0TSxyQtlbRAkmzvLuknkjaT9HZJh0t6QNLVtl9R2E47bfPFkj4n6WxJr5U0v/OHhYL9JL1PcXj+SMXh+u/a3iStvzgtO7JYKb2+L5E0q95GbT9H0pWSnpE0XdLrJH1EzUcR904/Z6bf95Z0Qdre6xXbwqOSjpV0tKSNJP3I9taFbcyQdKqkr0l6o6TvS/pOk8esZ53U7mu3Cc0K2z5e0kWSblE8V56s2HavTc9DrdwZkj4v6WpJh0p6v+Lfw/dqj2H7GEmflvR1SZMlHSPpMsW/sbUXQhiXN8VGGCRt30JZKzbUYyWtkPS8wroFkp6StENh2RaKDf3UwrLrJC2StE5h2V5pHxb0+vng1r1boa2Vb3dL2qNQbr7iSWPdwrIJadnlDbY9WttcIWm3Xj8HVbwpBs+SBusOSK/hgYVliyU9JGnTwrLhVO7owrKrJF1f2t5nU9310v2hVG96aTu7ttDOhgrLgqSP1in7O0nzS8s2lrRM0mfT/U0VA/aLpXIfSNudMcrzV3uOyrclhTLl45wg6T5J15S29apU7oRCvWckfbhUbt9U7o3p/jmSft6tNkIPtAHbG9v+hO3fS1quGJIXKZ6wdigVvz2EcHvtTghhqWLPYpu0rQmS9pB0WQhhRaHcDYp/dBgfpii2gz0V380vkjTX9kvS8OurJV0qaUXt3bpie7ta0v61jbTZNheHEH7Z5ePCKteHEB4q3P91+rlNYdksSXvVRqjS6/yPkr4ZGl+fvl3SnyWdl4Y3t25QblS2d1AcNftasWco6a+SrteqtraLpGdL+mZpE99o8yHfrdjua7fJTcruqNgB+VpxYQjhx4pfvfjqtOg1iiOo5WP4qaRHCsdwo6TdbJ/teJltwzb3vSkCtLGvKg6RfU7xxdpDsSFI0vqlsg/Wqb+8UG6S4nWu++qUq7cMg+nmEMJICOHGEMK3FYedrDhMtpniu+8PKQZi8fYeSZvarv29ttM27xVyPa34mtQzoVCmaLVzQSEQi6/LtyQ9Jum4dP8gxdCoO3ybtvOwpL+TdI+kcyX90XGuxeGjHEM9W6SfX9aabe0QSbXrs7VrjuVzVLvnrN+mdl+73dSkbG1otV67/VNhfe0Yfqc1j2EjrTqGWZLeJemVikPgD9r+lgsf9VkbzMKtw/b6kg5THKI4q7B8l8xNLlN8Ybess25L8aXW41II4XHbd0jaVbF3sULxmk7dE2kIYUVG2+TLrvMtlTTJ9rohhCdL6/4m/Wz7DXAI4THbcxSvx52mOPx+RwjhulHq/VLS4amnNSzpFEnftP3yEMLNbezCA+nnKYqjG2W1Y62F2JaSflNYX+881im1NyDPr7Pu+ZIWpt9rx3CQ4tB32QOSFOI47nmKPfdNU/lPS7pEMVTXCj3Q+tZTfIf5VGn59JyNhRCeURxKOKLQi5DtVyqO5WMcSsNJ20m6P4TwmKQfSXq54jWbkfItVeto20RT1yh2Mg6ts+5wxYC5LXPbsyRtZ/u1isP5s1utGEJ4Ol3++ZDiOfwlTYo/KWmD0rLbFC8dvbReOyv0EG9S7Cm/pVT/qFb3NcNtim9KVnsM2/tI2lZpwp3ideQVkrZpcAx3ljccQngohHCJ4pD0y8rrc9ADlQ62/afSsocVP4P1Ptv3KvYg/0nSC9bicU5TnMF2ue3zJG0u6XTFYQmMD7vZnqQ4bLuV4tDsZoozZCXpJEk/lHSl7S8rnqAnSdpd0oQQwskhhIdtd7ptor6rFU/UM23vpHh9bSPFk/thkt5anNPQpvmKw7FfVgy4i5oVtn2IpOMlXS7pTsVrkycoXu+7vknVRZJeb3ueYk/tnhDCPbbfLenbttdVDJRlij3LfST9MYRwZgjhz7Y/o/gxmEcUz197SHpb5jGPKoTwjO0PK/YYZyu+sXiB4pdB3C7pK6nc721/QtI5jt/mda2kJyRtrXhZ44IQwjW2z9eq52ip4qz049KxdGSHx+VNjWdGBkk3K/YMv5ee/KWKs7len9YfUNjOAkk/rrP9xZJmlpb9o+I7rOWKQyJTUv0FvX4+uI15W1uq+O0wry2VfYniJI2lqZ0sUfzYwORCmbVqm9zaeu02kPRRSb9Nr8cjiiMFh9Upu1jS7DrL685YVfwoW5D0kzrrhrT67NQdFYcd71QMivslzZX0yjrtbKiwbF/FYc8nyvuh+LGW7yoG6xNp/78hae9CmQnp+P8k6fHUpnZudEylYzhApZnKox1nYfmxkn6VnvMHFN9gbFWn/nGKnZ3HFGcM35L+Hl6Y1k9L+1z7e7pT0mckbdyJ9sH/AwUAIAPXQAEAyECAAgCQgQAFACADAQoAQAYCFACADG19DnTSpElhaGioS7uCQbJ48WItW7bM7dajjaEdCxcuXBZC2LzderQztKrZuaytAB0aGtLIyMjoBTHuDQ8PZ9WjjaEdtrO+BpN2hlY1O5cxhAsAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMEzu1Idud2hTaEELo+DZ5LQFgdB0LUIydboRms+0TqACwJoZwAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMjANxFVEN8MBAC9Rw8UAIAMBCgAABkGLkBDCF3/snWMb7YZRgcweAEKAMBYGJhJROVeZ71eKL0GrI1y+6nXnhj9AMaPcdUDZXgX3cbwLjB+DESAEoroNkIRQNlABCgAAGNtIAKU3gG6jVEOAGUDEaAhBK49oatscw0dwGoGIkAJTnQbwQmgbCACtBnCFd1GuALj08AHKAAA3UCAAgCQYWC+iais3tAtw7nopHpDtwznAuPHwARooxMXJzR0SqM3YLwxA8angQlQTm7oNt6kASjiGigAABk61gPlXTiKhoeHe70LANBV9EABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZOjYP9QGAPQX2w3XhRCo12a9MgIUAAZMVQKpKvUaYQgXAAZIVUKpKvWaoQcKAAOgKoFUlXqtoAcKABVXlVCqSr1WEaAAUGFVCaWq1GsHQ7gAUGG5YUC9tUcPFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKLB5oA8AABH6SURBVAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyOAQQuuF7fsl/aF7u4MBsm0IYfN2K9HG0CbaGbqtYRtrK0ABAEDEEC4AABkIUAAAMvQsQG1fbvsh2+s1WL+R7cdsz0z3Z9hua7zZ9gLbCwr3d0vb2Wxt9r3O40y3HWwPjVJuZipX7zYxlWn7ONGc7b1tf8P2EttP2v6L7Rtt/5ftrXqwP4tr7brF8sekNvKLDjx2sD1jbbdTlccdDwrnn+3brMe5Zi1N7OFjXyjpMEmHSPrfOuuPkLRhKidJF0ia1+Zj/Evp/m6STpM0W9KDbW6rU+6XdGh5YQjh6R7sy8Cz/T5Jn5J0jaT/lHSHpOdI2kfS8ZKGJb2uZzvYmmnp5262dwkh/HottrW3pCUd2Cdg3OtlgF4h6QFJU1U/QKdK+qOkBZIUQliiNv/wQwiL1m4Xu+LJEMINvd6J8cD23ymG51khhPeWVs+1/XFJbx77PWud7RdI+gdJ31MM+mmS/j13e7Q9oHN6NoQbQnhS0tclvc7284rrbG8j6dWSLgppmnC94QbbJ9q+xfbjaTh4xPaUwvqVQ7i2p0v6alp1e2HodCitn2j7FNu32l5u+x7bn7a9fukxX2T7Ctt/tX2/7bMk1R2G7hTbG9s+J+3Tctu32X6vbZfKbW77i7bvTuVutX18qczzbV9Y2Na9tr9re4tuHkOPfEDSsvRzDSGEx0IIM4vLbG9le5btZen5ucn2seW6tve0fbXtR9Olhvm296xT7sQ0ZPtEap/7tXkMxyn+nZ4m6TpJx9ieUHqMA1JbPjS1k2XpNtv2JqWyqw2l1v6ubO9k+8p0LH+0/da0/rjUjh61fY3t7UrbO8r2D9LfwqO2f2F7mtAztifY/mj62/5ren12amUYvY1zzY6259j+czr/3mD74K4eWB/qZQ9UisOz75F0lKTPF5YfK8mSZjWqaPsYSZ+W9BFJP5K0gaRdJTW6vnmFpI8qDuO9Wat6s/emn7MlvUHSJyT9RNJLJP2XpCFJh6fHXFfSVemx3i1pqaR3SHpTS0e7at/Lz/uKEMKKBmXXSfu+u6QPS/q1pNdLOlPS5pJOTeU2lvTjtG8zJN0p6bWSvmB7vRDC2WmTF0naVtL7Jd0laUvFHs6G7RxDv0vP8aslfSu9WWulzrMlXStpU8Xn9S7FtniR7Q1DCOencrumcoskTZcUJJ0s6Vrbe4UQfpXKvU3SZyXNlHSJpO0V3zRu1MahTJN0SwjhRtuzJJ0n6SDFHmnZWZK+K+loSTtK+qSkZ7RqCLiZSyV9SdL/KF76+IrtHSQdkI7tWWn7F0t6ZaHeiyRdJukMSSsk7S/pAtsbhBC+2MZxonNOV2y/n5J0taRXSPrOaJXaONf8jeK55hHF8/fDiufDK2wfEkKo1zYHUwihpzdJv5H009KyWyRdX1o2I+7uyvvnSPr5KNteIGlB4f50xZPd9qVy+6XlU0vLj0nLd0v3357u71Uos046hiBpaJT9mZnKlW8fbXKch6Qy00vbukDSckmT0v0PSXpC0g6lcl9S7IVNTPcflXRCr1/3MWhXW6bn7eN11k0s3grL35PqHFAqf7Xim6UJ6f5lkv4saZNCmY0Vr6t/q9Au7pI0r7StI9NjzGzhGPZMZU9J9zeR9Likb5TKHZDKXVhafk5qEy4sC5JmlNtbse0rvoF4WvESy8aF5Seksts22N910nP6JUm/Kq1b7XG5dbStT0/P7/bptXtU0rmlMic1eu0L91s91/xPah/bF8pMkHSbRjknD9qtHz7GcqGkPW2/WIpDY5J20qrJQ43cqDip4mzbB9pemx7UwZKelHRZGsqdmHow30/r908/95Z0VyhcRwqx5/jNNh5rqaQ9Srdzm5TfX/Gd/cWl5bMlrZv2qXYMP5V0Z+kYrpT0PEk7p3I3Snp/GlrcpTw0M+hsP1/SU8VbYURgf0l3hxAWlKrNVnwHvnOh3HdDCH+uFQgh/EXxXf6r06IXplu5bfyv4smnFdMUX/vZ6TH+LOnbkg6z/dw65a8o3f+14uWFLVt4rJW9hhDCQ4rt9IZ0XDW3pp9b1xbY3sH2123frVXP6T8r9oAx9naR9GzFEYWiy1qo2+q5Zn/FtvG7WoEQwjOKoyu7pdGwcaEfAnS24os2Nd2fqvhu55JR6s2S9C7F4aQrJT1o+1se5aMkDWyh2EAe0+on16Vpfe0a7VaS7qtTv96yRp4KIYyUbvc0Kb+ZpAfDmsOQfyqsrx3D/qX9f0qr/pBqx3Ck4on+PyTdJOlu2x9OwzeD5AHF3tc2peXLtOqNy5dK6zbTqiH9ovJz3azcpun32sdjVmsbIc62fmCUfa9dLjhK0vWSHrG9SbqeOUfS+pLeUqdaeWb58vRz/XLBOh4q3X+ywbKV27P9HMVLGi9XHObdT/F5/Yq6PC8ADdXa3dLS8lbOUa2ea5q1f2vV38DA6/U1UIUQ7rF9laRjbX9E8QT/f+ldcLN6QfF60Hm2N1W8LvRpxeB9ZbO6ddROto0meNQC7l5JL62zvpV3+LkelLSZ7XVLDfv5hfVSPIalkk5ssJ3bJCmEsFTxesW7be+o2Ms5XfHjNV/o8L73TAjhads/lPSa4nOXAmxEkmwfUqr2oOr3nMrP9YOFZeVytXZbO8Gs1jZSb3e1SXMNvEHxRLWv1gwyKb5u5TcAY21vxevp+4UQflxbWOcaP8ZOrd1toXhpqaaVc1Sr55pm7T+ofnsdSP3S67hQ8Q/x45ImafTh29WEEB4KIVyiOFz2siZFa+/INygtn6f4rvq5dXqHxR7i9ZK2tr1XrWLqudXrDXTKtYqvU/njFsco9giuLxzDTpL+2OAYHilvOIRwWwjhVMUG3+x5q6pPKranT7RY/lpJL7S9b2n50YpvThYVyk22vXIyUPr9DUofu1KcpHaX1mwbh6u1N67TFEdEDpT0d6XbTEn7lmfE9kDtsslTtQXpzexhvdkdKA7bP6Y1zxetfFyr1XPNtZL2Ko72pZnhR0r6RWnYf6D1yzvFyyX9RdJ7FU9Uo35hgu3zFWeBXZ/qvFhxyv/3m1SrnQDfbftCxT/8m0IIC2x/XfEa6JmSfqY4rDwkabKkD4QQfqsY7CdL+pbtU9PjvlNxAkm3fE9xxtsXbW+u+K5ysuJ1po+HEJalcp9RbMA/sv0ZxR7nsxVDdb8QQu262dWSvqZ4PespxZPdpmr+vFVSCGG+7ZMlnZFmzs5SnJ28vmJ7OUrxZFP7eNRMxR78t2x/UDEEj5H0GknvSNd5pDg7+xBJ821/ItX/gGKgfCQ99grbpyvOSP2qpG8oTvI4WbGtN+T4kaLXSZodQphfZ/2fFCeOTFX8eEuv/ETxWD5v+zTF9vafisPk9a7RostCCA/Z/qykU20/ovj3vrukt6UidWf7J+2ca6ZLuiq97n9RnLn9YsVZu+NHr2cx1W6Kw1FB0mcarJ+h1WeMTVN8t79UsWd5p+ILW5w1uECFWbhp2WmS7lac3r9y5qziO68TJf1KcTj34fT7JxV7prX6L5I0V9JfFYc9z1L8KEurs3CXjFJmteNMyzZWnFF5r+I7wd8qvtlwqdym6Tm4M5VbqvgRn39L69dTHPb+jeJMvb8oTio6utevf5fb1r6KoxN3p+eldtynS9qqVHYrxY/6LEvt6iZJx9bZ5isVT06PKobwfEl71il3ouK/zXpCcej4VZIWq8ksXEn/ltrTfk3KXJdeZ2vVLNwDS2Wml9ulGs/CnViqu1gxwIvL1ngcSX8v6ReKs4N/rzhTt14bZhZu99p37XXePt2fIOm/Fa9JPq54HtwnlTmx/NqXttXquWZHxY7Pw6lt3yDp4F4/F2N949+ZAcCAs32E4oTC/UMIP+r1/gwKAhQABojtVyoOpf5UsXf4CsVLB7dJ2idw0u+YfrkGCgDojEcVP9L2bsUh2aWKlzBOITw7ix4oAAAZ+uVjLAAAVAoBCgBAhraugU6aNCkMDQ11aVcwSBYvXqxly5a1/T27tDG0Y+HChctCCJu3W492hlY1O5e1FaBDQ0MaGRnpzF5hoA0PD2fVo42hHbb/kFOPdoZWNTuXMYQLAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyTOz1DgAAUGY7q14IocN70hg9UAAAMhCgAABkYAgXANBzuUO2zbbT7eFceqAAAGQgQAEAyECAAgCQgWugAICe6NR1z1a33+lrovRAAQDIQIACAJCBIVwAQF9rNPTa7SHg0dADBQBU0lh+bV89BCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIGPsQAAeqITs2h7OROXHigAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyTOzVA9tuuC6EQL026wEAxtaYB2hVAqkq9QAAvcEQLgAAGcY0QKvSq6tKPQBA74zJEG5VAqkq9QAAvdfVAK1KIFWlHgCgf3Q1QHPDgHoAgH7HJCIAADIQoAAAZCBAAQDIQIACWMl200luAFYhQAFIWn12eO13whRojAAFINtrzA4nPIHmCFAAkuoHJh+5AhojQAE0RC8UaIwABVC3p0nvE2iOAAUgafXArP1OiAKN9ewfagPoPwQm0Dp6oAAAZCBAAQDIQIACAJCBAAUAIAMBCgBAho7Nwm32getmM/uoBwCooq59jKWfwqgq9QAA1dGVIdx+CqWq1AMAVAvXQAEAyNDxAO2nXl1V6gEAqqdj10D7KZCqUg8AUF1j8l24uSEy6PUAANXFNVAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkIEABQAgAwEKAEAGAhQAgAwd+3+gxX8q3U//tLoq9QAA1dLxHmg/hVJV6gEAqqejAdpPoVSVegCAaurYEG6jkKhKkBGcnbVw4cJe7wIAdFXXJxH1U2j1U71B94pXvKLXuwAAXdWxHmg9/RRY/VRv0I3nNw4Axo+uBGhVgozg7CyCE8B40pUAzQ2RQa836IrPy/DwcA/3BAC6jy9SAAAgAwEKAEAGAhQAgAwEKAAAGQhQAAAyEKAAAGQgQAEAyECAAgCQgQAFACADAQoAQAYCFACADAQoAAAZCFAAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMhCgAABkIEABAMhAgAIAkMEhhNYL2/dL+kP3dgcDZNsQwubtVqKNoU20M3RbwzbWVoACAICIIVwAADIQoAAAZCBAAQDIQIACAJCBAAUAIAMBCgBABgIUAIAMBCgAABkIUAAAMvx/rv01NiQUOhUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x792 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame_idx = 1230\n",
    "obs = sample_full_color_observations[frame_idx]\n",
    "# state = observation_to_model(masks_only_env, sample_full_color_observations[frame_idx]).cpu().numpy()\n",
    "state = masks_only_env.masker(torch.tensor(sample_full_color_observations[frame_idx]).to(masks_only_env.device)).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 11))\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "pixels_ax = plt.subplot(3, 3, 1)\n",
    "# pixels_ax.imshow(cv2.resize(obs, (160, 160), interpolation=cv2.INTER_LINEAR))\n",
    "pixels_ax.imshow(obs)\n",
    "pixels_ax.set_title('Pixels', fontsize=16)\n",
    "pixels_ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "pixels_ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "for i, name in enumerate(DEFAULT_MASK_NAMES[1:]):\n",
    "    ax = plt.subplot(3, 3, i + 2)\n",
    "    ax.imshow(state[i], cmap='Greys')\n",
    "    ax.set_title(name.replace('_', ' ').title(), fontsize=16)\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "\n",
    "save('mask_examples_original_res.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More organized take at this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChannelAugmentation = namedtuple('ChannelAugmentation', \n",
    "                                 ('channel_index', 'added_object', 'added_location'))\n",
    "\n",
    "ModelAugmentation = namedtuple('ModelAugmentation', \n",
    "                               ('name', 'model', 'env', 'pixel_augmentations', 'mask_augmentations'))\n",
    "\n",
    "\n",
    "def augment_pixels(state, pixel_augmentation):\n",
    "    dst_slices = [slice(pixel_augmentation.added_location[i], \n",
    "                        pixel_augmentation.added_location[i] + pixel_augmentation.added_object.shape[i])\n",
    "                  for i in range(len(pixel_augmentation.added_location))] \n",
    "    \n",
    "    state[dst_slices[0], dst_slices[1], :] = pixel_augmentation.added_object\n",
    "    return state\n",
    "\n",
    "\n",
    "def augment_mask_channel(state, channel_augmentation):\n",
    "    dst_slices = [slice(channel_augmentation.added_location[i], \n",
    "                        channel_augmentation.added_location[i] + channel_augmentation.added_object.shape[i])\n",
    "                  for i in range(len(channel_augmentation.added_location))] \n",
    "    state[channel_augmentation.channel_index, dst_slices[0], dst_slices[1]] = channel_augmentation.added_object\n",
    "    return state\n",
    "    \n",
    "\n",
    "def modify_observation_to_model_state(obs, model_augmentation, return_pixels=False):\n",
    "    # pixel augmentations if any exist\n",
    "    obs_tensor = model_augmentation.env._to_tensor(obs)\n",
    "    if model_augmentation.pixel_augmentations is not None:\n",
    "        for pixel_aug in model_augmentation.pixel_augmentations:\n",
    "            obs_tensor = augment_pixels(obs_tensor, pixel_aug)\n",
    "    \n",
    "    # take this tensor and drop it to grayscale and low-res\n",
    "    gray_obs_tensor = rgb_to_grayscale(obs_tensor)\n",
    "    \n",
    "    masks = None\n",
    "    \n",
    "    # create masks for this model\n",
    "    if hasattr(model_augmentation.env, 'masker'):\n",
    "        masks = model_augmentation.env.masker(model_augmentation.env._to_tensor(obs))\n",
    "    \n",
    "        # mask augmentations if any exist\n",
    "        if model_augmentation.mask_augmentations is not None:\n",
    "            for augmentation in model_augmentation.mask_augmentations:\n",
    "                masks = augment_mask_channel(masks, augmentation)\n",
    "                \n",
    "        masks = model_augmentation.env._resize(masks.unsqueeze(0))\n",
    "    \n",
    "    # combine (handles resizing of the raw observation)\n",
    "    final_state = model_augmentation.env._prepare_state(gray_obs_tensor.type(torch.float32), None, masks)\n",
    "    \n",
    "    if return_pixels is True:\n",
    "        return final_state, obs_tensor.type(torch.uint8)\n",
    "#     \n",
    "    return final_state\n",
    "\n",
    "\n",
    "def augmented_state_q_values(observations, augmented_index, model_augmentations, state_length=4, \n",
    "                             return_before_and_after=False, return_variance=False):\n",
    "    if augmented_index < state_length or augmented_index >= len(observations):\n",
    "        raise ValueError(f'Augmented index should be in [{state_length}, {len(observations)}), received {augmented_index}')\n",
    "\n",
    "    before_q_values_per_model = []\n",
    "    before_q_variances_per_model = []\n",
    "    after_q_values_per_model = []\n",
    "    after_q_variances_per_model = []\n",
    "    before_pixels, after_pixels = None, None\n",
    "    \n",
    "    for model_augmentation in model_augmentations:\n",
    "        # TODO: verify I don't have an off-by-one here\n",
    "        state_buffer = [observation_to_model(model_augmentation.env, obs) \n",
    "                        for obs in observations[augmented_index - state_length + 1:augmented_index + 1]]\n",
    "        \n",
    "        if return_before_and_after and before_pixels is None:\n",
    "#             before_pixels = state_buffer[-1][0].cpu().numpy()\n",
    "            before_pixels = observations[augmented_index]\n",
    "        \n",
    "        model_ready_state = torch.cat(list(state_buffer), 0)\n",
    "        if return_variance:\n",
    "            mean, variance = model_augmentation.model.q_value_mean_variance(model_ready_state)\n",
    "            before_q_values_per_model.append(mean)\n",
    "            before_q_variances_per_model.append(variance)\n",
    "        else:\n",
    "            before_q_values_per_model.append(model_augmentation.model.expected_q_values(model_ready_state))\n",
    "\n",
    "        augmented_state = modify_observation_to_model_state(observations[augmented_index], model_augmentation)\n",
    "        if return_before_and_after and after_pixels is None:\n",
    "            augmented_state, after_pixels_tensor = modify_observation_to_model_state(observations[augmented_index], \n",
    "                                                                                     model_augmentation, return_pixels=True)\n",
    "            after_pixels = after_pixels_tensor.cpu().numpy()\n",
    "            \n",
    "        else:\n",
    "            augmented_state = modify_observation_to_model_state(observations[augmented_index], model_augmentation)\n",
    "#             after_pixels = state_buffer[-1][0].cpu().numpy()\n",
    "            \n",
    "        # TODO: consider the case of augmenting more than one consecutive states\n",
    "        state_buffer[-1] = augmented_state\n",
    "        model_ready_state = torch.cat(list(state_buffer), 0)\n",
    "        \n",
    "        if return_variance:\n",
    "            mean, variance = model_augmentation.model.q_value_mean_variance(model_ready_state)\n",
    "            after_q_values_per_model.append(mean)\n",
    "            after_q_variances_per_model.append(variance)\n",
    "        else:\n",
    "            after_q_values_per_model.append(model_augmentation.model.expected_q_values(model_ready_state))\n",
    "\n",
    "    ret_val = [before_q_values_per_model, after_q_values_per_model]\n",
    "    \n",
    "    if return_variance:\n",
    "        ret_val.extend((before_q_variances_per_model, after_q_variances_per_model))\n",
    "        \n",
    "    if return_before_and_after:\n",
    "        ret_val.extend((before_pixels, after_pixels))\n",
    "    \n",
    "    return ret_val\n",
    "\n",
    "\n",
    "def max_wrapper(tensor, axis=0):\n",
    "    val, idx = tensor.max(axis)\n",
    "    return float(val), int(idx)\n",
    "\n",
    "\n",
    "def evaluate_augmented_models(observations, augmented_index, model_augmentations, state_length=4,\n",
    "                              before_color='red', after_color='blue', bar_alpha=0.5,\n",
    "                              bar_width=0.8, fontdict=dict(fontsize=16), force_text=None, text_epsilon=0,\n",
    "                              plot_state=True):\n",
    "    if plot_state:\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(8 * len(model_augmentations) + 1, 6))\n",
    "        gs = fig.add_gridspec(2, 7)\n",
    "        before_q_values_per_model, after_q_values_per_model, before_state, after_state = augmented_state_q_values(observations, \n",
    "            augmented_index, model_augmentations, state_length=state_length, return_before_and_after=True)\n",
    "\n",
    "        before_ax = fig.add_subplot(gs[0, 0])\n",
    "        if len(before_state.shape) == 3:\n",
    "            before_ax.imshow(before_state)\n",
    "        else:\n",
    "            before_ax.imshow(before_state, cmap='gray')\n",
    "        before_ax.set_title('Before')\n",
    "        \n",
    "        after_ax = fig.add_subplot(gs[1, 0])\n",
    "        if len(after_state.shape) == 3:\n",
    "            after_ax.imshow(after_state)\n",
    "        else:\n",
    "            after_ax.imshow(after_state, cmap='gray')\n",
    "        after_ax.set_title('After')\n",
    "    \n",
    "    else:\n",
    "        plt.figure(figsize=(8 * len(model_augmentations), 6))\n",
    "        before_q_values_per_model, after_q_values_per_model = augmented_state_q_values(observations, augmented_index, \n",
    "                                                                                       model_augmentations, state_length=state_length)\n",
    "\n",
    "    \n",
    "\n",
    "    for i, (model_augmentation, before_q, after_q) in enumerate(zip(model_augmentations, \n",
    "                                                                                       before_q_values_per_model, after_q_values_per_model, \n",
    "                                                                                      )):\n",
    "        print(f'For model {model_augmentation.name}:')\n",
    "        before_mean, after_mean = before_q.mean().cpu().numpy(), after_q.mean().cpu().numpy()\n",
    "        before_v, after_v = before_q.max().cpu().numpy(), after_q.max().cpu().numpy()\n",
    "        print(f'Baseline mean: {before_mean:.3f} | Augmented mean: {after_mean:.3f} | Difference: {after_mean - before_mean:.3f}')\n",
    "        \n",
    "        diff = after_q - before_q\n",
    "        max_diff, max_diff_idx = max_wrapper(diff.abs())\n",
    "        print(f'Max Q value diff is {max_diff:.3f} for action {ALE_ACTIONS[max_diff_idx]} [{max_diff_idx}]')\n",
    "\n",
    "        if plot_state:\n",
    "            ax = fig.add_subplot(gs[:, 1 + 2 * i :1 + 2 * (i + 1)])\n",
    "        else:\n",
    "            ax = plt.subplot(1, len(model_augmentations), i + 1)\n",
    "        \n",
    "        locations = np.arange(before_q.shape[0]) * bar_width * 2.5\n",
    "        ax.bar(locations, before_q.cpu().numpy(), color=before_color, alpha=bar_alpha)\n",
    "        ax.bar(locations + bar_width, after_q.cpu().numpy(), color=after_color, alpha=bar_alpha)\n",
    "        ax.hlines([before_mean, after_mean], *ax.get_xlim(), colors=[before_color, after_color],\n",
    "                  linestyles='dashed')\n",
    "        \n",
    "        ax.hlines([before_v, after_v], *ax.get_xlim(), colors=[before_color, after_color],\n",
    "                  linestyles='dotted')\n",
    "        \n",
    "        text_fd = {k:fontdict[k] for k in fontdict}\n",
    "        ax.text(locations[max_diff_idx] + bar_width / 2, max(before_q[max_diff_idx], after_q[max_diff_idx]) + text_epsilon,\n",
    "                '*', fontdict=text_fd)\n",
    "        \n",
    "        before_action_q, before_action_idx = max_wrapper(before_q)\n",
    "        after_action_q, after_action_idx = max_wrapper(after_q)\n",
    "        \n",
    "        print(f'Before action: {ALE_ACTIONS[before_action_idx]} [{before_action_idx}] (Q = {before_action_q:.3f}) | After action: {ALE_ACTIONS[after_action_idx]} [{after_action_idx}] (Q = {after_action_q:.3f})')\n",
    "        \n",
    "        if before_action_idx != after_action_idx or (force_text is not None and force_text):\n",
    "            text_fd['color'] = before_color\n",
    "            ax.text(locations[before_action_idx] - (bar_width / 2), before_action_q + text_epsilon, 'B', fontdict=text_fd)\n",
    "\n",
    "            text_fd['color'] = after_color\n",
    "            ax.text(locations[after_action_idx] + (bar_width / 2), after_action_q + text_epsilon, 'A', fontdict=text_fd)\n",
    "        \n",
    "        ax.set_xticks(locations + (bar_width / 2))\n",
    "        ax.set_xticklabels(sorted(ALE_ACTIONS.keys()))\n",
    "        \n",
    "        ax.set_xlabel('Action Index', fontdict=fontdict)\n",
    "        ax.set_ylabel('Q Value', fontdict=fontdict)\n",
    "        ax.set_title(model_augmentation.name, fontdict=fontdict)\n",
    "        print()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_multiple_models_single_augmented_state(observations, augmented_index, model_augmentations_per_group,\n",
    "                                                    key_actions=None, key_actions_aggregator_func=lambda t: t.mean(axis=1), \n",
    "                                                    state_length=4,\n",
    "                                                    before_color='red', after_color='blue', \n",
    "                                                    bar_alpha=0.5, bar_width=0.8, \n",
    "                                                    fontdict=dict(fontsize=16), force_text=None, \n",
    "                                                    text_epsilon=0, plot_state=True,\n",
    "                                                    state_value_title='State values',\n",
    "                                                    key_action_title='Key action Q-values',\n",
    "                                                    names=None):\n",
    "    # TODO: if using multiple states or models, we need to average properly over the before_q / after_q tensors\n",
    "    # That is, compute the average mean difference and the average max difference\n",
    "    # The per-action bar charts might make less sense?\n",
    "    # If we do this, we can plot a histogram of MMeanD and MMaxD and over the different random seeds (for one state)\n",
    "    # Or the different states for one (or more) random seeds\n",
    "    \n",
    "    plot_key_actions = key_actions is not None\n",
    "    num_panels = 1 + int(plot_state) + int(plot_key_actions)\n",
    "    constrained_layout = plot_state\n",
    "    fig = plt.figure(constrained_layout=constrained_layout, figsize=(8 * num_panels, 6))\n",
    "    \n",
    "    if plot_state:\n",
    "        gs = fig.add_gridspec(1, num_panels * 2)\n",
    "        _, _, before_state, after_state = augmented_state_q_values(observations, \n",
    "            augmented_index, [model_augmentations_per_group[0][0]], state_length=state_length, return_before_and_after=True)\n",
    "\n",
    "        before_ax = fig.add_subplot(gs[0, 0])\n",
    "        if len(before_state.shape) == 3:\n",
    "            before_ax.imshow(before_state)\n",
    "        else:\n",
    "            before_ax.imshow(before_state, cmap='gray')\n",
    "        before_ax.set_title('Before')\n",
    "        \n",
    "        after_ax = fig.add_subplot(gs[0, 1])\n",
    "        if len(after_state.shape) == 3:\n",
    "            after_ax.imshow(after_state)\n",
    "        else:\n",
    "            after_ax.imshow(after_state, cmap='gray')\n",
    "        after_ax.set_title('After')\n",
    "    \n",
    "    if plot_state:\n",
    "        state_value_ax = fig.add_subplot(gs[0, 2:4])\n",
    "        if plot_key_actions:\n",
    "            key_actions_ax = fig.add_subplot(gs[0, 4:])\n",
    "    else:\n",
    "        state_value_ax = plt.subplot(1, num_panels, 1)\n",
    "        if plot_key_actions:\n",
    "            key_actions_ax = plt.subplot(1, 2, 2)\n",
    "    \n",
    "    all_state_values_before = []\n",
    "    all_state_values_after = []\n",
    "    \n",
    "    all_state_variances_before = []\n",
    "    all_state_variances_after = []\n",
    "    \n",
    "    if plot_key_actions:\n",
    "        all_key_action_values_before = []\n",
    "        all_key_action_values_after = []\n",
    "        \n",
    "        all_key_action_variances_before = []\n",
    "        all_key_action_variances_after = []\n",
    "        \n",
    "    if names is None:\n",
    "        names = [aug_group[0].name for aug_group in model_augmentations_per_group]\n",
    "    \n",
    "    for model_augmentations in model_augmentations_per_group:\n",
    "        q_values_before, q_variances_before, q_values_after, q_variances_after = augmented_state_q_values(observations, augmented_index, model_augmentations, \n",
    "                                                                                                          state_length=state_length, return_variance=True)\n",
    "        \n",
    "        q_values_before = torch.stack(q_values_before)\n",
    "        q_variances_before = torch.stack(q_variances_before)\n",
    "        q_values_after = torch.stack(q_values_after)\n",
    "        q_variances_after = torch.stack(q_variances_after)\n",
    "        \n",
    "        state_values_before, before_max_indices = q_values_before.max(1)\n",
    "        state_values_after, after_max_indices = q_values_after.max(1)\n",
    "        \n",
    "        all_state_values_before.append(state_values_before.cpu().numpy())\n",
    "        all_state_values_after.append(state_values_after.cpu().numpy())\n",
    "        \n",
    "        all_state_variances_before.append(q_variances_before.gather(1, before_max_indices.unsqueeze(1)).squeeze().cpu().numpy())\n",
    "        all_state_variances_after.append(q_variances_after.gather(1, after_max_indices.unsqueeze(1)).squeeze().cpu().numpy())\n",
    "        \n",
    "        if plot_key_actions:\n",
    "            all_key_action_values_before.append(key_actions_aggregator_func(q_values_before[:, key_actions]).cpu().numpy())\n",
    "            all_key_action_values_after.append(key_actions_aggregator_func(q_values_after[:, key_actions]).cpu().numpy())\n",
    "            \n",
    "            all_key_action_variances_before.append(key_actions_aggregator_func(q_variances_before[:, key_actions]).cpu().numpy())\n",
    "            all_key_action_variances_after.append(key_actions_aggregator_func(q_variances_after[:, key_actions]).cpu().numpy())\n",
    "        \n",
    "    create_box_plot_set(state_value_ax, \n",
    "                        [all_state_values_before, all_state_variances_before], \n",
    "                        [all_state_values_after, all_state_variances_after], names,\n",
    "                        before_color, after_color, 'Model Type', 'V(s)', state_value_title, \n",
    "                        fontdict, (None, dict(linestyle='dashed')))\n",
    "    \n",
    "    if plot_key_actions:\n",
    "        create_box_plot_set(key_actions_ax, \n",
    "                            [all_key_action_values_before, all_key_action_variances_before], \n",
    "                            [all_key_action_values_after, all_key_action_variances_after], names,\n",
    "                            before_color, after_color, 'Model Type', 'Q(s, a)', key_action_title, \n",
    "                            fontdict, (None, dict(linestyle='dashed')))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def create_box_plot_set(ax, value_sets_before, value_sets_after, names, before_color, after_color,\n",
    "                        x_label, y_label, title, fontdict, additional_boxplot_properties=None):\n",
    "    \n",
    "    if additional_boxplot_properties is None:\n",
    "        additional_boxplot_properties = [None] * len(value_sets_before)\n",
    "    \n",
    "    before_positions = np.arange(len(names)) * 2 * len(value_sets_before)\n",
    "    after_positions = before_positions + len(value_sets_before)\n",
    "    \n",
    "    for i, (values_before, values_after, boxplot_props) in enumerate(zip(value_sets_before, \n",
    "                                                                         value_sets_after, \n",
    "                                                                         additional_boxplot_properties)):\n",
    "        if boxplot_props is None:\n",
    "            boxplot_props = {}\n",
    "    \n",
    "        ax.boxplot(np.array(values_before).T, positions=before_positions + i, \n",
    "                   boxprops=dict(color=before_color, **boxplot_props), \n",
    "                   whiskerprops=dict(color=before_color, **boxplot_props),\n",
    "                   capprops=dict(color=before_color, **boxplot_props),\n",
    "                   medianprops=boxplot_props)\n",
    "        ax.boxplot(np.array(values_after).T, positions=after_positions + i, \n",
    "                   boxprops=dict(color=after_color, **boxplot_props), \n",
    "                   whiskerprops=dict(color=after_color, **boxplot_props),\n",
    "                   capprops=dict(color=after_color, **boxplot_props), \n",
    "                   medianprops=boxplot_props)\n",
    "\n",
    "    dashed_line_positions = (before_positions - .5)[1:]\n",
    "    ax.vlines(dashed_line_positions, *ax.get_ylim(), colors='gray', linestyles='dashed')\n",
    "\n",
    "    xtick_positions = np.concatenate([before_positions, after_positions]) + 0.5 * (len(value_sets_before) - 1)\n",
    "    xtick_labels = [f'{name}\\nBefore' for name in names] + [f'{name}\\nAfter' for name in names]\n",
    "    ax.set_xticks(xtick_positions)\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "    ax.set_xlabel(x_label, fontdict=fontdict)\n",
    "    ax.set_ylabel(y_label, fontdict=fontdict)\n",
    "    ax.set_title(title, fontdict=fontdict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGONAL_CONNECTIVITY_STRUCTURE = ndimage.generate_binary_structure(2, 2)\n",
    "\n",
    "\n",
    "def to_tensor(numpy_arr, model):\n",
    "    return torch.Tensor(numpy_arr).to(model.online_net.convs[0].weight.device)\n",
    "\n",
    "\n",
    "def extract_raw_pixels_object(observations, obs_index, model, env, loc):\n",
    "    obs = observations[obs_index]\n",
    "    return env._to_tensor(obs[loc[0], loc[1], :])\n",
    "#     model_ready_obs = to_tensor(observation_to_model(env, observations[obs_index]).cpu().numpy(), model)\n",
    "#     return model_ready_obs.squeeze()[loc]\n",
    "\n",
    "\n",
    "def extract_object(observations, obs_index, model, env, channel_index, \n",
    "                   object_index=0, structure=DIAGONAL_CONNECTIVITY_STRUCTURE,\n",
    "                   return_tensor=True, return_location=False):\n",
    "#     state = observation_to_model(env, observations[obs_index]).cpu().numpy()\n",
    "    state = env.masker(env._to_tensor(observations[obs_index])).cpu().numpy()\n",
    "    labeled, count = ndimage.label(state[channel_index], structure)\n",
    "    locations = ndimage.find_objects(labeled, 0)\n",
    "    loc = locations[object_index]\n",
    "    \n",
    "    object_arr = state[channel_index, loc[0], loc[1]]\n",
    "    if return_tensor:\n",
    "        object_arr = to_tensor(object_arr, model)\n",
    "        \n",
    "    if not return_location:\n",
    "        return object_arr\n",
    "    \n",
    "    return object_arr, loc\n",
    "    \n",
    "\n",
    "def copy_model_augmentation(model_augmentation, name=None, model=None, env=None, \n",
    "                            pixel_augmentations=None, mask_augmentations=None):\n",
    "    if name is None:\n",
    "        name = model_augmentation.name\n",
    "    \n",
    "    if model is None:\n",
    "        model = model_augmentation.model\n",
    "        \n",
    "    if env is None:\n",
    "        env = model_augmentation.env\n",
    "    \n",
    "    if pixel_augmentations is None:\n",
    "        pixel_augmentations = model_augmentation.pixel_augmentations\n",
    "        \n",
    "    if mask_augmentations is None:\n",
    "        mask_augmentations = model_augmentation.mask_augmentations\n",
    "        \n",
    "    return ModelAugmentation(name, model, env, pixel_augmentations, mask_augmentations)\n",
    "\n",
    "\n",
    "def make_augmentations_all_models(b_aug, m_p_aug, m_o_aug, pixels_tensor, mask_tensor, mask_channel_index, locations):\n",
    "    pixel_augs = [ChannelAugmentation(0, pixels_tensor, loc) for loc in locations]\n",
    "    mask_augs = [ChannelAugmentation(mask_channel_index, mask_tensor, loc) for loc in locations]\n",
    "    \n",
    "    return copy_model_augmentation(b_aug, pixel_augmentations=pixel_augs),\\\n",
    "        copy_model_augmentation(m_p_aug, pixel_augmentations=pixel_augs, mask_augmentations=mask_augs),\\\n",
    "        copy_model_augmentation(m_o_aug, mask_augmentations=mask_augs)\n",
    "\n",
    "\n",
    "def plot_tensors(*tensors, norm=False):\n",
    "    normalizer = None\n",
    "    if norm:\n",
    "        normalizer = matplotlib.colors.Normalize(0, 1)\n",
    "    \n",
    "    n = len(tensors)\n",
    "    plt.figure(figsize=(4 * n, 4))\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        ax.imshow(tensors[i].cpu().numpy(), cmap='gray', norm=normalizer)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_observations(observations, start, count, step):\n",
    "    plt.figure(figsize=(4 * count, 4))\n",
    "    for i, idx in enumerate(range(start, start + count * step, step)):\n",
    "        ax = plt.subplot(1, count, i + 1)\n",
    "        ax.imshow(observations[idx])\n",
    "        ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.set_title(idx)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_observations_by_indices(observations, indices):\n",
    "    n = len(indices)\n",
    "    plt.figure(figsize=(4 * n, 4))\n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        ax.imshow(observations[idx])\n",
    "        ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.set_title(idx)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def change_intensity(tensor, additive=0, multiplicative=1, min_val=0, max_val=1):\n",
    "    return torch.clamp(tensor * multiplicative + additive, min_val, max_val)\n",
    "    \n",
    "\n",
    "DEFAULT_MASK_NAMES = (\n",
    "    'None',\n",
    "    'player',\n",
    "    'bad_animal',\n",
    "    'land',\n",
    "    'bear',\n",
    "    'unvisited_floes',\n",
    "    'visited_floes',\n",
    "    'good_animal',\n",
    "    'igloo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5671, 210, 160, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/guy/anaconda3/envs/rainbow/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model: /scratch/gd1279/masks-and-pixels-replication-306-10000000.pth\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__new__() missing 1 required positional argument: 'mask_augmentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d90cd5ea4e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmasks_only_model_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpass_states_through_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_only_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_only_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_full_color_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mbaseline_aug_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelAugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmasks_and_pixels_aug_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelAugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Masks+Pixels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_and_pixels_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_and_pixels_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmasks_only_aug_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelAugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Masks-Only'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_only_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_only_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() missing 1 required positional argument: 'mask_augmentations'"
     ]
    }
   ],
   "source": [
    "SAMPLE_SAVED_STATES = r'/home/gd1279/scratch/rainbow-evaluation-state-traces/baseline-rainbow-305/evaluation/states/eval-baseline-rainbow-305-34350000-0-env.pickle'\n",
    "\n",
    "with open(SAMPLE_SAVED_STATES, 'rb') as state_file:\n",
    "    sample_full_color_observations = pickle.load(state_file)\n",
    "    \n",
    "    \n",
    "sample_full_color_observations = sample_full_color_observations.astype(np.uint8)\n",
    "print(sample_full_color_observations.shape)\n",
    "\n",
    "\n",
    "baseline_run = api.run('augmented-frostbite/initial-experiments/runs/fdxobftk')\n",
    "baseline_model, baseline_env = load_model_from_run(baseline_run)\n",
    "baseline_model_results = pass_states_through_model(baseline_model, baseline_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "masks_and_pixels_run = api.run('augmented-frostbite/masks-and-pixels-replication/runs/grh1bzvv')\n",
    "masks_and_pixels_model, masks_and_pixels_env = load_model_from_run(masks_and_pixels_run)\n",
    "masks_and_pixels_model_results = pass_states_through_model(masks_and_pixels_model, masks_and_pixels_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "masks_only_run = api.run('augmented-frostbite/masks-only-replication/runs/0khc2n2c')\n",
    "masks_only_model, masks_only_env = load_model_from_run(masks_only_run)\n",
    "masks_only_model_results = pass_states_through_model(masks_only_model, masks_only_env, sample_full_color_observations, skip=1)\n",
    "\n",
    "baseline_aug_template = ModelAugmentation('Baseline', baseline_model, baseline_env, list(), list())\n",
    "masks_and_pixels_aug_template = ModelAugmentation('Masks+Pixels', masks_and_pixels_model, masks_and_pixels_env, list(), list())\n",
    "masks_only_aug_template = ModelAugmentation('Masks-Only', masks_only_model, masks_only_env, list(), list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(*run_urls, run_checker=lambda t: True, step=None):\n",
    "    runs = [run for run in api.runs(run_urls[0]) if run_checker(run)]\n",
    "    for url in run_urls[1:]:\n",
    "        runs.extend([run for run in api.runs(url) if run_checker(run)])\n",
    "    \n",
    "    loaded_models_and_envs = [load_model_from_run(run, step=step) for run in runs]\n",
    "    return zip(*loaded_models_and_envs)\n",
    "\n",
    "\n",
    "all_baseline_models, all_baseline_envs = load_all_models('augmented-frostbite/initial-experiments/runs', \n",
    "                                                         run_checker=lambda run: run.name.lower().startswith('baseline-rainbow-3'),\n",
    "                                                         step=10000000)\n",
    "\n",
    "all_masks_and_pixels_models, all_masks_and_pixels_envs = load_all_models(#'augmented-frostbite/masks-and-pixels-fixed-resume/runs',\n",
    "                                                                         'augmented-frostbite/masks-and-pixels-replication/runs',\n",
    "                                                                         step=10000000)\n",
    "\n",
    "all_masks_only_models, all_masks_only_envs = load_all_models(#'augmented-frostbite/masks-only/runs',\n",
    "                                                             'augmented-frostbite/masks-only-replication/runs',\n",
    "                                                             step=10000000)\n",
    "\n",
    "print(len(all_baseline_models), len(all_masks_and_pixels_models), len(all_masks_only_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A -- Same shape, same colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.1 Additional player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model Baseline:\n",
      "Baseline mean: 4.134 | Augmented mean: 3.893 | Difference: -0.241\n",
      "Max Q value diff is 0.338 for action right + fire [11]\n",
      "Before action: down + right + fire [16] (Q = 4.825) | After action: down + right + fire [16] (Q = 4.760)\n",
      "\n",
      "For model Masks+Pixels:\n",
      "Baseline mean: 2.892 | Augmented mean: 3.306 | Difference: 0.414\n",
      "Max Q value diff is 0.556 for action down + right [8]\n",
      "Before action: down + right + fire [16] (Q = 3.598) | After action: down + right + fire [16] (Q = 4.098)\n",
      "\n",
      "For model Masks-Only:\n",
      "Baseline mean: 4.474 | Augmented mean: 5.158 | Difference: 0.683\n",
      "Max Q value diff is 1.212 for action up + left + fire [15]\n",
      "Before action: down + right [8] (Q = 6.038) | After action: down + right [8] (Q = 5.964)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "OBS_INDEX = 1449\n",
    "\n",
    "player_mask_tensor, player_loc = extract_object(sample_full_color_observations, OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=0, object_index=0, return_location=True)\n",
    "player_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, OBS_INDEX, baseline_model, baseline_env, player_loc)\n",
    "\n",
    "NEW_PLAYER_LOCATION = (45, 45)\n",
    "\n",
    "baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  player_pixels_tensor, player_mask_tensor, 0, [NEW_PLAYER_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, OBS_INDEX, \n",
    "                         [baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.2 Adding a completed igloo to a state without a complete igloo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1200\n",
    "DST_OBS_INEX = 2904\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 2600\n",
    "DST_OBS_INEX = 2100\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.3 Adding many of a particular good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 330\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, 5 + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A.4 Same but with a bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, bad_animal_loc[1].start + 5 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B -- Same shape, new color\n",
    "* New color in this case means new intensity since grayscale.\n",
    "* Can probaly repeat the same experiments as above but just changing the pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.1 Another player, new color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_INDEX = 1449\n",
    "CHANEL_INDEX = 0\n",
    "\n",
    "player_mask_tensor, player_loc = extract_object(sample_full_color_observations, OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANEL_INDEX, object_index=0, return_location=True)\n",
    "player_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, OBS_INDEX, baseline_model, baseline_env, player_loc)\n",
    "\n",
    "lightened_player_pixels_tensor = change_intensity(player_pixels_tensor, multiplicative=3.0)\n",
    "\n",
    "# plot_tensors(player_mask_tensor, player_pixels_tensor, lightened_player_pixels_tensor, norm=True)\n",
    "\n",
    "NEW_PLAYER_LOCATION = (45, 45)\n",
    "\n",
    "baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_player_pixels_tensor, player_mask_tensor, CHANEL_INDEX, [NEW_PLAYER_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, OBS_INDEX, \n",
    "                         [baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_INDEX = 1449\n",
    "CHANEL_INDEX = 0\n",
    "\n",
    "player_mask_tensor, player_loc = extract_object(sample_full_color_observations, OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANEL_INDEX, object_index=0, return_location=True)\n",
    "player_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, OBS_INDEX, baseline_model, baseline_env, player_loc)\n",
    "\n",
    "darkened_player_pixels_tensor = change_intensity(player_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(player_mask_tensor, player_pixels_tensor, darkened_player_pixels_tensor, norm=True)\n",
    "\n",
    "NEW_PLAYER_LOCATION = (45, 45)\n",
    "\n",
    "baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_player_pixels_tensor, player_mask_tensor, CHANEL_INDEX, [NEW_PLAYER_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, OBS_INDEX, \n",
    "                         [baseline_player_aug, masks_and_pixels_player_aug, masks_only_player_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.2 Adding a completed igloo to a state without a complete igloo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 2600\n",
    "DST_OBS_INEX = 2100\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "lightened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=1.75)\n",
    "\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 2600\n",
    "DST_OBS_INEX = 2100\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "\n",
    "darkened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor, darkened_igloo_pixels_tensor, norm=True)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "lightened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=1.75)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 450\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 7\n",
    "\n",
    "igloo_mask_tensor, igloo_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "igloo_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, igloo_loc)\n",
    "darkened_igloo_pixels_tensor = change_intensity(igloo_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(igloo_mask_tensor, igloo_pixels_tensor)\n",
    "\n",
    "NEW_IGLOO_LOCATION = (igloo_loc[0].start, igloo_loc[1].start)\n",
    "\n",
    "baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_igloo_pixels_tensor, igloo_mask_tensor, CHANNEL_INDEX, [NEW_IGLOO_LOCATION])\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_igloo_aug, masks_and_pixels_igloo_aug, masks_only_igloo_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.3 Adding many of a particular good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor, lightened_fish_tensor, norm=True)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor, lightened_fish_tensor, norm=True)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start - 10, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 330\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, 5 + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 330\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "lightened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=1.5)\n",
    "darkened_fish_tensor = change_intensity(fish_pixels_tensor, multiplicative=0.5)\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, 5 + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_fish_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B.4 Same but with a bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, bad_animal_loc[1].start + 5 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, bad_animal_loc[1].start + 5 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "# NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor, lightened_bad_animal_pixels_tensor, darkened_bad_animal_tensor, norm=True)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "# NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start + 11, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 94\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  lightened_bad_animal_pixels_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 1270\n",
    "DST_OBS_INEX = 1112\n",
    "CHANNEL_INDEX = 1\n",
    "OBJECT_INDEX = 1\n",
    "\n",
    "bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=OBJECT_INDEX, return_location=True)\n",
    "bad_animal_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, bad_animal_loc)\n",
    "lightened_bad_animal_pixels_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=1.5)\n",
    "darkened_bad_animal_tensor = change_intensity(bad_animal_pixels_tensor, multiplicative=0.33)\n",
    "# plot_tensors(bad_animal_mask_tensor, bad_animal_pixels_tensor)\n",
    "# print(bad_animal_loc)\n",
    "\n",
    "NEW_BAD_ANIMAL_LOCATIONS = [(bad_animal_loc[0].start, 10 + 7 * i) for i in range(10)]\n",
    "\n",
    "baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  darkened_bad_animal_tensor, bad_animal_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "evaluate_augmented_models(sample_full_color_observations, DST_OBS_INEX, \n",
    "                         [baseline_bad_animal_aug, masks_and_pixels_bad_animal_aug, masks_only_bad_animal_aug],\n",
    "                          force_text=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C -- New shape (and presumably, new color)\n",
    "* What happens if an alien from space invaders comes to play Frostbite?\n",
    "* The current alien is a little bit larger than some existing other objects\n",
    "* **TODO: do we use the same image as the mask and the pixels?** Let's make it a little bit later brighter, like the other images tend to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_pixels_tensor = torch.tensor([[  0,   0,  52, 243,  52,   0,   0],\n",
    "       [  0,   0, 243, 243, 243,   0,   0],\n",
    "       [ 52, 243, 202, 243, 202, 243,  52],\n",
    "       [243, 243, 148, 243, 148, 243, 243],\n",
    "       [ 52, 243,  93, 243,  93, 243,  52],\n",
    "       [243,   0,   0,   0,   0,   0, 243],\n",
    "       [  0, 243,   0,   0,   0, 243,   0]], dtype=torch.float32, device=bad_animal_pixels_tensor.device) / 255\n",
    "\n",
    "print(alien_pixels_tensor.mean())\n",
    "print(bad_animal_pixels_tensor.mean(), fish_pixels_tensor.mean(), igloo_pixels_tensor.mean(), player_pixels_tensor.mean())\n",
    "\n",
    "alien_mask_tensor = change_intensity(alien_pixels_tensor, multiplicative=2)\n",
    "print(alien_mask_tensor.mean())\n",
    "print(bad_animal_mask_tensor.mean(), fish_mask_tensor.mean(), igloo_mask_tensor.mean(), player_mask_tensor.mean())\n",
    "\n",
    "plot_tensors(alien_pixels_tensor, alien_mask_tensor, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_alien_pixels_tensor = torch.tensor([[  0,  24, 243,  24,   0],\n",
    "       [ 24, 243, 243, 243,  24],\n",
    "       [243, 122, 243, 122, 243],\n",
    "       [ 24,   0, 243,   0,  24],\n",
    "       [ 44,   0,   0,   0,  44]], dtype=torch.float32, device=bad_animal_pixels_tensor.device) / 255\n",
    "                                         \n",
    "print(small_alien_pixels_tensor.mean())\n",
    "print(bad_animal_pixels_tensor.mean(), fish_pixels_tensor.mean(), igloo_pixels_tensor.mean(), player_pixels_tensor.mean())\n",
    "\n",
    "small_alien_mask_tensor = change_intensity(small_alien_pixels_tensor, multiplicative=2)\n",
    "print(small_alien_mask_tensor.mean())\n",
    "print(bad_animal_mask_tensor.mean(), fish_mask_tensor.mean(), igloo_mask_tensor.mean(), player_mask_tensor.mean())\n",
    "\n",
    "darker_small_alien_tensor = change_intensity(small_alien_pixels_tensor, multiplicative=0.5)\n",
    "\n",
    "plot_tensors(alien_pixels_tensor, alien_mask_tensor, small_alien_pixels_tensor, small_alien_mask_tensor, darker_small_alien_tensor, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.1 Aliens as bad animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 1\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((0, 10, 20), (0, 10, 20))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          small_alien_pixels_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.2 Darker alien as bad animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 1\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((10, 20, 30), (10, 20, 30))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          darker_small_alien_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.3 Aliens as good animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 6\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((0, 10, 20), (0, 10, 20))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          small_alien_pixels_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.4 Darker alien as good animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 6\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                   ((0, 10, 20), (0, 10, 20))):\n",
    "    for row_loc_inc in row_loc_incs:\n",
    "    \n",
    "        bad_animal_mask_tensor, bad_animal_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "\n",
    "        NEW_BAD_ANIMAL_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "        baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "            make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                          darker_small_alien_tensor, small_alien_mask_tensor, CHANNEL_INDEX, NEW_BAD_ANIMAL_LOCATIONS)\n",
    "\n",
    "        evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                 [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                  force_text=True)\n",
    "        \n",
    "        display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C.5 Aliens as visited and unvisited ice floes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 325\n",
    "CHANNEL_INDEX = 6\n",
    "START_LOC = (44, 10)\n",
    "\n",
    "for name, channel_index in zip(('Unvisited Floes', 'Visited Floes'), (4, 5)):\n",
    "    display(Markdown(f'## {name}'))\n",
    "    \n",
    "    for dst_index, row_loc_incs in zip((94, 1112),\n",
    "                                       ((0, 10, 20), (0, 10, 20))):\n",
    "        for row_loc_inc in row_loc_incs:\n",
    "\n",
    "            NEW_LOCATIONS = [(START_LOC[0] + row_loc_inc, START_LOC[1] + 7 * i) for i in range(10)]\n",
    "\n",
    "            baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug =\\\n",
    "                make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                              darker_small_alien_tensor, small_alien_mask_tensor, channel_index, NEW_LOCATIONS)\n",
    "\n",
    "            evaluate_augmented_models(sample_full_color_observations, dst_index, \n",
    "                                     [baseline_alien_as_bad_animal_aug, masks_and_pixels_alien_as_bad_animal_aug, masks_only_alien_as_bad_animal_aug],\n",
    "                                      force_text=True)\n",
    "\n",
    "            display(Markdown('----'))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D Same as above but in aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmentation_per_model(base_augmentation, models):\n",
    "    return [copy_model_augmentation(base_augmentation, model=model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SRC_OBS_INDEX = 310\n",
    "DST_OBS_INDEX = 875\n",
    "CHANNEL_INDEX = 6\n",
    "\n",
    "fish_mask_tensor, fish_loc = extract_object(sample_full_color_observations, SRC_OBS_INDEX, masks_only_model, masks_only_env,\n",
    "                                           channel_index=CHANNEL_INDEX, object_index=0, return_location=True)\n",
    "fish_pixels_tensor = extract_raw_pixels_object(sample_full_color_observations, SRC_OBS_INDEX, baseline_model, baseline_env, fish_loc)\n",
    "\n",
    "# plot_tensors(fish_mask_tensor, fish_pixels_tensor)\n",
    "# print(fish_loc)\n",
    "\n",
    "NEW_FISH_LOCATIONS = [(fish_loc[0].start, fish_loc[1].start + 5 * i) for i in range(15)]\n",
    "\n",
    "baseline_fish_aug, masks_and_pixels_fish_aug, masks_only_fish_aug =\\\n",
    "    make_augmentations_all_models(baseline_aug_template, masks_and_pixels_aug_template, masks_only_aug_template,\n",
    "                                  fish_pixels_tensor, fish_mask_tensor, CHANNEL_INDEX, NEW_FISH_LOCATIONS)\n",
    "\n",
    "all_baseline_fish_augs = make_augmentation_per_model(baseline_fish_aug, all_baseline_models)\n",
    "all_masks_and_pixels_fish_augs = make_augmentation_per_model(masks_and_pixels_fish_aug, all_masks_and_pixels_models)\n",
    "all_masks_only_fish_augs = make_augmentation_per_model(masks_only_fish_aug, all_masks_only_models)\n",
    "\n",
    "\n",
    "evaluate_multiple_models_single_augmented_state(sample_full_color_observations, DST_OBS_INDEX, \n",
    "                                                (all_baseline_fish_augs, all_masks_and_pixels_fish_augs, all_masks_only_fish_augs),\n",
    "#                                                 key_actions=[5, 8, 9, 13, 16, 17],\n",
    "                                                names=('Rainbow', 'With Objects', 'Only Objects'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALE_ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Plotting samples states to grab objects from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_observations(sample_full_color_observations, 1230, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: finding intriguing states to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_sets = (baseline_model_results, masks_and_pixels_model_results, masks_only_model_results)\n",
    "\n",
    "q_value_arrays = [np.array([x.cpu().numpy() for x in result_set.q_values])\n",
    "                  for result_set in result_sets]\n",
    "mean_q_value_arrays = [np.tile(q_vals.mean(1), (q_vals.shape[1], 1)).T\n",
    "                       for q_vals in q_value_arrays]\n",
    "msd_array = np.array([np.power(q - mean, 2).mean(1) for (q, mean)\n",
    "                      in zip(q_value_arrays, mean_q_value_arrays)])\n",
    "\n",
    "mean_q_values = np.array([q_vals.mean(1) for q_vals in q_value_arrays])\n",
    "\n",
    "indices_without_extrema_q = np.argwhere(np.all(np.logical_and(mean_q_values > 3, mean_q_values < 7), axis=0))[:,0]\n",
    "\n",
    "indices_with_all_models_msd = np.argwhere(np.all(msd_array > 1, axis=0))[:,0]\n",
    "indices_with_two_models_msd = np.argwhere(np.sum(msd_array > 1, axis=0) > 0.5)[:,0]\n",
    "\n",
    "interesting_indices = sorted(set(list(indices_without_extrema_q)).intersection(set(list(indices_with_two_models_msd))))\n",
    "print(len(indices_without_extrema_q), len(indices_with_two_models_msd), len(interesting_indices))\n",
    "print(interesting_indices)\n",
    "\n",
    "s = 5\n",
    "for i in range(len(interesting_indices) // s):\n",
    "    plot_observations_by_indices(sample_full_color_observations, interesting_indices[i * s:(i + 1) * s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values(result_sets, names, **kwargs):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    q_value_arrays = [np.array([x.cpu().numpy() for x in result_set.q_values])\n",
    "                         for result_set in result_sets]\n",
    "    mean_q_value_arrays = [np.tile(q_vals.mean(1), (q_vals.shape[1], 1)).T\n",
    "                           for q_vals in q_value_arrays]\n",
    "    msd_arrays = [np.power(q - mean, 2).mean(1) for (q, mean)\n",
    "                  in zip(q_value_arrays, mean_q_value_arrays)]\n",
    "    \n",
    "    mean_ax = plt.subplot(2, 2, 1)\n",
    "    msd_ax = plt.subplot(2, 2, 2)\n",
    "    \n",
    "    for vals, msds, name in zip(q_value_arrays, msd_arrays, names):\n",
    "        mean_ax.plot(vals.mean(1), label=name, **kwargs)\n",
    "        msd_ax.plot(msds, label=name, **kwargs)\n",
    "        \n",
    "    mean_array = np.array([np.array([x.cpu().numpy() for x in res.q_values]).mean(1)\n",
    "                           for res in result_sets])\n",
    "    \n",
    "    min_max_mean_ax = plt.subplot(2, 2, 3)\n",
    "    min_max_mean_ax.plot(mean_array.min(0), label='Min')\n",
    "    min_max_mean_ax.plot(mean_array.max(0), label='Max')\n",
    "    \n",
    "    mean_msd_ax = plt.subplot(2, 2, 4)\n",
    "        \n",
    "    mean_ax.legend(loc='best')\n",
    "    msd_ax.legend(loc='best')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_q_values((baseline_model_results, masks_and_pixels_model_results, masks_only_model_results),\n",
    "              ('Baseline', 'Masks+Pixels', 'Masks-Only'), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_without_extrema_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(100, 100 + 10 * 10, 10))dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = masks_only_run.history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h['human_hours'][h['human_hours'].last_valid_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_hours_all_models(*run_urls, run_checker=lambda t: True):\n",
    "    runs = [run for run in api.runs(run_urls[0]) if run_checker(run)]\n",
    "    for url in run_urls[1:]:\n",
    "        runs.extend([run for run in api.runs(url) if run_checker(run)])\n",
    "    \n",
    "    all_human_hours = []\n",
    "    for run in runs:\n",
    "        h = run.history()\n",
    "        human_hours = h['human_hours'][h['human_hours'].last_valid_index()]\n",
    "        all_human_hours.append(human_hours)\n",
    "    \n",
    "    all_human_hours = np.array(all_human_hours)\n",
    "    print(np.min(all_human_hours), np.mean(all_human_hours), np.max(all_human_hours))\n",
    "\n",
    "\n",
    "human_hours_all_models('augmented-frostbite/initial-experiments/runs', run_checker=lambda run: run.name.lower().startswith('baseline-rainbow-3'))\n",
    "\n",
    "human_hours_all_models('augmented-frostbite/masks-and-pixels-fixed-resume/runs',\n",
    "                       'augmented-frostbite/masks-and-pixels-replication/runs')\n",
    "\n",
    "human_hours_all_models('augmented-frostbite/masks-only/runs',\n",
    "                       'augmented-frostbite/masks-only-replication/runs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [observation_to_model(baseline_env, sample_full_color_observations[i])\n",
    "         for i in range(100, 104)]\n",
    "model_state = torch.cat(state, 0)\n",
    "model_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    probs = baseline_model.online_net(model_state.unsqueeze(0))\n",
    "    mean = (probs * baseline_model.support).sum(2)\n",
    "    var = (probs * (baseline_model.support - mean) ** 2).sum(2)\n",
    "    mean = mean.squeeze(0)\n",
    "    var = var.squeeze(0)\n",
    "    print(mean)\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = baseline_model.support.cpu().numpy()\n",
    "for i in range(var.shape[0]):\n",
    "    p = probs.squeeze(0)[i].cpu().numpy()\n",
    "    dist = rv_discrete(values=(x, p))\n",
    "    print(np.allclose(dist.var(), var[i].cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.stats??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.q_value_mean_variance(model_state)[0] == baseline_model.expected_q_values(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rainbow] *",
   "language": "python",
   "name": "conda-env-rainbow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
